{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>KOTAK SALESIAN SCHOOL</b></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>STUDENTS DATABASE MANAGEMENT</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Backup Files Before running New**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backup successful: D:/postgres_backups\\backup_kotakschooldb_2025-09-22_06-25-15.sql\n"
     ]
    }
   ],
   "source": [
    "# * PostgreSQL Credentials\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Hari@123\"\n",
    "DB_NAME = \"kotakschooldb\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "BACKUP_DIR = \"D:/postgres_backups\"  # * Backup directory\n",
    "\n",
    "# * Full path to pg_dump (if needed)\n",
    "PG_DUMP_PATH = r\"C:\\Program Files\\PostgreSQL\\17\\bin\\pg_dump.exe\"\n",
    "\n",
    "# * Ensure the backup directory exists\n",
    "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "\n",
    "# * Generate a timestamp for the backup file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "backup_file = os.path.join(BACKUP_DIR, f\"backup_{DB_NAME}_{timestamp}.sql\")\n",
    "\n",
    "# * Run pg_dump\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            PG_DUMP_PATH,  # Use full path if not in PATH\n",
    "            \"-U\", DB_USER,\n",
    "            \"-h\", DB_HOST,\n",
    "            \"-p\", DB_PORT,\n",
    "            \"-F\", \"c\",\n",
    "            \"-b\",\n",
    "            \"-v\",\n",
    "            \"-f\", backup_file,\n",
    "            DB_NAME\n",
    "        ],\n",
    "        env={**os.environ, \"PGPASSWORD\": DB_PASSWORD},\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    # * Check for errors\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Backup successful: {backup_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Backup failed!\\nError: {result.stderr}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è pg_dump not found at {PG_DUMP_PATH}. Check PostgreSQL installation or system PATH.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries & Define Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "GOOGLE_JSON_PATHS = {\n",
    "    \"2024-25\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2024-25.json\",\n",
    "    \"2025-26\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2025-26.json\",\n",
    "}\n",
    "\n",
    "GOOGLE_SHEET_TITLES = {\n",
    "    \"2024-25\": \"STUDENTS DETAILS 2024-25\",\n",
    "    \"2025-26\": \"STUDENTS DETAILS 2025-26\",\n",
    "}\n",
    "\n",
    "UNIQUE_KEY = \"Adm No.\"\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME1 = \"students\"\n",
    "TABLE_NAME2 = \"student_list\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract Data from Google Sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FETCH GOOGLE SHEET ===\n",
    "def fetch_data(sheet_title, worksheet_name=\"Overall\", json_path=None):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(sheet_title)\n",
    "    sheet = spreadsheet.worksheet(worksheet_name)\n",
    "    data = sheet.get_all_records(head=3)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === CLEAN COLUMN NAMES ===\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_APPLIED_SHEET_NAME = \"TC LIST\"\n",
    "\n",
    "def merge_and_tag():\n",
    "    # Fetch and clean data\n",
    "    df_2024 = clean_column_names(fetch_data(\n",
    "        GOOGLE_SHEET_TITLES[\"2024-25\"], \"Overall\", GOOGLE_JSON_PATHS[\"2024-25\"]\n",
    "    ))\n",
    "    df_2025 = clean_column_names(fetch_data(\n",
    "        GOOGLE_SHEET_TITLES[\"2025-26\"], \"Overall\", GOOGLE_JSON_PATHS[\"2025-26\"]\n",
    "    ))\n",
    "\n",
    "    # ‚úÖ Fetch TC applied sheet\n",
    "    df_tc_applied = clean_column_names(fetch_data(\n",
    "        GOOGLE_SHEET_TITLES[\"2024-25\"], TC_APPLIED_SHEET_NAME, GOOGLE_JSON_PATHS[\"2024-25\"]\n",
    "    ))\n",
    "\n",
    "    # Tag academic year\n",
    "    df_2024[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    # Ensure no NaN in unique key column\n",
    "    df_2024 = df_2024.dropna(subset=[UNIQUE_KEY])\n",
    "    df_2025 = df_2025.dropna(subset=[UNIQUE_KEY])\n",
    "\n",
    "    # Get sets of unique keys\n",
    "    codes_2024 = set(df_2024[UNIQUE_KEY])\n",
    "    codes_2025 = set(df_2025[UNIQUE_KEY])\n",
    "    tc_applied_ids = set(df_tc_applied[UNIQUE_KEY])\n",
    "\n",
    "    # Determine who left and who is new\n",
    "    left = codes_2024 - codes_2025\n",
    "    new = codes_2025 - codes_2024\n",
    "\n",
    "    # Find graduates = left students in max grade\n",
    "    max_grade = df_2024[\"GRADES\"].max()\n",
    "    graduates = set(\n",
    "        df_2024[(df_2024[\"GRADES\"] == max_grade) & (df_2024[UNIQUE_KEY].isin(left))][UNIQUE_KEY]\n",
    "    )\n",
    "\n",
    "    # üîπ Assign status_id for 2024\n",
    "    def get_status_2024(x):\n",
    "        if x in graduates:\n",
    "            return 4  # Graduated\n",
    "        elif x in tc_applied_ids:\n",
    "            return 5  # TC Applied\n",
    "        elif x in left:\n",
    "            return 2  # Not coming\n",
    "        else:\n",
    "            return 1  # Continuing\n",
    "\n",
    "    df_2024[\"status_id\"] = df_2024[UNIQUE_KEY].apply(get_status_2024)\n",
    "\n",
    "    # üîπ Assign status_id for 2025\n",
    "    df_2025[\"status_id\"] = df_2025[UNIQUE_KEY].apply(\n",
    "        lambda x: 3 if x in new else 1\n",
    "    )\n",
    "\n",
    "    # Merge and return\n",
    "    return pd.concat([df_2024, df_2025], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ‚ú® Rename columns to match your database structure\n",
    "    df.columns = [\n",
    "        \"sno\", \"adm_no\", \"name\", \"class\", \"gender\", \"mother_name\", \"father_name\",\n",
    "        \"pen_number\", \"dob\", \"phone_no\", \"religion\", \"caste\", \"sub_caste\",\n",
    "        \"second_lang\", \"remarks\", \"class_nos\", \"joined_year\", \"grade_id\",\"student_aadhar\", \"father_aadhar\", \"mother_aadhar\",\n",
    "        \"academic_year\", \"status_id\"\n",
    "    ]\n",
    "\n",
    "    # Lowercase and strip spaces for consistency\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # üóìÔ∏è Convert DOB to PostgreSQL-friendly format\n",
    "    df[\"dob\"] = pd.to_datetime(df[\"dob\"], format=\"%d-%m-%Y\", errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # üî¢ Convert joined_year to integer\n",
    "    df[\"joined_year\"] = pd.to_numeric(df[\"joined_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # üßπ Remove optional junk column\n",
    "    if \"apaar_status\" in df.columns:\n",
    "        df.drop(columns=[\"apaar_status\"], inplace=True)\n",
    "\n",
    "    # Capitalize gender and reset S.No\n",
    "    df[\"gender\"] = df[\"gender\"].str.upper()\n",
    "    df[\"sno\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # üü¢ Clean text fields\n",
    "    df[\"adm_no\"] = df[\"adm_no\"].astype(str).str.strip()\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "    df[\"academic_year\"] = df[\"academic_year\"].astype(str).str.strip()\n",
    "    \n",
    "\n",
    "    conditions = [\n",
    "        df[\"grade_id\"].between(1, 3),   # grade_id from 1 to 3\n",
    "        df[\"grade_id\"].between(4, 8)    # grade_id from 4 to 8\n",
    "    ]\n",
    "    choices = [1, 2]  # branch_id values\n",
    "\n",
    "    df[\"branch_id\"] = np.select(conditions, choices, default=3)\n",
    "\n",
    "\n",
    "    # Sort for visual clarity\n",
    "    df = df.sort_values(by=[\"academic_year\", \"class_nos\", \"gender\", \"name\"])\n",
    "\n",
    "    # Prefer non-null mother/father names when dropping duplicates\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[\"adm_no\", \"mother_name\", \"father_name\"],\n",
    "        ascending=[True, True, True],\n",
    "        na_position='last'  # Non-null values come first\n",
    "    )\n",
    "    \n",
    "    df[\"academic_year_id\"] = df[\"academic_year\"].apply(lambda x: 1 if x== \"2024-25\" else 2)\n",
    "    \n",
    "    # üßæ Save CSV for auditing\n",
    "    df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\students_data.csv\", index=False)\n",
    "\n",
    "    student_list_df = df_sorted.drop_duplicates(subset=\"adm_no\", keep=\"first\")[\n",
    "        [\n",
    "            \"adm_no\", \"name\", \"gender\", \"mother_name\", \"father_name\",\n",
    "            \"pen_number\", \"dob\", \"phone_no\", \"religion\", \"caste\",\n",
    "            \"sub_caste\", \"second_lang\", \"remarks\", \"student_aadhar\", \"father_aadhar\", \"mother_aadhar\",\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    student_list_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\student_list.csv\", index=False)\n",
    "\n",
    "\n",
    "    students_df = df[\n",
    "        [\n",
    "            \"adm_no\", \"class_nos\",\n",
    "            \"grade_id\", \"academic_year_id\", \"status_id\",\"branch_id\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"‚úÖ Cleaned and split data saved.\")\n",
    "\n",
    "    return student_list_df, students_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df, table_name):\n",
    "    import numpy as np\n",
    "    password = urllib.parse.quote_plus(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\",\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Table creation logic\n",
    "    table_create_sql = {\n",
    "        \"students\": \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS students (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                adm_no VARCHAR,\n",
    "                class_nos VARCHAR,\n",
    "                grade_id VARCHAR,\n",
    "                academic_year_id INT,\n",
    "                status_id INT,\n",
    "                branch_id INT\n",
    "            );\n",
    "        \"\"\",\n",
    "        \"student_list\": \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS student_list (\n",
    "                adm_no VARCHAR PRIMARY KEY,\n",
    "                name VARCHAR,\n",
    "                gender VARCHAR,\n",
    "                mother_name VARCHAR,\n",
    "                father_name VARCHAR,\n",
    "                pen_number VARCHAR,\n",
    "                dob DATE,\n",
    "                phone_no VARCHAR,\n",
    "                religion VARCHAR,\n",
    "                caste VARCHAR,\n",
    "                sub_caste VARCHAR,\n",
    "                second_lang VARCHAR,\n",
    "                remarks TEXT,\n",
    "                student_aadhar VARCHAR,\n",
    "                father_aadhar VARCHAR,\n",
    "                mother_aadhar VARCHAR\n",
    "            );\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # ‚úÖ Create table if it does not exist\n",
    "            if table_name in table_create_sql:\n",
    "                conn.execute(text(table_create_sql[table_name]))\n",
    "                print(f\"üì¶ Table '{table_name}' created if it didn't exist.\")\n",
    "\n",
    "            # üóëÔ∏è Truncate before insert\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;\"))\n",
    "            print(f\"üßπ Old records deleted from '{table_name}'.\")\n",
    "\n",
    "        df = df.replace({pd.NA: None, np.nan: None})\n",
    "        print(f\"‚è≥ Inserting data into '{table_name}'...\")\n",
    "\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=500)\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name};\"))\n",
    "            count = result.scalar()\n",
    "            print(f\"‚úÖ Insert complete. üìä Table '{table_name}' now contains {count} records.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating table '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean Extracted Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full student import pipeline...\n",
      "\n",
      "‚úÖ Cleaned and split data saved.\n",
      "üì¶ Table 'students' created if it didn't exist.\n",
      "üßπ Old records deleted from 'students'.\n",
      "‚è≥ Inserting data into 'students'...\n",
      "‚úÖ Insert complete. üìä Table 'students' now contains 3385 records.\n",
      "\n",
      "üì¶ Table 'student_list' created if it didn't exist.\n",
      "üßπ Old records deleted from 'student_list'.\n",
      "‚è≥ Inserting data into 'student_list'...\n",
      "‚úÖ Insert complete. üìä Table 'student_list' now contains 1945 records.\n",
      "\n",
      "üéâ All done! Both 'student_list' and 'students' tables updated successfully.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting full student import pipeline...\\n\")\n",
    "\n",
    "    merged_df = merge_and_tag()\n",
    "    student_list_df, students_df = clean_data(merged_df)\n",
    "\n",
    "    # Update master (student_list) and academic (students) tables\n",
    "    update_database(students_df, \"students\")\n",
    "    update_database(student_list_df, \"student_list\")\n",
    "\n",
    "    print(\"üéâ All done! Both 'student_list' and 'students' tables updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE REPORTS</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Google Console Service Account: myschooldb@woven-solution-446513-f2.iam.gserviceaccount.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Necessary Libraries & Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "# === CONFIG ===\n",
    "GOOGLE_JSON_PATHS = {\n",
    "    \"2024-25\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2024-25-fees.json\",\n",
    "    \"2025-26\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2025-26-fees.json\"\n",
    "}\n",
    "\n",
    "GOOGLE_SHEET_TITLES = {\n",
    "    \"2024-25\": \"Fee Reports 2024-25\",\n",
    "    \"2025-26\": \"Fee Reports 2025-26\",\n",
    "}\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "\n",
    "TABLE_NAME = \"fees_table\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Fetching Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FETCH GOOGLE SHEET ===\n",
    "def fetch_data(sheet_title, worksheet_name=\"Overall Sheet\", json_path=None):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(sheet_title)\n",
    "    sheet = spreadsheet.worksheet(worksheet_name)\n",
    "    data = sheet.get_all_records(head=3)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === CLEAN COLUMN NAMES ===\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MERGE AND TAG STATUS ===\n",
    "def merge_and_tag():\n",
    "    df_2024 = clean_column_names(fetch_data(GOOGLE_SHEET_TITLES[\"2024-25\"], \"Overall Sheet\", GOOGLE_JSON_PATHS[\"2024-25\"]))\n",
    "    df_2025 = clean_column_names(fetch_data(GOOGLE_SHEET_TITLES[\"2025-26\"], \"Overall Sheet\", GOOGLE_JSON_PATHS[\"2025-26\"]))\n",
    "\n",
    "    df_2024[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    return pd.concat([df_2024, df_2025], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Cleaning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df[:-1][:-6]  # Remove unwanted rows\n",
    "\n",
    "    df.columns = ['SNo', 'STUDENT_NAME', 'ADM_NO', 'FB_NO', 'CLASS',\n",
    "                  'Term1', 'Term2', 'Term3', 'Term4', 'Total_Fee_Paid',\n",
    "                  'Discount_Concession', 'Exempted', 'Total_Fee_Due', 'PermissionUpto',\n",
    "                  'Fine', 'Payment_Status', 'ClassNo', \"AcNo\", 'Concession_type', \n",
    "                  \"staff_name\", \"academic_year\"]\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # üö´ Remove blank admission numbers & student names\n",
    "    df = df[df[\"adm_no\"].astype(str).str.strip() != \"\"]\n",
    "    df = df.dropna(subset=[\"adm_no\"])\n",
    "    df = df[df[\"student_name\"].astype(str).str.strip() != \"\"]\n",
    "    df = df.dropna(subset=[\"student_name\"])\n",
    "\n",
    "    # üî¢ Convert numeric columns\n",
    "    columns_to_convert = [\"term1\", \"term2\", \"term3\", \"term4\", \"total_fee_paid\",\n",
    "                          \"discount_concession\", 'exempted', \"total_fee_due\", \"fine\"]\n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    # ‚ùå Drop unused columns\n",
    "    df = df.drop(columns=[\"acno\", 'concession_type'])\n",
    "\n",
    "    # üî¢ Add serial number\n",
    "    df[\"sno\"] = range(1, len(df) + 1)\n",
    "    df = df.sort_values(by=[\"sno\"])\n",
    "\n",
    "    # üí∞ Calculate total fees\n",
    "    df[\"total_fees\"] = df[\"total_fee_paid\"] + df[\"discount_concession\"] + df[\"total_fee_due\"] + df[\"exempted\"]\n",
    "\n",
    "    # üÜî Academic year mapping\n",
    "    df['academic_year_id'] = df['academic_year'].apply(lambda x: 1 if x == \"2024-25\" else 2)\n",
    "    df = df.sort_values(by=[\"academic_year_id\", \"classno\", \"student_name\"], ascending=[True, True, True])\n",
    "\n",
    "    # üìÇ Save main fees report\n",
    "    df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fees_report.csv\", index=False)\n",
    "\n",
    "    # ‚úÖ Ensure payment_status column exists\n",
    "    if \"payment_status\" not in df.columns:\n",
    "        df[\"payment_status\"] = \"Unknown\"\n",
    "\n",
    "    # üìÇ Create payment status table\n",
    "    payment_status_df = df[[\"payment_status\"]].sort_values(by=\"payment_status\").drop_duplicates().reset_index(drop=True).copy()\n",
    "    payment_status_df[\"payment_status_id\"] = range(1, len(payment_status_df) + 1)\n",
    "    payment_status_df = payment_status_df[[\"payment_status_id\", \"payment_status\"]]\n",
    "    payment_status_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\payment_status_table.csv\", index=False)\n",
    "    print(\"‚úÖ Fees Report & Payment Status Table created successfully.\\n\")\n",
    "\n",
    "    \n",
    "    # üìÇ Create staff child table\n",
    "    df[\"staff\"] = np.where(df['staff_name'].notnull() & df['staff_name'].str.strip().ne(''),1,0)    \n",
    "    \n",
    "    # ‚úÖ Extract only staff records for the child table\n",
    "    staff_child_df = df[df[\"staff\"] == 1][[\"staff_name\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Assign staff IDs sequentially\n",
    "    staff_child_df[\"staff_id\"] = range(1, len(staff_child_df) + 1)\n",
    "\n",
    "    # Save staff child table\n",
    "    staff_child_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\staff_child_table.csv\", index=False)\n",
    "    print(\"‚úÖ Staff Child Table created successfully.\\n\")\n",
    "\n",
    "    # --- Step 1: Merge Payment Status ---\n",
    "    if \"payment_status\" in df.columns and \"payment_status\" in payment_status_df.columns:\n",
    "        payment_status_df = payment_status_df.drop_duplicates(subset=[\"payment_status\"])\n",
    "        df = df.merge(payment_status_df, on=\"payment_status\", how=\"left\")\n",
    "\n",
    "    # --- Step 2: Merge Staff Child ---\n",
    "    # Drop duplicate merge keys to avoid _x / _y\n",
    "    merge_keys = [\"staff_name\"]\n",
    "    staff_child_clean = staff_child_df.drop(\n",
    "        columns=[col for col in staff_child_df.columns if col in df.columns and col not in merge_keys],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    df = df.merge(staff_child_clean, on=merge_keys, how=\"left\")\n",
    "\n",
    "    # Merge Payment Status first\n",
    "    df = pd.merge(df, payment_status_df, on=\"payment_status\", how=\"left\")\n",
    "    \n",
    "    # --- Step 3: Final Cleanup ---\n",
    "    # Drop any remaining _y columns\n",
    "    df = df.drop(columns=[c for c in df.columns if c.endswith(\"_y\")], errors=\"ignore\")\n",
    "\n",
    "    # Rename _x columns back to original\n",
    "    df.columns = [c.replace(\"_x\", \"\") for c in df.columns]\n",
    "\n",
    "    # ‚ùå Drop extra columns before DB insert\n",
    "    cols_to_drop = [\"permissionupto\", \"payment_status\", \"student_name\", \"class\", \"staff_name\", \"academic_year\"]\n",
    "    df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Updating the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode password for URL safety\n",
    "password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "\n",
    "# Create Engine\n",
    "engine = create_engine(f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "                       f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\")\n",
    "\n",
    "def table_exists(table_name):\n",
    "    check_query = \"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' AND table_name = :table_name\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(check_query), {\"table_name\": table_name}).scalar()\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    \"\"\"Create table only if it does not exist\"\"\"\n",
    "    print(\"üîß create_table() function called.\")  # Debug print\n",
    "    table_name = \"fees_table\"\n",
    "    \n",
    "    if table_exists(table_name):\n",
    "        print(f\"‚úÖ Table '{table_name}' already exists.\")\n",
    "        return\n",
    "\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE fees_table (\n",
    "    sno SERIAL PRIMARY KEY,\n",
    "    adm_no TEXT,\n",
    "    fb_no TEXT,\n",
    "    term1 NUMERIC DEFAULT 0,\n",
    "    term2 NUMERIC DEFAULT 0,\n",
    "    term3 NUMERIC DEFAULT 0,\n",
    "    term4 NUMERIC DEFAULT 0,\n",
    "    total_fee_paid NUMERIC DEFAULT 0,\n",
    "    discount_concession NUMERIC DEFAULT 0,\n",
    "    exempted NUMERIC DEFAULT 0,\n",
    "    total_fee_due NUMERIC DEFAULT 0,\n",
    "    fine NUMERIC DEFAULT 0,\n",
    "    classno INTEGER,\n",
    "    staff INTEGER,\n",
    "    staff_id INTEGER,\n",
    "    academic_year_id INTEGER NOT NULL,\n",
    "    total_fees INTEGER DEFAULT 0,\n",
    "    payment_status_id INTEGER\n",
    ");\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_query))\n",
    "            print(f\"‚úÖ Table '{table_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df):\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # ‚úÖ Truncate existing table and reset serial ID\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} RESTART IDENTITY CASCADE;\"))\n",
    "            print(f\"‚úÖ All records from the '{TABLE_NAME}' table have been deleted.\\n\")\n",
    "\n",
    "            # ‚úÖ Add UNIQUE constraint on 'admissionno' (if it doesn't exist)\n",
    "            conn.execute(text(f\"\"\"\n",
    "                DO $$ \n",
    "                BEGIN \n",
    "                    -- Drop old constraint if exists\n",
    "                    IF EXISTS (\n",
    "                        SELECT 1 FROM information_schema.table_constraints \n",
    "                        WHERE table_name = '{TABLE_NAME}' AND constraint_name = 'unique_admissionno'\n",
    "                    ) THEN\n",
    "                        ALTER TABLE {TABLE_NAME} DROP CONSTRAINT unique_admissionno;\n",
    "                    END IF;\n",
    "\n",
    "                    -- Add new composite unique constraint if not exists\n",
    "                    IF NOT EXISTS (\n",
    "                        SELECT 1 FROM information_schema.table_constraints \n",
    "                        WHERE table_name = '{TABLE_NAME}' AND constraint_name = 'unique_adm_year'\n",
    "                    ) THEN\n",
    "                        ALTER TABLE {TABLE_NAME} ADD CONSTRAINT unique_adm_year UNIQUE (\"adm_no\", \"academic_year_id\");\n",
    "                    END IF;\n",
    "                END $$;\n",
    "            \"\"\"))\n",
    "\n",
    "            print(f\"‚úÖ Unique constraint on 'admissionno' ensured in the '{TABLE_NAME}' table.\\n\")\n",
    "\n",
    "        print(\"‚úÖ Table cleared. Proceeding with data insertion...\\n\")\n",
    "\n",
    "        # ‚úÖ Normalize column names\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # ‚úÖ Insert data in chunks\n",
    "        df.to_sql(\n",
    "            name=TABLE_NAME,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ {len(df)} records successfully inserted into '{TABLE_NAME}'.\\n\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"‚ùå An error occurred during database update: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main Execution Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Raw data merged from both years.\n",
      "\n",
      "‚úÖ Fees Report & Payment Status Table created successfully.\n",
      "\n",
      "‚úÖ Staff Child Table created successfully.\n",
      "\n",
      "‚úÖ Data cleaned and transformed successfully.\n",
      "\n",
      "‚úÖ Final columns are:\n",
      " ['sno', 'adm_no', 'fb_no', 'term1', 'term2', 'term3', 'term4', 'total_fee_paid', 'discount_concession', 'exempted', 'total_fee_due', 'fine', 'classno', 'total_fees', 'academic_year_id', 'staff', 'payment_status_id', 'staff_id']\n",
      "üîß create_table() function called.\n",
      "‚úÖ Table 'fees_table' already exists.\n",
      "\n",
      "‚úÖ Table check/creation complete.\n",
      "\n",
      "‚úÖ Deduplicated. Final records to upload: 3384\n",
      "\n",
      "‚úÖ All records from the 'fees_table' table have been deleted.\n",
      "\n",
      "‚úÖ Unique constraint on 'admissionno' ensured in the 'fees_table' table.\n",
      "\n",
      "‚úÖ Table cleared. Proceeding with data insertion...\n",
      "\n",
      "‚úÖ 3384 records successfully inserted into 'fees_table'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # * Merge and tag both years\n",
    "    combined_df = merge_and_tag()\n",
    "    print(\"‚úÖ Raw data merged from both years.\\n\")\n",
    "\n",
    "    # * Clean and process the merged data\n",
    "    cleaned_df = clean_data(combined_df)\n",
    "    print(\"‚úÖ Data cleaned and transformed successfully.\\n\")\n",
    "    print(\"‚úÖ Final columns are:\\n\", cleaned_df.columns.to_list())\n",
    "\n",
    "    # * Create table if it does not exist\n",
    "    create_table()\n",
    "    print(\"\\n‚úÖ Table check/creation complete.\\n\")\n",
    "\n",
    "    # * Drop duplicates by adm_no + year before insert\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=[\"adm_no\", \"academic_year_id\"])\n",
    "    print(f\"‚úÖ Deduplicated. Final records to upload: {len(cleaned_df)}\\n\")\n",
    "\n",
    "    # * Upload data using safe insertion\n",
    "    update_database(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>DAY WISE REPORTS</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Login Credentials and MySQL Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * üîπ Login Credentials\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "urls_to_fetch = [\n",
    "    \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_reports_day_wise_receipt_wise_print?academic_years_id=1\",\n",
    "    \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_reports_day_wise_receipt_wise_print?academic_years_id=7\",\n",
    "]\n",
    "\n",
    "credentials = {\n",
    "    \"uname\": \"harikiran\",\n",
    "    \"psw\": \"812551\"\n",
    "}\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"daywise_fees_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Function to determine academic year from URL**\n",
    "def get_academic_year_from_url(url):\n",
    "    if \"academic_years_id=1\" in url:\n",
    "        return \"2024-25\"\n",
    "    elif \"academic_years_id=7\" in url:\n",
    "        return \"2025-26\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected academic_years_id in URL: {url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Functions for Each Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    login_response = session.post(login_url, data=credentials)\n",
    "\n",
    "    if \"Invalid\" in login_response.text:\n",
    "        print(\"‚ùå Login failed! Check credentials.\\n\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"‚úÖ Login successful!\\n\")\n",
    "        return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Fetch Fee Report Page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fee_report_page(session, data_url):\n",
    "    response = session.get(data_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Extract Data from Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Function to Extract Data from Table**\n",
    "def extract_data_from_table(table):\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cols = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "        if cols:\n",
    "            rows.append(cols)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No rows found in table\")\n",
    "\n",
    "    num_cols = len(rows[0])\n",
    "\n",
    "    if num_cols == 12:\n",
    "        header_row = [\n",
    "            \"SNo\", \"RecieptNo\", \"Class\", \"AdmissionNo\", \"StudentName\", \n",
    "            \"Date\", \"-\", \"Abacus / Vediic Maths\", \"TERM FEE\", \"TERM FEE2\", \n",
    "            \"ReceivedAmount\", \"Remarks\"\n",
    "        ]\n",
    "    elif num_cols == 11:\n",
    "        header_row = [\n",
    "            \"SNo\", \"RecieptNo\", \"Class\", \"AdmissionNo\", \"StudentName\", \n",
    "            \"Date\", \"-\", \"Abacus / Vediic Maths\", \"TERM FEE\", \n",
    "            \"ReceivedAmount\", \"Remarks\"\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"‚ö†Ô∏è Unexpected number of columns: {num_cols}\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=header_row)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tag_data(df, academic_year):\n",
    "    # 1Ô∏è‚É£ Date parsing\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y', errors='coerce')\n",
    "\n",
    "    # 2Ô∏è‚É£ Data type conversions\n",
    "    df['AdmissionNo'] = df['AdmissionNo'].astype(str)\n",
    "    if \"Class\" in df.columns:\n",
    "        df[\"Class\"] = df[\"Class\"].str.replace(\"/\", \" - \", regex=False)\n",
    "    if \"ReceivedAmount\" in df.columns:\n",
    "        df[\"ReceivedAmount\"] = pd.to_numeric(df[\"ReceivedAmount\"], errors=\"coerce\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Academic year tagging (numeric code)\n",
    "    df[\"academic_year_id\"] = 1 if academic_year == \"2024-25\" else 2\n",
    "\n",
    "    # 4Ô∏è‚É£ Apply \"TERM\" row slicing only for 2024-25\n",
    "    if academic_year == \"2024-25\" and \"SNo\" in df.columns:\n",
    "        term_index = df[df[\"SNo\"].astype(str).str.contains(\"TERM\", na=False)].index\n",
    "        if not term_index.empty:\n",
    "            df = df.iloc[:term_index[0]]\n",
    "\n",
    "    # 5Ô∏è‚É£ Drop unnecessary columns if they exist\n",
    "    cols_to_drop = [ \"-\", \"Abacus / Vediic Maths\", \"TERM FEE\", \"TERM FEE2\", \"RecieptNo\", \"Class\", \"StudentName\", 'Remarks']\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns])\n",
    "\n",
    "    # 6Ô∏è‚É£ Drop rows without valid dates\n",
    "    df = df.dropna(subset=[\"Date\"])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Update Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df):\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "    # Define SQL table creation (only if not exists)\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        \"SNo\" TEXT,\n",
    "        \"AdmissionNo\" TEXT,\n",
    "        \"Date\" DATE,\n",
    "        \"ReceivedAmount\" NUMERIC,\n",
    "        \"academic_year_id\" INTEGER\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(create_table_sql))  # ‚úÖ Ensure table exists\n",
    "\n",
    "            conn.execute(text(\"BEGIN;\"))\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME};\"))\n",
    "            conn.execute(text(\"COMMIT;\"))\n",
    "            print(f\"‚úÖ Table '{TABLE_NAME}' ensured and truncated.\\n\")\n",
    "\n",
    "            # ‚úÖ Insert DataFrame\n",
    "            df.to_sql(name=TABLE_NAME, con=engine, if_exists='append', index=False, method=\"multi\", chunksize=1000)\n",
    "            print(f\"‚úÖ {len(df)} records inserted into '{TABLE_NAME}' successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error inserting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Main Function**\n",
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for url in urls_to_fetch:\n",
    "        academic_year = get_academic_year_from_url(url)\n",
    "        print(f\"üìÑ Fetching data for academic year: {academic_year}\")\n",
    "        table = fetch_fee_report_page(session, url)\n",
    "\n",
    "        if table:\n",
    "            df = extract_data_from_table(table)\n",
    "            df = clean_and_tag_data(df, academic_year)\n",
    "            print(f\"‚úÖ Data extracted for year {academic_year} with {len(df)} records.\\n\")\n",
    "\n",
    "            df.to_csv(fr\"D:\\GITHUB\\kotak-school-dbms\\output_data\\daywise_fees_collection_{academic_year}.csv\", index=False)\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"‚ùå Table not found for year {academic_year}!\")\n",
    "\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        final_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\daywise_fees_collection.csv\", index=False)\n",
    "        print(\"‚úÖ Data saved to daywise_fees_collection.csv\\n\")\n",
    "\n",
    "        update_database(final_df)\n",
    "        print(\"‚úÖ Columns:\\n\", final_df.columns)\n",
    "        print(f\"‚úÖ Total {len(final_df)} records entered into database.\")\n",
    "    else:\n",
    "        print(\"‚ùå No data to process!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run the Main Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n",
      "\n",
      "üìÑ Fetching data for academic year: 2024-25\n",
      "‚úÖ Data extracted for year 2024-25 with 5824 records.\n",
      "\n",
      "üìÑ Fetching data for academic year: 2025-26\n",
      "‚úÖ Data extracted for year 2025-26 with 2109 records.\n",
      "\n",
      "‚úÖ Data saved to daywise_fees_collection.csv\n",
      "\n",
      "‚úÖ Table 'daywise_fees_collection' ensured and truncated.\n",
      "\n",
      "‚úÖ 7933 records inserted into 'daywise_fees_collection' successfully.\n",
      "\n",
      "‚úÖ Columns:\n",
      " Index(['SNo', 'AdmissionNo', 'Date', 'ReceivedAmount', 'academic_year_id'], dtype='object')\n",
      "‚úÖ Total 7933 records entered into database.\n"
     ]
    }
   ],
   "source": [
    "## **Run the Script**\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>ATTENDANCE REPORT</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Login to Website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚úÖ Config\n",
    "# login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "# attendance_url = \"https://app.myskoolcom.tech/kotak_vizag/admin/attedance_grid\"\n",
    "\n",
    "# credentials = {\n",
    "#     \"uname\": \"harikiran\",\n",
    "#     \"psw\": \"812551\"\n",
    "# }\n",
    "\n",
    "# download_folder = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\"\n",
    "# merged_output_path = os.path.join(download_folder, \"MergedAttendance_2025_26.csv\")\n",
    "\n",
    "# academic_ranges = {\n",
    "#     \"2025-26\": (\"2025-06-16\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "#     # \"2025-26\": (\"2025-06-16\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚úÖ Setup Chrome Driver\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# prefs = {\"download.default_directory\": download_folder}\n",
    "# chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # ‚úÖ Functions\n",
    "# def login():\n",
    "#     driver.get(login_url)\n",
    "#     wait.until(EC.presence_of_element_located((By.NAME, \"uname\"))).send_keys(credentials[\"uname\"])\n",
    "#     driver.find_element(By.NAME, \"psw\").send_keys(credentials[\"psw\"])\n",
    "#     driver.find_element(By.NAME, \"psw\").send_keys(Keys.RETURN)\n",
    "#     print(\"‚úÖ Logged in successfully!\")\n",
    "#     time.sleep(5)\n",
    "\n",
    "# def set_date_range(start, end):\n",
    "#     driver.get(attendance_url)\n",
    "#     time.sleep(2)\n",
    "#     from_date_input = wait.until(EC.presence_of_element_located((By.ID, \"from_attendance_date\")))\n",
    "#     driver.execute_script(\"arguments[0].removeAttribute('readonly')\", from_date_input)\n",
    "#     from_date_input.clear()\n",
    "#     from_date_input.send_keys(start)\n",
    "\n",
    "#     to_date_input = wait.until(EC.presence_of_element_located((By.ID, \"to_attendance_date\")))\n",
    "#     driver.execute_script(\"arguments[0].removeAttribute('readonly')\", to_date_input)\n",
    "#     to_date_input.clear()\n",
    "#     to_date_input.send_keys(end)\n",
    "\n",
    "#     print(f\"‚úÖ Date range set: {start} to {end}\")\n",
    "\n",
    "# def download_csv(filename):\n",
    "#     try:\n",
    "#         if os.path.exists(filename):\n",
    "#             os.remove(filename)\n",
    "#         download_button = wait.until(EC.element_to_be_clickable((By.ID, \"smaplecsv\")))\n",
    "#         download_button.click()\n",
    "#         time.sleep(8)\n",
    "#         downloaded = sorted(\n",
    "#             [f for f in os.listdir(download_folder) if f.endswith(\".csv\")],\n",
    "#             key=lambda x: os.path.getctime(os.path.join(download_folder, x)),\n",
    "#             reverse=True\n",
    "#         )[0]\n",
    "#         os.rename(os.path.join(download_folder, downloaded), filename)\n",
    "#         print(f\"‚úÖ Downloaded and renamed to: {filename}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error downloading file: {e}\")\n",
    "\n",
    "# def date_batches(start, end, months=1):\n",
    "#     start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "#     end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "#     while start_date < end_date:\n",
    "#         batch_end = min(start_date + timedelta(days=30 * months), end_date)\n",
    "#         yield (start_date.strftime(\"%Y-%m-%d\"), batch_end.strftime(\"%Y-%m-%d\"))\n",
    "#         start_date = batch_end + timedelta(days=1)\n",
    "\n",
    "# def merge_csvs(folder, output_file, year_filter=\"2025-26\"):\n",
    "#     all_csvs = [\n",
    "#         os.path.join(folder, f)\n",
    "#         for f in os.listdir(folder)\n",
    "#         if f.endswith(\".csv\") and year_filter in f\n",
    "#     ]\n",
    "\n",
    "#     merged_df = pd.DataFrame()\n",
    "\n",
    "#     for f in all_csvs:\n",
    "#         try:\n",
    "#             df = pd.read_csv(f, low_memory=False)\n",
    "#             if \"Students Number\" in df.columns:\n",
    "#                 # Merge logic: remove duplicates by date + student number\n",
    "#                 df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "#                 df = df.dropna(subset=[\"Date\", \"Students Number\"])\n",
    "\n",
    "#                 # Merge with deduplication\n",
    "#                 if not merged_df.empty:\n",
    "#                     merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "#                     merged_df.drop_duplicates(subset=[\"Date\", \"Students Number\"], keep=\"last\", inplace=True)\n",
    "#                 else:\n",
    "#                     merged_df = df\n",
    "#                 print(f\"üîÑ Merged file (with Students Number): {os.path.basename(f)}\")\n",
    "#             else:\n",
    "#                 # Append directly if \"Students Number\" not found\n",
    "#                 merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "#                 print(f\"‚ûï Appended file (no Students Number): {os.path.basename(f)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error reading file {f}: {e}\")\n",
    "\n",
    "#     merged_df.to_csv(output_file, index=False)\n",
    "#     print(f\"‚úÖ Final merged file saved: {output_file}\")\n",
    "\n",
    "#     # ‚úÖ MAIN Execution\n",
    "# login()\n",
    "\n",
    "# # ‚úÖ MAIN Execution\n",
    "# login()\n",
    "\n",
    "# for year, (start, end) in academic_ranges.items():\n",
    "#     print(f\"\\nüìÖ Downloading attendance for {year}\")\n",
    "#     for i, (s, e) in enumerate(date_batches(start, end)):\n",
    "#         s_fmt = datetime.strptime(s, \"%Y-%m-%d\")\n",
    "#         e_fmt = datetime.strptime(e, \"%Y-%m-%d\")\n",
    "#         filename = f\"Attendance_{year}_{s_fmt.strftime('%b')}_{e_fmt.strftime('%b')}.csv\"\n",
    "#         filepath = os.path.join(download_folder, filename)\n",
    "#         set_date_range(s_fmt.strftime(\"%Y-%m-%d\"), e_fmt.strftime(\"%Y-%m-%d\"))\n",
    "#         download_csv(filepath)\n",
    "\n",
    "# # ‚ùå No merging now ‚Äì only individual files will be downloaded and renamed\n",
    "# # merge_csvs(download_folder, merged_output_path, year_filter=\"2025-26\")\n",
    "\n",
    "# driver.quit()\n",
    "# print(\"‚úÖ All attendance downloads complete ‚Äì individual files saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine\n",
    "# import logging\n",
    "# import numpy as np\n",
    "# import urllib\n",
    "# import traceback\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# # * Configure logging\n",
    "# logging.basicConfig(filename=r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.log\", level=logging.ERROR, \n",
    "#                     format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 2: Define PostgreSQl Credentials & Table Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # * POSTGRES_CREDENTIALS\n",
    "# POSTGRES_CREDENTIALS = {\n",
    "#     \"username\": \"postgres\",\n",
    "#     \"password\": \"Hari@123\",\n",
    "#     \"host\": \"localhost\",\n",
    "#     \"port\": \"5432\",\n",
    "#     \"database\": \"kotakschooldb\",\n",
    "# }\n",
    "# TABLE_NAME = \"attendance_report\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 3: Load and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def load_and_clean_data(file1, file2, file3=None, file4=None, file5=None):\n",
    "#     # Load DataFrames (read as str to avoid DtypeWarning)\n",
    "#     dfs = [pd.read_csv(f, dtype=str) for f in [file1, file2, file3, file4, file5] if f is not None]\n",
    "\n",
    "#     # Clean column names\n",
    "#     for i in range(len(dfs)):\n",
    "#         dfs[i].columns = dfs[i].columns.str.strip().str.replace('\"', '', regex=False)\n",
    "\n",
    "#         # Strip spaces from all string columns\n",
    "#         dfs[i] = dfs[i].apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "#         # üõ† Date cleaning (if column exists)\n",
    "#         if 'Date' in dfs[i].columns:\n",
    "#             dfs[i]['Date'] = pd.to_datetime(dfs[i]['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "#             # Warn about invalid dates\n",
    "#             bad_dates = dfs[i][dfs[i]['Date'].isna()]\n",
    "#             if not bad_dates.empty:\n",
    "#                 print(f\"‚ö†Ô∏è Invalid dates found in file {i+1}:\")\n",
    "#                 print(bad_dates[['Date']].head(10))  # Show first 10\n",
    "\n",
    "#     # Merge logic\n",
    "#     base_df = dfs[0]\n",
    "#     for df in dfs[1:]:\n",
    "#         conflict_cols = [col for col in df.columns if col in base_df.columns and col != 'Students Number']\n",
    "#         df = df.drop(columns=conflict_cols, errors='ignore')\n",
    "#         base_df = base_df.merge(df, on=\"Students Number\", how=\"outer\")\n",
    "\n",
    "#     df = base_df\n",
    "\n",
    "#     # Merge fields like Name, Class if duplicated\n",
    "#     for field in ['Name', 'Class']:\n",
    "#         col_x, col_y = f\"{field}_x\", f\"{field}_y\"\n",
    "#         if col_x in df.columns and col_y in df.columns:\n",
    "#             df[field] = df[col_x].combine_first(df[col_y])\n",
    "#             df.drop([col_x, col_y], axis=1, inplace=True)\n",
    "#         elif col_x in df.columns:\n",
    "#             df[field] = df.pop(col_x)\n",
    "#         elif col_y in df.columns:\n",
    "#             df[field] = df.pop(col_y)\n",
    "\n",
    "#     # Drop remaining _x/_y columns\n",
    "#     df = df.drop(columns=[col for col in df.columns if col.endswith('_x') or col.endswith('_y')], errors='ignore')\n",
    "\n",
    "#     # Rename key identifier\n",
    "#     df = df.rename(columns={\"Students Number\": \"AdmissionNo\"})\n",
    "\n",
    "#     # Drop unnecessary columns\n",
    "#     drop_cols = ['Present Days', 'Absent Days', 'Toral Working Days']\n",
    "#     df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "#     # Reorder columns\n",
    "#     key_cols = ['AdmissionNo', 'Name', 'Class']\n",
    "#     other_cols = [col for col in df.columns if col not in key_cols]\n",
    "#     df = df[key_cols + other_cols]\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìå Step 4: Process Attendance Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def process_attendance_data(df):\n",
    "#     # * Step 1: Clean AdmissionNo (remove 786 and purely alphabetical ones)\n",
    "#     df = df[~(df[\"AdmissionNo\"].astype(str) == \"786\") & ~df[\"AdmissionNo\"].astype(str).str.match(r\"^[a-zA-Z]+$\")].copy()\n",
    "\n",
    "#     # * Step 2: Clean Class name (remove ICSE wrapper)\n",
    "#     df[\"Class\"] = df[\"Class\"].astype(str).str.replace(r\"ICSE \\((.*?)\\)\", r\"\\1\", regex=True)\n",
    "\n",
    "#     # * Step 3: Load class info with academic year\n",
    "#     student_df = pd.read_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fees_report.csv\")[[\"adm_no\", \"academic_year_id\", \"class\"]]\n",
    "#     print(\"‚úÖ Students Before Merging\\n\", len(df[\"AdmissionNo\"].unique()))\n",
    "#     df = df[df[\"AdmissionNo\"].isin(student_df[\"adm_no\"])]\n",
    "#     print(\"‚úÖ Students After Merging\\n\", len(df[\"AdmissionNo\"].unique()))\n",
    "\n",
    "#     # * Step 4: Unpivot attendance columns to Date-wise rows\n",
    "#     df_unpivot = pd.melt(df, id_vars=[\"AdmissionNo\", \"Name\", \"Class\"], var_name=\"Date\", value_name=\"AttendanceStatus\")\n",
    "#     df_unpivot[\"Date\"] = pd.to_datetime(df_unpivot[\"Date\"], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "#     # * Step 5: Remove invalid past records for new students\n",
    "#     numeric_mask = df_unpivot[\"AdmissionNo\"].str.isnumeric()\n",
    "#     df_unpivot.loc[numeric_mask, \"adm_no_int\"] = df_unpivot.loc[numeric_mask, \"AdmissionNo\"].astype(int)\n",
    "#     df_unpivot = df_unpivot[\n",
    "#         ~((df_unpivot[\"Date\"] < datetime(2024, 4, 1)) & (df_unpivot[\"adm_no_int\"] > 17165))\n",
    "#     ]\n",
    "#     df_unpivot.drop(columns=[\"adm_no_int\"], inplace=True)\n",
    "\n",
    "#     df_unpivot[\"id\"] = range(1, len(df_unpivot) + 1)\n",
    "\n",
    "#     if df_unpivot[\"Date\"].isna().sum() > 0:\n",
    "#         print(\"‚ö†Ô∏è Warning: Some Date values were invalid and converted to NaT.\")\n",
    "\n",
    "#     df_unpivot = df_unpivot.sort_values(\"Date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     # * Step 6: Mark \"Not Joined\"\n",
    "#     first_attendance_dates = df_unpivot[df_unpivot['AttendanceStatus'].notna()].groupby('AdmissionNo')['Date'].min()\n",
    "#     df_unpivot['AttendanceStatus'] = df_unpivot.apply(\n",
    "#         lambda row: \"Not Joined\" if row['Date'] < first_attendance_dates.get(row['AdmissionNo'], row['Date']) else row['AttendanceStatus'],\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "#     # * Step 7: Prioritize and deduplicate attendance\n",
    "#     priority_map = {'P': 2, 'A': 1, 'H': 3, 'Not Joined': 4, 'TC': 5}\n",
    "#     df_unpivot['Priority'] = df_unpivot[\"AttendanceStatus\"].map(priority_map)\n",
    "#     df_unpivot = df_unpivot.sort_values(by=['AdmissionNo', 'Date', 'Priority']) \\\n",
    "#                            .drop_duplicates(subset=['AdmissionNo', 'Date'], keep='first') \\\n",
    "#                            .drop(columns=['Priority'])\n",
    "\n",
    "#     # * Step 8: Clean Class + Standardize AttendanceStatus\n",
    "#     df_unpivot['Class'] = df_unpivot['Class'].str.replace(\"Pre KG - \", \"Pre KG\")\n",
    "#     df_unpivot[\"AttendanceStatus\"] = df_unpivot[\"AttendanceStatus\"].replace({\n",
    "#         'P': \"Present\", 'A': \"Absent\", 'H': \"Holiday\"\n",
    "#     })\n",
    "#     df_unpivot.sort_values(by=['Date'], ascending=False, inplace=True)\n",
    "\n",
    "#     # * Step 9: Assign academic year from Date\n",
    "#     df_unpivot['academic_year_id'] = df_unpivot['Date'].apply(\n",
    "#         lambda d: 1 if pd.Timestamp(\"2024-07-17\") <= d <= pd.Timestamp(\"2025-03-31\")\n",
    "#         else 2 if pd.Timestamp(\"2025-06-16\") <= d <= pd.Timestamp(datetime.today().date())\n",
    "#         else \"\"\n",
    "#     )\n",
    "\n",
    "#     # * Step 10: Assign ClassNo by academic year\n",
    "#     student_df[\"adm_no\"] = student_df[\"adm_no\"].astype(str)\n",
    "#     lookup_2024 = student_df[student_df[\"academic_year_id\"] == 1]\n",
    "#     lookup_2025 = student_df[student_df[\"academic_year_id\"] == 2]\n",
    "\n",
    "#     lookup_map_2024 = {row[\"adm_no\"]: row[\"class\"] for _, row in lookup_2024.iterrows()}\n",
    "#     lookup_map_2025 = {row[\"adm_no\"]: row[\"class\"] for _, row in lookup_2025.iterrows()}\n",
    "\n",
    "#     class_mapping = {\n",
    "#         \"Pre KG\": 1, \"LKG - A\": 2, \"LKG - B\": 3, \"UKG - A\": 4, \"UKG - B\": 5, \"UKG - C\": 6,\n",
    "#         \"I - A\": 7, \"I - B\": 8, \"I - C\": 9, \"I - D\": 10,\n",
    "#         \"II - A\": 11, \"II - B\": 12, \"II - C\": 13, \"II - D\": 14,\n",
    "#         \"III - A\": 15, \"III - B\": 16, \"III - C\": 17, \"III - D\": 18,\n",
    "#         \"IV - A\": 19, \"IV - B\": 20, \"IV - C\": 21, \"IV - D\": 22,\n",
    "#         \"V - A\": 23, \"V - B\": 24, \"V - C\": 25, \"V - D\": 26,\n",
    "#         \"VI - A\": 27, \"VI - B\": 28, \"VI - C\": 29, \"VI - D\": 30,\n",
    "#         \"VII - A\": 31, \"VII - B\": 32, \"VII - C\": 33, \"VII - D\": 34,\n",
    "#         \"VIII - A\": 35, \"VIII - B\": 36, \"VIII - C\": 37, \"VIII - D\": 38,\n",
    "#         \"IX - A\": 39, \"IX - B\": 40, \"IX - C\": 41,\n",
    "#         \"X - A\": 42, \"X - B\": 43, \"X - C\": 44\n",
    "#     }\n",
    "\n",
    "#     def get_class_no_2024(adm_no):\n",
    "#         class_name = lookup_map_2024.get(str(adm_no), \"\")\n",
    "#         return class_mapping.get(class_name, np.nan)\n",
    "\n",
    "#     def get_class_no_2025(adm_no):\n",
    "#         class_name = lookup_map_2025.get(str(adm_no), \"\")\n",
    "#         return class_mapping.get(class_name, np.nan)\n",
    "\n",
    "#     df_2024 = df_unpivot[df_unpivot[\"academic_year_id\"] == 1].copy()\n",
    "#     df_2025 = df_unpivot[df_unpivot[\"academic_year_id\"] == 2].copy()\n",
    "\n",
    "#     valid_adm_nos_2025 = set(lookup_map_2025.keys())\n",
    "#     df_2025 = df_2025[df_2025[\"AdmissionNo\"].astype(str).isin(valid_adm_nos_2025)].copy()\n",
    "\n",
    "#     df_2024[\"ClassNo\"] = df_2024[\"AdmissionNo\"].apply(get_class_no_2024)\n",
    "#     df_2025[\"ClassNo\"] = df_2025[\"AdmissionNo\"].apply(get_class_no_2025)\n",
    "\n",
    "#     df_unpivot = pd.concat([df_2024, df_2025], ignore_index=True)\n",
    "#     df_unpivot[\"ClassNo\"] = df_unpivot[\"ClassNo\"].fillna(0).astype(int)\n",
    "\n",
    "#     # * Step 11: Grade level (classId)\n",
    "#     grade_mapping = [\n",
    "#         (\"Pre KG\", 1), (\"LKG\", 2), (\"UKG\", 3),\n",
    "#         (\"I\", 4), (\"II\", 5), (\"III\", 6), (\"IV\", 7), (\"V\", 8),\n",
    "#         (\"VI\", 9), (\"VII\", 10), (\"VIII\", 11), (\"IX\", 12), (\"X\", 13)\n",
    "#     ]\n",
    "#     conditions = [df_unpivot['Class'].str.contains(fr\"\\b{k}\\b\", na=False, regex=True) for k, _ in grade_mapping]\n",
    "#     choices = [v for _, v in grade_mapping]\n",
    "#     df_unpivot['classId'] = np.select(conditions, choices, default=0).astype(int)\n",
    "\n",
    "#     # * Step 12: AttendanceStatusId\n",
    "#     AttendanceStatus_mapping = [(\"Absent\", 1), (\"Present\", 2), (\"Not Joined\", 3), (\"Holiday\", 4)]\n",
    "#     conditions = [df_unpivot['AttendanceStatus'].str.contains(k, na=False) for k, _ in AttendanceStatus_mapping]\n",
    "#     choices = [v for _, v in AttendanceStatus_mapping]\n",
    "#     df_unpivot['AttendanceStatusId'] = np.select(conditions, choices, default=0).astype(int)\n",
    "\n",
    "#     # * Step 13: BranchId\n",
    "#     branch_mapping = [\n",
    "#         ('Pre KG', 1), ('LKG', 1), ('UKG', 1),\n",
    "#         ('I', 2), ('II', 2), ('III', 2), ('IV', 2), ('V', 2),\n",
    "#         ('VI', 3), ('VII', 3), ('VIII', 3), ('IX', 3), ('X', 3)\n",
    "#     ]\n",
    "#     conditions = [df_unpivot['Class'].str.contains(fr\"\\b{k}\\b\", na=False, regex=True) for k, _ in branch_mapping]\n",
    "#     choices = [v for _, v in branch_mapping]\n",
    "#     df_unpivot['branchId'] = np.select(conditions, choices, default=0).astype(int)\n",
    "\n",
    "#     # ‚úÖ Final output\n",
    "#     df_unpivot = df_unpivot[[\n",
    "#         \"id\", \"Date\", \"AdmissionNo\", \"ClassNo\", \"classId\", \"branchId\", \"AttendanceStatusId\", \"academic_year_id\"\n",
    "#     ]]\n",
    "#     df_unpivot.columns = [c.lower() for c in df_unpivot.columns]\n",
    "\n",
    "#     print(f\"‚úÖ Processed data with {len(df_unpivot)} rows.\")\n",
    "#     print(f\"‚úÖ Columns are:\\n {df_unpivot.columns}\")\n",
    "#     return df_unpivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 5: Insert Data into PostgreSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import text\n",
    "\n",
    "# def ensure_table_exists():\n",
    "#     create_table_sql = f\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         date DATE,\n",
    "#         admissionno TEXT,\n",
    "#         classno INTEGER,\n",
    "#         classid INTEGER,\n",
    "#         branchid INTEGER,\n",
    "#         attendancestatusid INTEGER,\n",
    "#         academic_year_id INTEGER\n",
    "#     );\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         with engine.begin() as connection:  # ‚úÖ ensures DDL is committed\n",
    "#             connection.execute(text(create_table_sql))\n",
    "#         print(f\"‚úÖ Table '{TABLE_NAME}' ensured.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to create or check table '{TABLE_NAME}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create database engine\n",
    "# password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "# engine = create_engine(\n",
    "#     f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "#     f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "# )\n",
    "\n",
    "# def update_database(df):\n",
    "#     \"\"\"Use PostgreSQL COPY for ultra-fast data insertion.\"\"\"\n",
    "#     csv_path = (r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.csv\")\n",
    "\n",
    "#     # ‚úÖ Ensure column names are lowercase to match table definition\n",
    "#     df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "#     # ‚úÖ Save DataFrame to CSV\n",
    "#     df.to_csv(csv_path, index=False, header=False)\n",
    "\n",
    "#     try:\n",
    "#         conn = engine.raw_connection()\n",
    "#         cursor = conn.cursor()\n",
    "\n",
    "#         print(f\"üîÑ Truncating table: {TABLE_NAME}\")\n",
    "#         cursor.execute(f\"TRUNCATE TABLE {TABLE_NAME};\")\n",
    "#         conn.commit()\n",
    "\n",
    "#         with open(csv_path, \"r\") as f:\n",
    "#             cursor.copy_from(f, TABLE_NAME, sep=\",\")  # ‚úÖ lowercase and unquoted\n",
    "\n",
    "#         conn.commit()\n",
    "#         cursor.close()\n",
    "#         conn.close()\n",
    "\n",
    "#         print(f\"‚úÖ Data copied to '{TABLE_NAME}' using COPY command!\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå COPY failed: {e}\")\n",
    "#         logging.error(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 6: Run the Full Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # üìå Already-clean 2024-25 data\n",
    "#     file_2024_25 = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report_2024_25.csv\"\n",
    "\n",
    "#     # üìå Raw 2025-26 files\n",
    "#     file4 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jun_Jul.csv\"\n",
    "#     file5 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jul_Aug.csv\"\n",
    "\n",
    "#     output_file = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.csv\"\n",
    "\n",
    "#     try:\n",
    "#         print(\"üìÇ Loading already-clean 2024-25 data...\\n\")\n",
    "#         df_2024 = pd.read_csv(file_2024_25)\n",
    "#         print(f\"‚úÖ 2024-25 data loaded with {df_2024.shape[0]} rows.\")\n",
    "\n",
    "#         print(\"\\nüõ† Cleaning and loading 2025-26 raw files...\\n\")\n",
    "#         df_2025 = load_and_clean_data(file4, file5)  # No need to pass None for unused files\n",
    "#         print(f\"‚úÖ 2025-26 data cleaned with {df_2025.shape[0]} rows.\")\n",
    "\n",
    "#         # üìå Combine datasets\n",
    "#         df = pd.concat([df_2024, df_2025], ignore_index=True)\n",
    "#         print(f\"\\nüîÑ Combined dataset has {df.shape[0]} rows.\\n\")\n",
    "\n",
    "#         # üìå Process attendance data\n",
    "#         print(\"‚öôÔ∏è Processing attendance data...\\n\")\n",
    "#         df_unpivot = process_attendance_data(df)\n",
    "#         df_unpivot.to_csv(output_file, index=False)\n",
    "\n",
    "#         print(f\"‚úÖ Processed data saved with {df_unpivot.shape[0]} rows.\\n\")\n",
    "#         print(\"‚úÖ Columns are:\\n\", df_unpivot.columns)\n",
    "#         if not df_unpivot.empty:\n",
    "#             print(\"üìÖ Max date in dataset:\", df_unpivot[\"date\"].max())\n",
    "#         print(df_unpivot.head())\n",
    "\n",
    "#         # üìå Update database\n",
    "#         print(\"\\nüíæ Updating database...\\n\")\n",
    "#         ensure_table_exists()\n",
    "#         update_database(df_unpivot)\n",
    "\n",
    "#         print(\"\\nüéØ Attendance report processing completed successfully!\")\n",
    "#         print(f\"üìä Final row count: {df_unpivot.shape[0]}\\n\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå An unexpected error occurred: {e}\\n\")\n",
    "#         logging.error(f\"‚ùå Unexpected error: {e}\\n\")\n",
    "\n",
    "\n",
    "# # Run script\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>Class Table</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, text\n",
    "\n",
    "# POSTGRES_CREDENTIALS = {\n",
    "#     \"username\": \"postgres\",\n",
    "#     \"password\": \"Hari@123\",\n",
    "#     \"host\": \"localhost\",\n",
    "#     \"port\": \"5432\",\n",
    "#     \"database\": \"kotakschooldb\",\n",
    "# }\n",
    "# TABLE_NAME = \"class_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\class_section_grade_table.csv\")\n",
    "# # df[\"ClassNo\"] = df[\"ClassNo\"].astype(int)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import traceback\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import urllib\n",
    "# import io\n",
    "# from sqlalchemy import create_engine, text\n",
    "# from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# # Retry settings\n",
    "# MAX_RETRIES = 3\n",
    "# RETRY_DELAY = 5  # Seconds\n",
    "\n",
    "# def bulk_insert_postgres(df, conn, table_name):\n",
    "#     \"\"\"Fast bulk insert using PostgreSQL COPY command.\"\"\"\n",
    "#     with conn.connection.cursor() as cur:\n",
    "#         output = io.StringIO()\n",
    "#         df.to_csv(output, sep=\"\\t\", index=False, header=False)\n",
    "#         output.seek(0)\n",
    "#         cur.copy_from(output, table_name, sep=\"\\t\", null=\"NULL\")\n",
    "#         conn.connection.commit()\n",
    "\n",
    "# def update_database(df):\n",
    "#     \"\"\"Insert attendance data into PostgreSQL database with retry logic.\"\"\"\n",
    "#     password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "#     engine = create_engine(f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "#                            f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\")\n",
    "\n",
    "#     for attempt in range(1, MAX_RETRIES + 1):\n",
    "#         try:\n",
    "#             print(f\"üîÑ Attempt {attempt}: Connecting to database {POSTGRES_CREDENTIALS['database']} at {POSTGRES_CREDENTIALS['host']}...\")\n",
    "#             with engine.begin() as conn:\n",
    "#                 print(f\"‚úÖ Connection established.\")\n",
    "\n",
    "#                 # Create Table if it does not exist\n",
    "#                 print(f\"Checking if table '{TABLE_NAME}' exists...\")\n",
    "                \n",
    "#                 # Truncate the table before inserting data\n",
    "#                 print(f\"Truncating existing table: {TABLE_NAME}\")\n",
    "#                 conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} CASCADE;\"))\n",
    "                \n",
    "#                 print(f\"Deleting data from {TABLE_NAME} table...\")\n",
    "#                 conn.execute(text(f\"DELETE FROM {TABLE_NAME};\"))\n",
    "\n",
    "\n",
    "#                 # Fast Bulk Insert\n",
    "#                 print(f\"Inserting data into {TABLE_NAME} table...\")\n",
    "#                 bulk_insert_postgres(df, conn, TABLE_NAME)\n",
    "\n",
    "#                 print(f\"‚úÖ Data successfully inserted into '{TABLE_NAME}' table.\")\n",
    "#                 return  # Exit function if successful\n",
    "\n",
    "#         except OperationalError as e:\n",
    "#             print(f\"‚ùå OperationalError: {e}\")\n",
    "#             logging.error(f\"‚ùå OperationalError: {e}\")\n",
    "#             logging.error(\"Error Traceback:\\n\" + traceback.format_exc())\n",
    "\n",
    "#             if attempt < MAX_RETRIES:\n",
    "#                 print(f\"üîÑ Retrying in {RETRY_DELAY} seconds...\")\n",
    "#                 time.sleep(RETRY_DELAY)\n",
    "#             else:\n",
    "#                 print(\"‚ùå Max retries reached. Could not update the database.\")\n",
    "#                 logging.error(\"‚ùå Max retries reached. Could not update the database.\")\n",
    "#                 return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_database(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE COLLECTION REPORT 2024-25</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Logging\n",
    "logging.basicConfig(filename=\"fee_collection_merge.log\", level=logging.ERROR)\n",
    "\n",
    "# üîê Credentials & URLs\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "credentials = {\"uname\": \"harikiran\", \"psw\": \"812551\"}\n",
    "urls = {\n",
    "    \"2024_25\": \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_consolidate_report_print?&from=2025-04-01&academic_years_id=1&status=1&imageField=Search\",\n",
    "    \"2025_26\": \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_consolidate_report_print?&from=2024-04-01&academic_years_id=7&status=1&imageField=Search\"\n",
    "}\n",
    "\n",
    "# üõ†Ô∏è PostgreSQL Config\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "TABLE_NAME = \"fees_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîå Create Engine\n",
    "def get_engine():\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    return create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "# üîë Login\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    response = session.post(login_url, data=credentials)\n",
    "    if \"Invalid\" in response.text:\n",
    "        print(\"‚ùå Login failed!\")\n",
    "        return None\n",
    "    print(\"‚úÖ Login successful!\")\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Convert HTML table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
    "    rows = [[td.get_text(strip=True) for td in tr.find_all(\"td\")] for tr in table.find_all(\"tr\")[1:]]\n",
    "    return pd.DataFrame(rows, columns=headers) if rows else None\n",
    "\n",
    "# üì• Fetch fee table from a given URL\n",
    "def fetch_fee_table(session, url):\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\", class_=\"b-t\")\n",
    "    all_data = []\n",
    "\n",
    "    for table in tables:\n",
    "        df = table_to_dataframe(table)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, academic_year):\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.startswith(\"Total\", na=False)].copy()\n",
    "    df[\"Admin No.\"] = df[\"Admin No.\"].astype(str)\n",
    "\n",
    "    if academic_year == \"2025_26\":\n",
    "        df.columns = ['SNo', 'AdmissionNo', 'Name', 'Abacus1', 'TermFee1', 'TermFee2',\n",
    "                      'Total_Fees', 'Abacus2', 'TermFee3', 'TermFee4',\n",
    "                      'Total_Fee_Paid', 'Discount_Concession', 'Total_Due']\n",
    "        df = df.drop(columns=[\"SNo\", \"Abacus1\", \"Abacus2\", \"TermFee1\", \"TermFee2\", \"TermFee3\", \"TermFee4\"])\n",
    "\n",
    "    elif academic_year == \"2024_25\":\n",
    "        df.columns = ['SNo', 'AdmissionNo', 'Name', 'Abacus1', 'TermFee1',\n",
    "                      'Total_Fees', 'Abacus2', 'TermFee2',\n",
    "                      'Total_Fee_Paid', 'Discount_Concession', 'Total_Due']\n",
    "        df = df.drop(columns=[\"SNo\", \"Abacus1\", \"Abacus2\", \"TermFee1\", \"TermFee2\"])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown academic year structure: {academic_year}\")\n",
    "\n",
    "    # Convert numeric columns safely\n",
    "    numeric_columns = [\"Total_Fees\", \"Total_Fee_Paid\", \"Discount_Concession\", \"Total_Due\"]\n",
    "    for col in numeric_columns:\n",
    "        df[col] = (\n",
    "            df[col].astype(str)\n",
    "            .str.replace(\",\", \"\", regex=False)\n",
    "            .replace([\"\", \"None\", \"nan\", \"NaN\", np.nan], 0)\n",
    "        )\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    df[\"academic_year_id\"] = 1 if academic_year ==\"2024_25\" else 2\n",
    "\n",
    "    # df = df[~((df[\"AdmissionNo\"].str.extract(r\"(\\d+)\").astype(int) > 17164) & (df[\"academic_year_id\"] == 1))].copy()\n",
    "\n",
    "    df.to_csv(f\"D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fees_collection.csv\", index=False)\n",
    "    df = df.drop(columns=[\"Name\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "def ensure_fees_collection_table(engine):\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fees_collection (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        admissionno TEXT,\n",
    "        total_fee_paid INTEGER,\n",
    "        academic_year_id INTEGER NOT NULL,\n",
    "        total_fees INTEGER default 0,\n",
    "        discount_concession INTEGER default 0,\n",
    "        total_due INTEGER default 0\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_sql))\n",
    "        print(\"‚úÖ Table 'fees_collection' ensured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ¢Ô∏è Insert into PostgreSQL\n",
    "def update_database(df, table_name):\n",
    "    engine = get_engine()\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            print(f\"‚ö†Ô∏è Deleting old records from '{table_name}'...\")\n",
    "            conn.execute(text(f\"DELETE FROM {table_name};\"))\n",
    "            print(f\"‚úÖ Table '{table_name}' cleared.\")\n",
    "        df.columns = df.columns.str.lower()\n",
    "        print(f\"üì• Inserting {len(df)} rows...\")\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "        print(f\"‚úÖ Inserted into '{table_name}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inserting: {e}\")\n",
    "        logging.error(f\"Database insert error: {e}\")\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n",
      "\n",
      "üîÑ Fetching data for 2024_25...\n",
      "\n",
      "üîÑ Fetching data for 2025_26...\n",
      "üìÅ Saved to merged_fee_collection.csv\n",
      "‚úÖ Table 'fees_collection' ensured.\n",
      "‚úÖ Fees collection table ensured.\n",
      "‚ö†Ô∏è Deleting old records from 'fees_collection'...\n",
      "‚úÖ Table 'fees_collection' cleared.\n",
      "üì• Inserting 3240 rows...\n",
      "‚úÖ Inserted into 'fees_collection' successfully.\n",
      "‚úÖ All done! Total records: 3240\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Main Logic\n",
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for year, url in urls.items():\n",
    "        print(f\"\\nüîÑ Fetching data for {year}...\")\n",
    "        raw_df = fetch_fee_table(session, url)\n",
    "        if raw_df.empty:\n",
    "            print(f\"‚ùå No data for {year}!\")\n",
    "            continue\n",
    "        clean_df = clean_data(raw_df, academic_year=year)\n",
    "        merged_df = pd.concat([merged_df, clean_df], ignore_index=True)\n",
    "\n",
    "    if merged_df.empty:\n",
    "        print(\"‚ùå No data collected from any year!\")\n",
    "        return\n",
    "\n",
    "    # Save CSV (optional)\n",
    "    merged_df.to_csv(\"merged_fee_collection.csv\", index=False)\n",
    "    print(\"üìÅ Saved to merged_fee_collection.csv\")\n",
    "\n",
    "    # Ensure table exists\n",
    "    engine = get_engine()\n",
    "    ensure_fees_collection_table(engine)\n",
    "    print(\"‚úÖ Fees collection table ensured.\")\n",
    "    # Push to DB\n",
    "    update_database(merged_df, TABLE_NAME)\n",
    "    print(f\"‚úÖ All done! Total records: {len(merged_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE CONCESSION REPORT</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import logging\n",
    "from datetime import date\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ------------------ Configuration ------------------\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "data_url_2024_25 = \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_discounts_report_receipt_wise_print?&academic_years_id=1\"\n",
    "data_url_2025_26 = \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_discounts_report_receipt_wise_print?&academic_years_id=7\"\n",
    "\n",
    "credentials = {\n",
    "    \"uname\": \"harikiran\",\n",
    "    \"psw\": \"812551\"\n",
    "}\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"fee_concession_report\"\n",
    "OUTPUT_PATH = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fee_concession_report.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Login Function ------------------\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    login_response = session.post(login_url, data=credentials)\n",
    "\n",
    "    if login_response.status_code != 200:\n",
    "        print(\"‚ùå Login request failed! Server error.\\n\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(login_response.text, \"html.parser\")\n",
    "    if soup.find(\"div\", class_=\"alert-danger\"):\n",
    "        print(\"‚ùå Login failed! Check credentials.\\n\")\n",
    "        return None\n",
    "\n",
    "    print(\"‚úÖ Login successful!\\n\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Fetch Table Data ------------------\n",
    "def fetch_all_concession_tables(session, data_url):\n",
    "    response = session.get(data_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\", class_=\"table_view\")\n",
    "    if not tables:\n",
    "        print(\"‚ùå No fee tables found! The page structure may have changed.\")\n",
    "        return None\n",
    "\n",
    "    all_data = []\n",
    "    for table in tables:\n",
    "        df = table_to_dataframe(table)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"‚ùå No data extracted from tables.\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ HTML Table to DataFrame ------------------\n",
    "def table_to_dataframe(table):\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
    "    if len(headers) > 8:\n",
    "        headers = headers[:8]\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        if len(cells) >= 8:\n",
    "            rows.append(cells[:8])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=headers) if rows else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Clean DataFrame ------------------\n",
    "def clean_data(df):\n",
    "    df.columns = df.columns.str.strip().str.replace(\" \", \"_\").str.lower()\n",
    "    df = df.dropna(subset=[\"student_number\"])\n",
    "    df[\"student_number\"] = df[\"student_number\"].astype(str).str.strip()\n",
    "    df[\"discount_given\"] = pd.to_numeric(df[\"discount_given\"], errors=\"coerce\").fillna(0.00)\n",
    "    df.drop(columns=['receipt_no', 'fee_name', 'fee_amount', 'total_due_amount'], errors=\"ignore\", inplace=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"].astype(str).str.strip(), errors=\"coerce\").dt.date\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "    df[\"id\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # Ensure academic_year is kept if present\n",
    "    cols = ['id', 'date', 'student_number', 'student_name', 'discount_given']\n",
    "    if \"academic_year\" in df.columns:\n",
    "        cols.append(\"academic_year\")\n",
    "\n",
    "    df = df[cols]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df['academic_year_id'] = df['academic_year'].apply(\n",
    "        lambda x: 1 if x == \"2024-25\" else 2 if x == \"2025-26\" else None\n",
    "    )\n",
    "    \n",
    "    df.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"‚úÖ Cleaned data saved to {OUTPUT_PATH}\\n\")\n",
    "\n",
    "    df = df.drop(columns=['student_name', \"academic_year\"], errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df: pd.DataFrame, table_name: str, postgres_credentials: dict):\n",
    "    password = urllib.parse.quote(postgres_credentials[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{postgres_credentials['username']}:{password}\"\n",
    "        f\"@{postgres_credentials['host']}:{postgres_credentials['port']}/{postgres_credentials['database']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            print(f\"üîÑ Connecting to database {postgres_credentials['database']}...\")\n",
    "\n",
    "            # ‚úÖ Create table if not exists\n",
    "            conn.execute(text(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    date DATE,\n",
    "                    student_number VARCHAR(20),\n",
    "                    discount_given NUMERIC(10, 2),\n",
    "                    academic_year_id INTEGER\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print(f\"‚úÖ Ensured '{table_name}' table exists.\")\n",
    "\n",
    "            # üîÑ Clear existing records\n",
    "            print(f\"‚ö†Ô∏è Deleting existing records from: {table_name}\")\n",
    "            conn.execute(text(f\"DELETE FROM {table_name};\"))\n",
    "            print(f\"‚úÖ Table '{table_name}' cleared.\\n\")\n",
    "\n",
    "        # üì• Insert Data\n",
    "        print(f\"üì• Inserting data into {table_name} table...\")\n",
    "        df.to_sql(name=table_name, con=engine, if_exists=\"append\", index=False, method=\"multi\", chunksize=1000)\n",
    "        print(f\"‚úÖ Data successfully inserted into '{table_name}' table.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error updating database: {e}\", exc_info=True)\n",
    "        print(f\"‚ùå Error occurred while updating database: {e}\")\n",
    "\n",
    "    finally:\n",
    "        engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    df_2024_25 = fetch_all_concession_tables(session, data_url_2024_25)\n",
    "    df_2025_26 = fetch_all_concession_tables(session, data_url_2025_26)\n",
    "\n",
    "    if df_2024_25 is None or df_2025_26 is None:\n",
    "        print(\"‚ùå Could not fetch data for one or both academic years.\")\n",
    "        return\n",
    "\n",
    "    df_2024_25[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025_26[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    merged_df = pd.concat([df_2024_25, df_2025_26], ignore_index=True)\n",
    "\n",
    "    print(\"‚úÖ Data extracted successfully! Cleaning data...\\n\")\n",
    "    cleaned_df = clean_data(merged_df)\n",
    "\n",
    "    output_file = r\"D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fee_concession_report_combined.csv\"\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    print(cleaned_df.columns)\n",
    "    print(f\"‚úÖ Data saved to '{output_file}'\\n\")\n",
    "\n",
    "    update_database(cleaned_df, TABLE_NAME, POSTGRES_CREDENTIALS)\n",
    "    print(f\"‚úÖ {len(cleaned_df)} records entered into the database\")\n",
    "\n",
    "    print(cleaned_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n",
      "\n",
      "‚úÖ Data extracted successfully! Cleaning data...\n",
      "\n",
      "‚úÖ Cleaned data saved to D:\\GITHUB\\kotak-school-dbms\\output_data\\fee_concession_report.csv\n",
      "\n",
      "Index(['id', 'date', 'student_number', 'discount_given', 'academic_year_id'], dtype='object')\n",
      "‚úÖ Data saved to 'D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fee_concession_report_combined.csv'\n",
      "\n",
      "üîÑ Connecting to database kotakschooldb...\n",
      "‚úÖ Ensured 'fee_concession_report' table exists.\n",
      "‚ö†Ô∏è Deleting existing records from: fee_concession_report\n",
      "‚úÖ Table 'fee_concession_report' cleared.\n",
      "\n",
      "üì• Inserting data into fee_concession_report table...\n",
      "‚úÖ Data successfully inserted into 'fee_concession_report' table.\n",
      "\n",
      "‚úÖ 234 records entered into the database\n",
      "      id        date student_number  discount_given  academic_year_id\n",
      "0      1  2024-07-27          15660           500.0                 1\n",
      "1      2  2024-07-27          16070           500.0                 1\n",
      "2      3  2024-07-27          16105           500.0                 1\n",
      "3      4  2024-07-27          16165           500.0                 1\n",
      "4      5  2024-07-27          15018           500.0                 1\n",
      "5      6  2024-07-27          15363           500.0                 1\n",
      "6      7  2024-07-27          16921           500.0                 1\n",
      "7      8  2024-07-27          16970           500.0                 1\n",
      "8      9  2024-07-27          16985           500.0                 1\n",
      "9     10  2024-07-27          16779           500.0                 1\n",
      "10    11  2024-07-27          16967           500.0                 1\n",
      "11    12  2024-07-27          15213           500.0                 1\n",
      "12    13  2024-07-27          15532           500.0                 1\n",
      "13    14  2024-07-27          15526           500.0                 1\n",
      "14    15  2024-07-27          16765           500.0                 1\n",
      "15    16  2024-07-27          15261           500.0                 1\n",
      "16    17  2024-07-27          16499           500.0                 1\n",
      "17    18  2024-07-27          16416           500.0                 1\n",
      "18    19  2024-07-27          15278           500.0                 1\n",
      "19    20  2024-07-27          17014           500.0                 1\n",
      "20    21  2024-07-27          14861           500.0                 1\n",
      "21    22  2024-07-27          15277           500.0                 1\n",
      "22    23  2024-07-27          14502           500.0                 1\n",
      "23    24  2024-07-27          16132           500.0                 1\n",
      "24    25  2024-07-27          15275           500.0                 1\n",
      "25    26  2024-07-27          14606           500.0                 1\n",
      "26    27  2024-07-27          15402           500.0                 1\n",
      "27    28  2024-07-29          16587           500.0                 1\n",
      "28    29  2024-07-29          16393           400.0                 1\n",
      "29    30  2024-07-29          16393           500.0                 1\n",
      "30    31  2024-07-29          16340           500.0                 1\n",
      "31    32  2024-07-29          17091           500.0                 1\n",
      "32    33  2024-07-29          16441           500.0                 1\n",
      "33    34  2024-07-29          16500           500.0                 1\n",
      "34    35  2024-07-29          17060           500.0                 1\n",
      "35    36  2024-07-29          15711           500.0                 1\n",
      "36    37  2024-07-30          16192           500.0                 1\n",
      "37    38  2024-07-31          16324           500.0                 1\n",
      "38    39  2024-08-01          15748           500.0                 1\n",
      "39    40  2024-08-02          14606         20900.0                 1\n",
      "40    41  2024-08-13          14881           500.0                 1\n",
      "41    42  2024-08-19          15347         15400.0                 1\n",
      "42    43  2024-08-19          14615         19800.0                 1\n",
      "43    44  2024-09-14          16228         20900.0                 1\n",
      "44    45  2024-09-28          14321         20900.0                 1\n",
      "45    46  2024-10-18          15744         14850.0                 1\n",
      "46    47  2024-10-29          14747         19800.0                 1\n",
      "47    48  2024-12-23          15402         20400.0                 1\n",
      "48    49  2024-12-24          15402           500.0                 1\n",
      "49    50  2025-02-04          16183         20900.0                 1\n",
      "50    51  2025-02-04          13917         20900.0                 1\n",
      "51    52  2025-02-04          14406         20900.0                 1\n",
      "52    53  2025-02-09          17069          7700.0                 1\n",
      "53    54  2025-02-09          17070          7975.0                 1\n",
      "54    55  2025-02-09          16405         14300.0                 1\n",
      "55    56  2025-02-09          15274          2500.0                 1\n",
      "56    57  2025-02-09          15546          2500.0                 1\n",
      "57    58  2025-02-09          16184          7425.0                 1\n",
      "58    59  2025-02-19          16600         14300.0                 1\n",
      "59    60  2025-02-19          15980          7425.0                 1\n",
      "60    61  2025-02-19          16975         14850.0                 1\n",
      "61    62  2025-02-20          15602          7700.0                 1\n",
      "62    63  2025-02-20          16564          7700.0                 1\n",
      "63    64  2025-02-20          14932          7975.0                 1\n",
      "64    65  2025-02-20          15325          5000.0                 1\n",
      "65    66  2025-02-21          16163          2500.0                 1\n",
      "66    67  2025-02-21          15526          7700.0                 1\n",
      "67    68  2025-02-21          15612          4000.0                 1\n",
      "68    69  2025-02-21          15261          7700.0                 1\n",
      "69    70  2025-02-21          14254          4000.0                 1\n",
      "70    71  2025-03-03          14850          3000.0                 1\n",
      "71    72  2025-03-04          14606           500.0                 1\n",
      "72    73  2025-03-08          14000         10450.0                 1\n",
      "73    74  2025-03-08          14931          2500.0                 1\n",
      "74    75  2025-03-08          15775          2500.0                 1\n",
      "75    76  2025-03-08          16917          6050.0                 1\n",
      "76    77  2025-03-08          16347          7150.0                 1\n",
      "77    78  2025-03-08          14868          3000.0                 1\n",
      "78    79  2025-03-08          15869          7700.0                 1\n",
      "79    80  2025-03-08          15939         10450.0                 1\n",
      "80    81  2025-03-08          17020          7150.0                 1\n",
      "81    82  2025-03-08          13845          6000.0                 1\n",
      "82    83  2025-03-08          14228          8000.0                 1\n",
      "83    84  2025-03-08          16374          5000.0                 1\n",
      "84    85  2025-03-08          15648          5000.0                 1\n",
      "85    86  2025-03-08          14108          3000.0                 1\n",
      "86    87  2025-03-19          16143          7425.0                 1\n",
      "87    88  2025-03-19          15788          5000.0                 1\n",
      "88    89  2025-03-19          15511          7700.0                 1\n",
      "89    90  2025-03-19          16200          7150.0                 1\n",
      "90    91  2025-03-19          16024          3000.0                 1\n",
      "91    92  2025-03-19          15231          3000.0                 1\n",
      "92    93  2025-03-19          16637          6050.0                 1\n",
      "93    94  2025-03-19          15945             0.0                 1\n",
      "94    95  2025-03-19          15945         15400.0                 1\n",
      "95    96  2025-03-19          14224         10450.0                 1\n",
      "96    97  2025-03-19          16026          3000.0                 1\n",
      "97    98  2025-03-19          15880          7700.0                 1\n",
      "98    99  2025-03-20          15479         15400.0                 1\n",
      "99   100  2025-03-20          16984         12100.0                 1\n",
      "100  101  2025-03-21          15780          3000.0                 1\n",
      "101  102  2025-03-22          17052          7150.0                 1\n",
      "102  103  2025-03-22          15923          7425.0                 1\n",
      "103  104  2025-03-22          15301          7975.0                 1\n",
      "104  105  2025-04-05          13834         20900.0                 1\n",
      "105  106  2025-04-05          17126          6050.0                 1\n",
      "106  107  2025-04-05          17125          7150.0                 1\n",
      "107  108  2025-04-05          15158          7975.0                 1\n",
      "108  109  2025-04-05          16783          4000.0                 1\n",
      "109  110  2025-04-05          16782          4000.0                 1\n",
      "110  111  2025-04-05          14488         10450.0                 1\n",
      "111  112  2025-04-05          15746          7425.0                 1\n",
      "112  113  2025-04-05          15746          7425.0                 1\n",
      "113  114  2025-04-05          15625          7425.0                 1\n",
      "114  115  2025-04-05          16969          6050.0                 1\n",
      "115  116  2025-04-05          14099         10450.0                 1\n",
      "116  117  2025-04-05          15631          7700.0                 1\n",
      "117  118  2025-04-05          16317          7425.0                 1\n",
      "118  119  2025-04-30          16654          6050.0                 1\n",
      "119  120  2025-04-30          14517          9900.0                 1\n",
      "120  121  2025-04-30          14867         15950.0                 1\n",
      "121  122  2025-04-30          14907         15950.0                 1\n",
      "122  123  2025-04-30          15370          4000.0                 1\n",
      "123  124  2025-04-30          15554          5000.0                 1\n",
      "124  125  2025-04-30          15185          4000.0                 1\n",
      "125  126  2025-04-30          17050          4000.0                 1\n",
      "126  127  2025-04-30          16179          5000.0                 1\n",
      "127  128  2025-04-30          15400          7975.0                 1\n",
      "128  129  2025-04-30          16263          7425.0                 1\n",
      "129  130  2025-04-30          16762         14300.0                 1\n",
      "130  131  2025-04-30          15621         20900.0                 1\n",
      "131  132  2025-04-30          14546          9900.0                 1\n",
      "132  133  2025-04-30          14669          7975.0                 1\n",
      "133  134  2025-04-30          14958          7975.0                 1\n",
      "134  135  2025-04-30          14543          4000.0                 1\n",
      "135  136  2025-04-30          16193          7425.0                 1\n",
      "136  137  2025-04-30          14630         19800.0                 1\n",
      "137  138  2025-04-30          14243         20900.0                 1\n",
      "138  139  2025-04-30          15669          5000.0                 1\n",
      "139  140  2025-04-30          16076          4000.0                 1\n",
      "140  141  2025-04-30          16414          3000.0                 1\n",
      "141  142  2025-04-30          15850          3000.0                 1\n",
      "142  143  2025-04-30          15613          3000.0                 1\n",
      "143  144  2025-04-30          14944          5000.0                 1\n",
      "144  145  2025-04-30          14863          3500.0                 1\n",
      "145  146  2025-04-30          13863          4000.0                 1\n",
      "146  147  2025-04-30          15520          3000.0                 1\n",
      "147  148  2025-04-30          15584          3000.0                 1\n",
      "148  149  2025-04-30          14953          3000.0                 1\n",
      "149  150  2025-04-30          16077          9900.0                 1\n",
      "150  151  2025-04-30          16078         10450.0                 1\n",
      "151  152  2025-05-03          14879         15950.0                 1\n",
      "152  153  2025-05-03          15459          7700.0                 1\n",
      "153  154  2025-05-03          15459          7700.0                 1\n",
      "154  155  2025-08-05          14497           500.0                 1\n",
      "155  156  2025-08-05          15019          1000.0                 1\n",
      "156  157  2025-08-05          16886          3000.0                 1\n",
      "157  158  2025-08-05          16887          3500.0                 1\n",
      "158  159  2025-08-05          16888          3000.0                 1\n",
      "159  160  2025-08-05          16926         12100.0                 1\n",
      "160  161  2025-08-05          16836         12100.0                 1\n",
      "161  162  2025-08-05          16360         14300.0                 1\n",
      "162  163  2025-08-05          14947         15950.0                 1\n",
      "163  164  2025-08-05          14977         15950.0                 1\n",
      "164  165  2025-08-05          16540         14300.0                 1\n",
      "165  166  2025-08-05          13869         20900.0                 1\n",
      "166  167  2025-08-05          15840         14850.0                 1\n",
      "167  168  2025-08-05          13828             0.0                 1\n",
      "168  169  2025-08-05          13828         20900.0                 1\n",
      "169  170  2025-08-05          15932         14850.0                 1\n",
      "170  171  2025-08-05          14614         19800.0                 1\n",
      "171  172  2025-08-05          16613          8000.0                 1\n",
      "172  173  2025-08-05          16614          7425.0                 1\n",
      "173  174  2025-08-05          16614          7425.0                 1\n",
      "174  175  2025-08-05          14384         20900.0                 1\n",
      "175  176  2025-08-05          13625         10450.0                 1\n",
      "176  177  2025-08-06          13856         10450.0                 1\n",
      "177  178  2025-08-06          13822         10450.0                 1\n",
      "178  179  2025-08-06          13822         10450.0                 1\n",
      "179  180  2025-08-06          14686          6000.0                 1\n",
      "180  181  2025-08-06          15012          7500.0                 1\n",
      "181  182  2025-08-06          15011          7500.0                 1\n",
      "182  183  2025-08-06          15155          7700.0                 1\n",
      "183  184  2025-08-06          15512          7425.0                 1\n",
      "184  185  2025-08-06          15215          7700.0                 1\n",
      "185  186  2025-08-06          15915          6000.0                 1\n",
      "186  187  2025-08-06          16295          7425.0                 1\n",
      "187  188  2025-08-06          16576          7150.0                 1\n",
      "188  189  2025-08-06          16390          6000.0                 1\n",
      "189  190  2025-08-06          16507          7150.0                 1\n",
      "190  191  2025-08-07          16483         14300.0                 1\n",
      "191  192  2025-08-07          16864         14300.0                 1\n",
      "192  193  2025-07-04          16967           500.0                 2\n",
      "193  194  2025-07-04          17221           500.0                 2\n",
      "194  195  2025-07-04          17274           500.0                 2\n",
      "195  196  2025-07-04          16393           500.0                 2\n",
      "196  197  2025-07-04          17012           500.0                 2\n",
      "197  198  2025-07-04          16340           500.0                 2\n",
      "198  199  2025-07-04          17023           500.0                 2\n",
      "199  200  2025-07-04          17084           500.0                 2\n",
      "200  201  2025-07-04          16813           500.0                 2\n",
      "201  202  2025-07-04          16105           500.0                 2\n",
      "202  203  2025-07-04          17025           500.0                 2\n",
      "203  204  2025-07-04          16070           500.0                 2\n",
      "204  205  2025-07-04          15744           500.0                 2\n",
      "205  206  2025-07-04          17034           500.0                 2\n",
      "206  207  2025-07-04          16192           500.0                 2\n",
      "207  208  2025-07-04          15213           500.0                 2\n",
      "208  209  2025-07-04          15396           500.0                 2\n",
      "209  210  2025-07-04          16068           500.0                 2\n",
      "210  211  2025-07-04          14502           500.0                 2\n",
      "211  212  2025-07-04          15018           500.0                 2\n",
      "212  213  2025-07-04          15363           500.0                 2\n",
      "213  214  2025-07-04          14488           500.0                 2\n",
      "214  215  2025-07-04         14384A           500.0                 2\n",
      "215  216  2025-07-04          16921           500.0                 2\n",
      "216  217  2025-07-04          17048           500.0                 2\n",
      "217  218  2025-07-04          17322           500.0                 2\n",
      "218  219  2025-07-04          16985           500.0                 2\n",
      "219  220  2025-07-08          16587           500.0                 2\n",
      "220  221  2025-07-08          16325           500.0                 2\n",
      "221  222  2025-07-08          15324           500.0                 2\n",
      "222  223  2025-07-08          15048           500.0                 2\n",
      "223  224  2025-07-15          15831           500.0                 2\n",
      "224  225  2025-07-15          15832           500.0                 2\n",
      "225  226  2025-07-15          14321           500.0                 2\n",
      "226  227  2025-07-18          15660           500.0                 2\n",
      "227  228  2025-07-18          16500           500.0                 2\n",
      "228  229  2025-07-18          17060           500.0                 2\n",
      "229  230  2025-07-18          16499           500.0                 2\n",
      "230  231  2025-07-21          16765           500.0                 2\n",
      "231  232  2025-08-05          14497           500.0                 2\n",
      "232  233  2025-08-05          14867           500.0                 2\n",
      "233  234  2025-08-22          15453           500.0                 2\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Run Script ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
