{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>KOTAK SALESIAN SCHOOL</b></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>STUDENTS DATABASE MANAGEMENT</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import datetime\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Backup Files Before running New**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# * PostgreSQL Credentials (from .env)\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\"),\n",
    "    \"database\": os.getenv(\"DB_NAME\"),\n",
    "}\n",
    "\n",
    "# * Backup Config (from .env)\n",
    "BACKUP_DIR = os.getenv(\"BACKUP_DIR\")\n",
    "DB_DUMP_PATH = os.getenv(\"DB_DUMP_PATH\")\n",
    "\n",
    "# * Ensure the backup directory exists\n",
    "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "\n",
    "# * Generate a timestamp for the backup file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "backup_file = os.path.join(BACKUP_DIR, f\"backup_{POSTGRES_CREDENTIALS['database']}_{timestamp}.sql\")\n",
    "\n",
    "# * Run DB_dump\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            DB_DUMP_PATH,  # Use full path if not in PATH\n",
    "            \"-U\", POSTGRES_CREDENTIALS[\"username\"],\n",
    "            \"-h\", POSTGRES_CREDENTIALS[\"host\"],\n",
    "            \"-p\", POSTGRES_CREDENTIALS[\"port\"],\n",
    "            \"-F\", \"c\",\n",
    "            \"-b\",\n",
    "            \"-v\",\n",
    "            \"-f\", backup_file,\n",
    "            POSTGRES_CREDENTIALS[\"database\"],\n",
    "        ],\n",
    "        env={**os.environ, \"PGPASSWORD\": POSTGRES_CREDENTIALS[\"password\"]},  # Pass password securely\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    # * Check for errors\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Backup successful: {backup_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Backup failed!\\nError: {result.stderr}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è DB_dump not found at {DB_DUMP_PATH}. Check PostgreSQL installation or system PATH.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries & Define Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_JSON_STUDENT_PATHS = {\n",
    "    \"2024-25\": os.getenv(\"GOOGLE_JSON_STUDENT_PATH_2024_25\"),\n",
    "    \"2025-26\": os.getenv(\"GOOGLE_JSON_STUDENT_PATH_2025_26\"),\n",
    "}\n",
    "\n",
    "GOOGLE_SHEET_TITLES = {\n",
    "    \"2024-25\": os.getenv(\"GOOGLE_SHEET_TITLE_2024_25\"),\n",
    "    \"2025-26\": os.getenv(\"GOOGLE_SHEET_TITLE_2025_26\"),\n",
    "}\n",
    "\n",
    "UNIQUE_KEY = os.getenv(\"UNIQUE_KEY\")\n",
    "\n",
    "\n",
    "# * Table names\n",
    "TABLE_NAME1 = \"students\"\n",
    "TABLE_NAME2 = \"student_list\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract Data from Google Sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FETCH GOOGLE SHEET ===\n",
    "def fetch_data(sheet_title, worksheet_name=\"Overall\", json_path=None):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(sheet_title)\n",
    "    sheet = spreadsheet.worksheet(worksheet_name)\n",
    "    data = sheet.get_all_records(head=3)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === CLEAN COLUMN NAMES ===\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TC_APPLIED_SHEET_NAME = \"TC LIST\"\n",
    "\n",
    "def merge_and_tag():\n",
    "\n",
    "    # Fetch and clean data\n",
    "    df_2024 = clean_column_names(fetch_data(\n",
    "        GOOGLE_SHEET_TITLES[\"2024-25\"], \"Overall\", GOOGLE_JSON_STUDENT_PATHS[\"2024-25\"]\n",
    "    ))\n",
    "\n",
    "    df_2025 = clean_column_names(fetch_data(\n",
    "        GOOGLE_SHEET_TITLES[\"2025-26\"], \"Overall\", GOOGLE_JSON_STUDENT_PATHS[\"2025-26\"]\n",
    "    ))\n",
    "\n",
    "    # ‚úÖ TC applied sheet\n",
    "    df_tc_applied = clean_column_names(fetch_data(\n",
    "        GOOGLE_SHEET_TITLES[\"2024-25\"], TC_APPLIED_SHEET_NAME, GOOGLE_JSON_STUDENT_PATHS[\"2024-25\"]\n",
    "    ))\n",
    "\n",
    "\n",
    "    # Tag academic year\n",
    "    df_2024[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    # Ensure no NaN in unique key column\n",
    "    df_2024 = df_2024.dropna(subset=[UNIQUE_KEY])\n",
    "    df_2025 = df_2025.dropna(subset=[UNIQUE_KEY])\n",
    "\n",
    "    # Get sets of unique keys\n",
    "    codes_2024 = set(df_2024[UNIQUE_KEY])\n",
    "    codes_2025 = set(df_2025[UNIQUE_KEY])\n",
    "    tc_applied_ids = set(df_tc_applied[UNIQUE_KEY])\n",
    "\n",
    "    # Determine who left and who is new\n",
    "    left = codes_2024 - codes_2025\n",
    "    new = codes_2025 - codes_2024\n",
    "\n",
    "    # Find graduates = left students in max grade\n",
    "    max_grade = df_2024[\"GRADES\"].max()\n",
    "    graduates = set(\n",
    "        df_2024[(df_2024[\"GRADES\"] == max_grade) & (df_2024[UNIQUE_KEY].isin(left))][UNIQUE_KEY]\n",
    "    )\n",
    "\n",
    "    # üîπ Assign status_id for 2024\n",
    "    def get_status_2024(x):\n",
    "        if x in graduates:\n",
    "            return 4  # Graduated\n",
    "        elif x in tc_applied_ids:\n",
    "            return 5  # TC Applied\n",
    "        elif x in left:\n",
    "            return 2  # Not coming\n",
    "        else:\n",
    "            return 1  # Continuing\n",
    "\n",
    "    df_2024[\"status_id\"] = df_2024[UNIQUE_KEY].apply(get_status_2024)\n",
    "\n",
    "    # üîπ Assign status_id for 2025\n",
    "    df_2025[\"status_id\"] = df_2025[UNIQUE_KEY].apply(\n",
    "        lambda x: 3 if x in new else 1\n",
    "    )\n",
    "\n",
    "    # Merge and return\n",
    "    return pd.concat([df_2024, df_2025], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ‚ú® Rename columns to match your database structure\n",
    "    df.columns = [\n",
    "        \"sno\", \"adm_no\", \"name\", \"class\", \"gender\", \"mother_name\", \"father_name\",\n",
    "        \"pen_number\", \"dob\", \"phone_no\", \"religion\", \"caste\", \"sub_caste\",\n",
    "        \"second_lang\", \"remarks\", \"class_nos\", \"joined_year\", \"grade_id\",\"student_aadhar\", \"father_aadhar\", \"mother_aadhar\",\"apaar_id\",\n",
    "        \"academic_year\", \"status_id\"\n",
    "    ]\n",
    "\n",
    "    # Lowercase and strip spaces for consistency\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # üóìÔ∏è Convert DOB to PostgreSQL-friendly format\n",
    "    df[\"dob\"] = pd.to_datetime(df[\"dob\"], format=\"%d-%m-%Y\", errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # üî¢ Convert joined_year to integer\n",
    "    df[\"joined_year\"] = pd.to_numeric(df[\"joined_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # üßπ Remove optional junk column\n",
    "    if \"apaar_status\" in df.columns:\n",
    "        df.drop(columns=[\"apaar_status\"], inplace=True)\n",
    "\n",
    "    # Capitalize gender and reset S.No\n",
    "    df[\"gender\"] = df[\"gender\"].str.upper()\n",
    "    df[\"sno\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # üü¢ Clean text fields\n",
    "    df[\"adm_no\"] = df[\"adm_no\"].astype(str).str.strip()\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "    df[\"academic_year\"] = df[\"academic_year\"].astype(str).str.strip()\n",
    "    \n",
    "\n",
    "    conditions = [\n",
    "        df[\"grade_id\"].between(1, 3),   # grade_id from 1 to 3\n",
    "        df[\"grade_id\"].between(4, 8)    # grade_id from 4 to 8\n",
    "    ]\n",
    "    choices = [1, 2]  # branch_id values\n",
    "\n",
    "    df[\"branch_id\"] = np.select(conditions, choices, default=3)\n",
    "\n",
    "\n",
    "    # Sort for visual clarity\n",
    "    df = df.sort_values(by=[\"academic_year\", \"class_nos\", \"gender\", \"name\"])\n",
    "\n",
    "    # Prefer non-null mother/father names when dropping duplicates\n",
    "    df_sorted = df.sort_values(\n",
    "        by=[\"adm_no\", \"mother_name\", \"father_name\"],\n",
    "        ascending=[True, True, True],\n",
    "        na_position='last'  # Non-null values come first\n",
    "    )\n",
    "    \n",
    "    df[\"academic_year_id\"] = df[\"academic_year\"].apply(lambda x: 1 if x== \"2024-25\" else 2)\n",
    "    \n",
    "    # üßæ Save CSV for auditing\n",
    "    df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\students_data.csv\", index=False)\n",
    "\n",
    "    student_list_df = df_sorted.drop_duplicates(subset=\"adm_no\", keep=\"first\")[\n",
    "        [\n",
    "            \"adm_no\", \"name\", \"gender\", \"mother_name\", \"father_name\",\n",
    "            \"pen_number\", \"dob\", \"phone_no\", \"religion\", \"caste\",\n",
    "            \"sub_caste\", \"second_lang\", \"remarks\", \"student_aadhar\", \"father_aadhar\", \"mother_aadhar\",\"apaar_id\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    student_list_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\student_list.csv\", index=False)\n",
    "\n",
    "\n",
    "    students_df = df[\n",
    "        [\n",
    "            \"adm_no\", \"class_nos\",\n",
    "            \"grade_id\", \"academic_year_id\", \"status_id\",\"branch_id\"\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "\n",
    "    print(\"‚úÖ Cleaned and split data saved.\")\n",
    "\n",
    "    return student_list_df, students_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df, table_name):\n",
    "    import numpy as np\n",
    "    password = urllib.parse.quote_plus(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\",\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    # Table creation logic\n",
    "    table_create_sql = {\n",
    "        \"students\": \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS students (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                adm_no VARCHAR,\n",
    "                class_nos VARCHAR,\n",
    "                grade_id VARCHAR,\n",
    "                academic_year_id INT,\n",
    "                status_id INT,\n",
    "                branch_id INT\n",
    "            );\n",
    "        \"\"\",\n",
    "        \"student_list\": \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS student_list (\n",
    "                adm_no VARCHAR PRIMARY KEY,\n",
    "                name VARCHAR,\n",
    "                gender VARCHAR,\n",
    "                mother_name VARCHAR,\n",
    "                father_name VARCHAR,\n",
    "                pen_number VARCHAR,\n",
    "                dob DATE,\n",
    "                phone_no VARCHAR,\n",
    "                religion VARCHAR,\n",
    "                caste VARCHAR,\n",
    "                sub_caste VARCHAR,\n",
    "                second_lang VARCHAR,\n",
    "                remarks TEXT,\n",
    "                student_aadhar VARCHAR,\n",
    "                father_aadhar VARCHAR,\n",
    "                mother_aadhar VARCHAR,\n",
    "                apaar_id VARCHAR\n",
    "            );\n",
    "        \"\"\"\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # ‚úÖ Create table if it does not exist\n",
    "            if table_name in table_create_sql:\n",
    "                conn.execute(text(table_create_sql[table_name]))\n",
    "                print(f\"üì¶ Table '{table_name}' created if it didn't exist.\")\n",
    "\n",
    "            # üóëÔ∏è Truncate before insert\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;\"))\n",
    "            print(f\"üßπ Old records deleted from '{table_name}'.\")\n",
    "\n",
    "        df = df.replace({pd.NA: None, np.nan: None})\n",
    "        print(f\"‚è≥ Inserting data into '{table_name}'...\")\n",
    "\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=500)\n",
    "\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name};\"))\n",
    "            count = result.scalar()\n",
    "            print(f\"‚úÖ Insert complete. üìä Table '{table_name}' now contains {count} records.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating table '{table_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean Extracted Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting full student import pipeline...\\n\")\n",
    "\n",
    "    merged_df = merge_and_tag()\n",
    "    student_list_df, students_df = clean_data(merged_df)\n",
    "\n",
    "    # Update master (student_list) and academic (students) tables\n",
    "    update_database(students_df, \"students\")\n",
    "    update_database(student_list_df, \"student_list\")\n",
    "\n",
    "    print(\"üéâ All done! Both 'student_list' and 'students' tables updated successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE REPORTS</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Necessary Libraries & Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env configuration\n",
    "load_dotenv(override=True)\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": os.getenv(\"DB_USER\"),\n",
    "    \"password\": os.getenv(\"DB_PASSWORD\"),\n",
    "    \"host\": os.getenv(\"DB_HOST\"),\n",
    "    \"port\": os.getenv(\"DB_PORT\"),\n",
    "    \"database\": os.getenv(\"DB_NAME\"),\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "GOOGLE_JSON_FEE_DATA_PATHS = {\n",
    "    \"2024-25\": os.getenv(\"GOOGLE_JSON_FEE_DATA_PATH_2024_25\"),\n",
    "    \"2025-26\": os.getenv(\"GOOGLE_JSON_FEE_DATA_PATH_2025_26\"),\n",
    "}\n",
    "\n",
    "GOOGLE_JSON_FEE_PATHS = {\n",
    "    \"2024-25\": os.getenv(\"GOOGLE_JSON_FEE_PATH_2024_25\"),\n",
    "    \"2025-26\": os.getenv(\"GOOGLE_JSON_FEE_PATH_2025_26\"),\n",
    "}\n",
    "\n",
    "UNIQUE_KEY = os.getenv(\"UNIQUE_KEY\")\n",
    "\n",
    "\n",
    "TABLE_NAME = \"fees_table\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Fetching Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FETCH GOOGLE SHEET ===\n",
    "def fetch_data(sheet_title, worksheet_name=\"Overall Sheet\", json_path=None):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(sheet_title)\n",
    "    sheet = spreadsheet.worksheet(worksheet_name)\n",
    "    data = sheet.get_all_records(head=3)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === CLEAN COLUMN NAMES ===\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Merge and Tag Fees Data**\n",
    "def merge_and_tag():\n",
    "    df_2024 = clean_column_names(fetch_data(\n",
    "        GOOGLE_JSON_FEE_PATHS[\"2024-25\"], \"Overall Sheet\", GOOGLE_JSON_FEE_DATA_PATHS[\"2024-25\"]\n",
    "    ))\n",
    "\n",
    "    df_2025 = clean_column_names(fetch_data(\n",
    "        GOOGLE_JSON_FEE_PATHS[\"2025-26\"], \"Overall Sheet\", GOOGLE_JSON_FEE_DATA_PATHS[\"2025-26\"]\n",
    "    ))\n",
    "\n",
    "    df_2024[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    return pd.concat([df_2024, df_2025], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Cleaning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.iloc[:-7, :]  # Drop the last 7 rows (adjust if necessary)\n",
    "\n",
    "    df.columns = ['SNo', 'ADM_NO', 'STUDENT_NAME', 'CLASS', \"GENDER\",'FB_NO',\n",
    "                  'Term1', 'Term2', 'Term3', 'Term4', 'Total_Fee_Paid',\n",
    "                  'Discount_Concession', 'Exempted', 'Total_Fee_Due', 'PermissionUpto',\n",
    "                  'Fine', 'Payment_Status', 'ClassNo', \"AcNo\", 'Concession_type', \n",
    "                  \"staff_name\", \"academic_year\"]\n",
    "\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # üö´ Remove blank admission numbers & student names\n",
    "    df = df[df[\"adm_no\"].astype(str).str.strip() != \"\"]\n",
    "    df = df.dropna(subset=[\"adm_no\"])\n",
    "    df = df[df[\"student_name\"].astype(str).str.strip() != \"\"]\n",
    "    df = df.dropna(subset=[\"student_name\"])\n",
    "\n",
    "    # üî¢ Convert numeric columns\n",
    "    columns_to_convert = [\"term1\", \"term2\", \"term3\", \"term4\", \"total_fee_paid\",\n",
    "                          \"discount_concession\", 'exempted', \"total_fee_due\", \"fine\"]\n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    # ‚ùå Drop unused columns\n",
    "    df = df.drop(columns=[\"acno\", 'concession_type','gender'])\n",
    "\n",
    "    # üî¢ Add serial number\n",
    "    df[\"sno\"] = range(1, len(df) + 1)\n",
    "    df = df.sort_values(by=[\"sno\"])\n",
    "\n",
    "    # üí∞ Calculate total fees\n",
    "    df[\"total_fees\"] = df[\"total_fee_paid\"] + df[\"discount_concession\"] + df[\"total_fee_due\"] + df[\"exempted\"]\n",
    "\n",
    "    # üÜî Academic year mapping\n",
    "    df['academic_year_id'] = df['academic_year'].apply(lambda x: 1 if x == \"2024-25\" else 2)\n",
    "    df = df.sort_values(by=[\"academic_year_id\", \"classno\", \"student_name\"], ascending=[True, True, True])\n",
    "\n",
    "    # üìÇ Save main fees report\n",
    "    df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fees_report.csv\", index=False)\n",
    "\n",
    "    # ‚úÖ Ensure payment_status column exists\n",
    "    if \"payment_status\" not in df.columns:\n",
    "        df[\"payment_status\"] = \"Unknown\"\n",
    "\n",
    "    # üìÇ Create payment status table\n",
    "    payment_status_df = df[[\"payment_status\"]].sort_values(by=\"payment_status\").drop_duplicates().reset_index(drop=True).copy()\n",
    "    payment_status_df[\"payment_status_id\"] = range(1, len(payment_status_df) + 1)\n",
    "    payment_status_df = payment_status_df[[\"payment_status_id\", \"payment_status\"]]\n",
    "    payment_status_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\payment_status_table.csv\", index=False)\n",
    "    print(\"‚úÖ Fees Report & Payment Status Table created successfully.\\n\")\n",
    "\n",
    "    \n",
    "    # üìÇ Create staff child table\n",
    "    df[\"staff\"] = np.where(df['staff_name'].notnull() & df['staff_name'].str.strip().ne(''),1,0)    \n",
    "    \n",
    "    # ‚úÖ Extract only staff records for the child table\n",
    "    staff_child_df = df[df[\"staff\"] == 1][[\"staff_name\"]].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "    # Assign staff IDs sequentially\n",
    "    staff_child_df[\"staff_id\"] = range(1, len(staff_child_df) + 1)\n",
    "\n",
    "    # Save staff child table\n",
    "    staff_child_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\staff_child_table.csv\", index=False)\n",
    "    print(\"‚úÖ Staff Child Table created successfully.\\n\")\n",
    "\n",
    "    # --- Step 1: Merge Payment Status ---\n",
    "    if \"payment_status\" in df.columns and \"payment_status\" in payment_status_df.columns:\n",
    "        payment_status_df = payment_status_df.drop_duplicates(subset=[\"payment_status\"])\n",
    "        df = df.merge(payment_status_df, on=\"payment_status\", how=\"left\")\n",
    "\n",
    "    # --- Step 2: Merge Staff Child ---\n",
    "    # Drop duplicate merge keys to avoid _x / _y\n",
    "    merge_keys = [\"staff_name\"]\n",
    "    staff_child_clean = staff_child_df.drop(\n",
    "        columns=[col for col in staff_child_df.columns if col in df.columns and col not in merge_keys],\n",
    "        errors=\"ignore\"\n",
    "    )\n",
    "    df = df.merge(staff_child_clean, on=merge_keys, how=\"left\")\n",
    "\n",
    "    # Merge Payment Status first\n",
    "    df = pd.merge(df, payment_status_df, on=\"payment_status\", how=\"left\")\n",
    "    \n",
    "    # --- Step 3: Final Cleanup ---\n",
    "    # Drop any remaining _y columns\n",
    "    df = df.drop(columns=[c for c in df.columns if c.endswith(\"_y\")], errors=\"ignore\")\n",
    "\n",
    "    # Rename _x columns back to original\n",
    "    df.columns = [c.replace(\"_x\", \"\") for c in df.columns]\n",
    "\n",
    "    # ‚ùå Drop extra columns before DB insert\n",
    "    cols_to_drop = [\"permissionupto\", \"payment_status\", \"student_name\", \"class\", \"staff_name\", \"academic_year\"]\n",
    "    df.drop(columns=[c for c in cols_to_drop if c in df.columns], inplace=True)\n",
    "\n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Updating the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode password for URL safety\n",
    "password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "\n",
    "# Create Engine\n",
    "engine = create_engine(f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "                       f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\")\n",
    "\n",
    "def table_exists(table_name):\n",
    "    check_query = \"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_schema = 'public' AND table_name = :table_name\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(check_query), {\"table_name\": table_name}).scalar()\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_table():\n",
    "    \"\"\"Create table only if it does not exist\"\"\"\n",
    "    print(\"üîß create_table() function called.\")  # Debug print\n",
    "    table_name = \"fees_table\"\n",
    "    \n",
    "    if table_exists(table_name):\n",
    "        print(f\"‚úÖ Table '{table_name}' already exists.\")\n",
    "        return\n",
    "\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE fees_table (\n",
    "    sno SERIAL PRIMARY KEY,\n",
    "    adm_no TEXT,\n",
    "    fb_no TEXT,\n",
    "    term1 NUMERIC DEFAULT 0,\n",
    "    term2 NUMERIC DEFAULT 0,\n",
    "    term3 NUMERIC DEFAULT 0,\n",
    "    term4 NUMERIC DEFAULT 0,\n",
    "    total_fee_paid NUMERIC DEFAULT 0,\n",
    "    discount_concession NUMERIC DEFAULT 0,\n",
    "    exempted NUMERIC DEFAULT 0,\n",
    "    total_fee_due NUMERIC DEFAULT 0,\n",
    "    fine NUMERIC DEFAULT 0,\n",
    "    classno INTEGER,\n",
    "    staff INTEGER,\n",
    "    staff_id INTEGER,\n",
    "    academic_year_id INTEGER NOT NULL,\n",
    "    total_fees INTEGER DEFAULT 0,\n",
    "    payment_status_id INTEGER\n",
    ");\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_query))\n",
    "            print(f\"‚úÖ Table '{table_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df):\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # ‚úÖ Truncate existing table and reset serial ID\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} RESTART IDENTITY CASCADE;\"))\n",
    "            print(f\"‚úÖ All records from the '{TABLE_NAME}' table have been deleted.\\n\")\n",
    "\n",
    "            # ‚úÖ Add UNIQUE constraint on 'admissionno' (if it doesn't exist)\n",
    "            conn.execute(text(f\"\"\"\n",
    "                DO $$ \n",
    "                BEGIN \n",
    "                    -- Drop old constraint if exists\n",
    "                    IF EXISTS (\n",
    "                        SELECT 1 FROM information_schema.table_constraints \n",
    "                        WHERE table_name = '{TABLE_NAME}' AND constraint_name = 'unique_admissionno'\n",
    "                    ) THEN\n",
    "                        ALTER TABLE {TABLE_NAME} DROP CONSTRAINT unique_admissionno;\n",
    "                    END IF;\n",
    "\n",
    "                    -- Add new composite unique constraint if not exists\n",
    "                    IF NOT EXISTS (\n",
    "                        SELECT 1 FROM information_schema.table_constraints \n",
    "                        WHERE table_name = '{TABLE_NAME}' AND constraint_name = 'unique_adm_year'\n",
    "                    ) THEN\n",
    "                        ALTER TABLE {TABLE_NAME} ADD CONSTRAINT unique_adm_year UNIQUE (\"adm_no\", \"academic_year_id\");\n",
    "                    END IF;\n",
    "                END $$;\n",
    "            \"\"\"))\n",
    "\n",
    "            print(f\"‚úÖ Unique constraint on 'admissionno' ensured in the '{TABLE_NAME}' table.\\n\")\n",
    "\n",
    "        print(\"‚úÖ Table cleared. Proceeding with data insertion...\\n\")\n",
    "\n",
    "        # ‚úÖ Normalize column names\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # ‚úÖ Insert data in chunks\n",
    "        df.to_sql(\n",
    "            name=TABLE_NAME,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ {len(df)} records successfully inserted into '{TABLE_NAME}'.\\n\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"‚ùå An error occurred during database update: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main Execution Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # * Merge and tag both years\n",
    "    combined_df = merge_and_tag()\n",
    "    print(\"‚úÖ Raw data merged from both years.\\n\")\n",
    "\n",
    "    # * Clean and process the merged data\n",
    "    cleaned_df = clean_data(combined_df)\n",
    "    print(\"‚úÖ Data cleaned and transformed successfully.\\n\")\n",
    "    print(\"‚úÖ Final columns are:\\n\", cleaned_df.columns.to_list())\n",
    "\n",
    "    # * Create table if it does not exist\n",
    "    create_table()\n",
    "    print(\"\\n‚úÖ Table check/creation complete.\\n\")\n",
    "\n",
    "    # * Drop duplicates by adm_no + year before insert\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=[\"adm_no\", \"academic_year_id\"])\n",
    "    print(f\"‚úÖ Deduplicated. Final records to upload: {len(cleaned_df)}\\n\")\n",
    "\n",
    "    # * Upload data using safe insertion\n",
    "    update_database(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>DAY WISE REPORTS</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ============================================\n",
    "# üìò KOTAK SALESIAN SCHOOL\n",
    "# DAYWISE FEE COLLECTION DATA EXTRACTOR (FINAL CLEANED VERSION)\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "\n",
    "# --- LOGIN & TARGET URLs ---\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "urls_to_fetch = [\n",
    "    \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_reports_day_wise_receipt_wise_print?academic_years_id=1\",\n",
    "    \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_reports_day_wise_receipt_wise_print?academic_years_id=7\",\n",
    "]\n",
    "\n",
    "TABLE_NAME = \"daywise_fees_collection\"\n",
    "\n",
    "credentials = {\"uname\": \"harikiran\", \"psw\": \"812551\"}\n",
    "\n",
    "OUTPUT_DIR = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTION: Determine Academic Year ---\n",
    "def get_academic_year_from_url(url):\n",
    "    if \"academic_years_id=1\" in url:\n",
    "        return \"2024-25\"\n",
    "    elif \"academic_years_id=7\" in url:\n",
    "        return \"2025-26\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected academic_years_id in URL: {url}\")\n",
    "\n",
    "\n",
    "# --- FUNCTION: Login ---\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    session.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/118.0.5993.70 Safari/537.36\"\n",
    "    })\n",
    "    response = session.post(login_url, data=credentials, timeout=15)\n",
    "    if \"Invalid\" in response.text or response.status_code != 200:\n",
    "        print(\"‚ùå Login failed! Check credentials or site status.\\n\")\n",
    "        return None\n",
    "    print(\"‚úÖ Login successful!\\n\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTION: Fetch & Split by Account ---\n",
    "def fetch_account_sections(session, data_url):\n",
    "    response = session.get(data_url, timeout=25)\n",
    "    html = response.text\n",
    "\n",
    "    # Stop before CANCELLED RECEIPTS section\n",
    "    if \"CANCELLED RECEIPTS\" in html:\n",
    "        html = html.split(\"CANCELLED RECEIPTS\")[0]\n",
    "\n",
    "    # Split by ‚ÄúAccount Name‚Äù headings\n",
    "    sections = html.split(\"Account Name:\")\n",
    "    sections = [f\"Account Name:{s}\" for s in sections if \"S.No\" in s or \"<table\" in s]\n",
    "    print(f\"üîç Found {len(sections)} account section(s) in this page.\")\n",
    "    return sections\n",
    "\n",
    "\n",
    "# --- FUNCTION: Extract Account Info + Table ---\n",
    "def extract_account_data(section_html):\n",
    "    soup = BeautifulSoup(section_html, \"html.parser\")\n",
    "    text = soup.get_text(separator=\"\\n\")\n",
    "\n",
    "    # Account and mode extraction\n",
    "    account_name, payment_mode = None, None\n",
    "    for line in text.splitlines():\n",
    "        if \"Account Name\" in line:\n",
    "            account_name = line.split(\":\")[-1].strip()\n",
    "        if \"Payment Mode\" in line:\n",
    "            payment_mode = line.split(\":\")[-1].strip()\n",
    "\n",
    "    table = soup.find(\"table\")\n",
    "    if not table:\n",
    "        print(f\"‚ö†Ô∏è Skipping '{account_name}' (no table found).\")\n",
    "        return None, account_name, payment_mode\n",
    "\n",
    "    df = extract_data_from_table(table)\n",
    "    return df, account_name, payment_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTION: Extract Data From Table ---\n",
    "def extract_data_from_table(table):\n",
    "    try:\n",
    "        rows = []\n",
    "        header_row = table.find(\"tr\")\n",
    "        if not header_row:\n",
    "            return None\n",
    "\n",
    "        headers = [th.get_text(strip=True) for th in header_row.find_all(\"th\")]\n",
    "        if not headers:\n",
    "            headers = [\n",
    "                \"S.No\", \"Receipt No.\", \"Class/Sec\", \"Student Number\",\n",
    "                \"Student Name\", \"Date Added\", \"-\", \"Abacus / Vedic Maths\",\n",
    "                \"TERM FEE 1\", \"TERM FEE 2\", \"Total Received Amount\"\n",
    "            ]\n",
    "\n",
    "        for tr in table.find_all(\"tr\")[1:]:\n",
    "            cols = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "            if cols and any(cell.strip() for cell in cols):\n",
    "                rows.append(cols)\n",
    "\n",
    "        if not rows:\n",
    "            return None\n",
    "\n",
    "        headers = headers[:max(len(r) for r in rows)]\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Table extraction failed: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCTION: Clean and Tag Data ---\n",
    "def clean_and_tag_data(df, academic_year, account_name, payment_mode):\n",
    "    df.columns = (\n",
    "        df.columns.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"_\")\n",
    "        .str.replace(\".\", \"\", regex=False)\n",
    "    )\n",
    "\n",
    "    rename_map = {\n",
    "        \"student_number\": \"AdmissionNo\",\n",
    "        \"date_added\": \"Date\",\n",
    "        \"total_received_amount\": \"ReceivedAmount\",\n",
    "        \"receipt_no\": \"ReceiptNo\",\n",
    "        \"class_sec\": \"ClassSec\",\n",
    "        \"student_name\": \"StudentName\",\n",
    "        \"abacus_/_vediic_maths\": \"AbacusVedicMaths\",\n",
    "    }\n",
    "    df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns}, inplace=True)\n",
    "\n",
    "    # --- Convert data types ---\n",
    "    if \"Date\" in df.columns:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=\"%d-%m-%Y\", errors=\"coerce\")\n",
    "    if \"ReceivedAmount\" in df.columns:\n",
    "        df[\"ReceivedAmount\"] = pd.to_numeric(df[\"ReceivedAmount\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    # --- Add metadata ---\n",
    "    df[\"academic_year_id\"] = 1 if academic_year == \"2024-25\" else 2\n",
    "    df[\"account_name\"] = account_name or \"Unknown\"\n",
    "    df[\"payment_mode\"] = payment_mode or \"Unknown\"\n",
    "\n",
    "    # --- Drop filler columns ---\n",
    "    drop_cols = [\"-\", \"abacus_vedic_maths\", \"term_fee\", \"term_fee1\", \"term_fee2\"]\n",
    "    df.drop(columns=[c for c in drop_cols if c in df.columns], inplace=True, errors=\"ignore\")\n",
    "\n",
    "    # --- Remove total/empty rows ---\n",
    "    # A total row usually has all numeric values or starts with 0 / Totals / blank name\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.match(r\"^(0|Totals?|)$\", na=False)]  # skip \"0\" or \"Totals\"\n",
    "    if \"StudentName\" in df.columns:\n",
    "        df = df[df[\"StudentName\"].astype(str).str.strip().ne(\"\")]\n",
    "\n",
    "    # --- Drop missing admission numbers ---\n",
    "    if \"AdmissionNo\" in df.columns:\n",
    "        df = df[df[\"AdmissionNo\"].astype(str).str.strip() != \"\"]\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df, truncate=True):\n",
    "    import numpy as np\n",
    "\n",
    "    # --- Define only the target DB columns ---\n",
    "    target_columns = [\n",
    "        \"SNo\",\n",
    "        \"AdmissionNo\",\n",
    "        \"Date\",\n",
    "        \"ReceivedAmount\",\n",
    "        \"academic_year_id\",\n",
    "        \"account_name\",\n",
    "        \"payment_mode\",\n",
    "    ]\n",
    "\n",
    "    # --- Normalize column names ---\n",
    "    df.columns = (\n",
    "        df.columns\n",
    "        .str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace(\" \", \"\", regex=False)\n",
    "        .str.replace(\".\", \"\", regex=False)\n",
    "        .str.replace(\"/\", \"\", regex=False)\n",
    "        .str.replace(\"-\", \"\", regex=False)\n",
    "        .str.replace(\"_\", \"\", regex=False)\n",
    "    )\n",
    "\n",
    "    # --- Map possible column variations to final schema ---\n",
    "    rename_map = {\n",
    "        \"sno\": \"SNo\",\n",
    "        \"admissionno\": \"AdmissionNo\",\n",
    "        \"date\": \"Date\",\n",
    "        \"receivedamount\": \"ReceivedAmount\",\n",
    "        \"academicyearid\": \"academic_year_id\",\n",
    "        \"accountname\": \"account_name\",\n",
    "        \"paymentmode\": \"payment_mode\",\n",
    "    }\n",
    "\n",
    "    # --- Apply renaming ---\n",
    "    df = df.rename(columns={k: v for k, v in rename_map.items() if k in df.columns})\n",
    "\n",
    "    # --- Drop everything not in target columns ---\n",
    "    df = df[[c for c in df.columns if c in target_columns]]\n",
    "\n",
    "    # --- Add any missing columns as None ---\n",
    "    for c in target_columns:\n",
    "        if c not in df.columns:\n",
    "            df[c] = None\n",
    "\n",
    "    # --- Reorder columns cleanly ---\n",
    "    df = df[target_columns]\n",
    "\n",
    "    # --- Connect to PostgreSQL ---\n",
    "    password = urllib.parse.quote_plus(POSTGRES_CREDENTIALS[\"password\"] or \"\")\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "    # --- Create table if not exists (clean structure) ---\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        \"SNo\" TEXT,\n",
    "        \"AdmissionNo\" TEXT,\n",
    "        \"Date\" DATE,\n",
    "        \"ReceivedAmount\" NUMERIC,\n",
    "        \"academic_year_id\" INTEGER,\n",
    "        \"account_name\" TEXT,\n",
    "        \"payment_mode\" TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_sql))\n",
    "            if truncate:\n",
    "                conn.execute(text(f'TRUNCATE TABLE {TABLE_NAME};'))\n",
    "                print(f\"‚úÖ Table '{TABLE_NAME}' ensured and truncated.\\n\")\n",
    "\n",
    "        # --- Insert cleaned data into DB ---\n",
    "        df.to_sql(\n",
    "            name=TABLE_NAME,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "        print(f\"‚úÖ {len(df)} records inserted into '{TABLE_NAME}' successfully.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error inserting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN FUNCTION ---\n",
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for url in urls_to_fetch:\n",
    "        academic_year = get_academic_year_from_url(url)\n",
    "        print(f\"\\nüìÑ Fetching data for academic year: {academic_year}\")\n",
    "\n",
    "        try:\n",
    "            sections = fetch_account_sections(session, url)\n",
    "            if not sections:\n",
    "                print(f\"‚ö†Ô∏è No account sections found for {academic_year}\")\n",
    "                continue\n",
    "\n",
    "            year_dfs = []\n",
    "\n",
    "            for idx, section in enumerate(sections, start=1):\n",
    "                df, account_name, payment_mode = extract_account_data(section)\n",
    "                if df is None or df.empty:\n",
    "                    continue\n",
    "\n",
    "                df = clean_and_tag_data(df, academic_year, account_name, payment_mode)\n",
    "                if df.empty:\n",
    "                    print(f\"‚ö†Ô∏è Section {idx}: Skipped totals-only data ({account_name})\")\n",
    "                    continue\n",
    "\n",
    "                year_dfs.append(df)\n",
    "                print(f\"‚úÖ {academic_year} | Section {idx}: {len(df)} rows | Account: {account_name} | Mode: {payment_mode}\")\n",
    "\n",
    "            if year_dfs:\n",
    "                combined_year = pd.concat(year_dfs, ignore_index=True)\n",
    "                out_path = os.path.join(OUTPUT_DIR, f\"daywise_fees_collection_{academic_year}.csv\")\n",
    "                combined_year.to_csv(out_path, index=False)\n",
    "                print(f\"üíæ Saved {len(combined_year)} records for {academic_year}\")\n",
    "                all_dfs.append(combined_year)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error fetching {academic_year}: {e}\")\n",
    "\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        master_path = os.path.join(OUTPUT_DIR, \"daywise_fees_collection.csv\")\n",
    "        final_df.to_csv(master_path, index=False)\n",
    "        print(\"\\n‚úÖ Combined CSV saved successfully!\")\n",
    "        print(f\"üìä Total Records Combined: {len(final_df)}\")\n",
    "\n",
    "        # --- Upload to Database ---\n",
    "        update_database(final_df)\n",
    "        print(\"‚úÖ Database update complete.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No valid data extracted from any academic year.\")\n",
    "\n",
    "\n",
    "# --- RUN ---\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>ATTENDANCE REPORT</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from selenium.webdriver.support.ui import WebDriverWait\n",
    "# from selenium.webdriver.support import expected_conditions as EC\n",
    "# from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Login to Website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚úÖ Config\n",
    "# login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "# attendance_url = \"https://app.myskoolcom.tech/kotak_vizag/admin/attedance_grid\"\n",
    "\n",
    "# credentials = {\n",
    "#     \"uname\": \"harikiran\",\n",
    "#     \"psw\": \"812551\"\n",
    "# }\n",
    "\n",
    "# download_folder = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\"\n",
    "# merged_output_path = os.path.join(download_folder, \"MergedAttendance_2025_26.csv\")\n",
    "\n",
    "# academic_ranges = {\n",
    "#     \"2025-26\": (\"2025-06-16\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "#     # \"2025-26\": (\"2025-06-16\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ‚úÖ Setup Chrome Driver\n",
    "# chrome_options = webdriver.ChromeOptions()\n",
    "# prefs = {\"download.default_directory\": download_folder}\n",
    "# chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "# driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "# wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# # ‚úÖ Functions\n",
    "# def login():\n",
    "#     driver.get(login_url)\n",
    "#     wait.until(EC.presence_of_element_located((By.NAME, \"uname\"))).send_keys(credentials[\"uname\"])\n",
    "#     driver.find_element(By.NAME, \"psw\").send_keys(credentials[\"psw\"])\n",
    "#     driver.find_element(By.NAME, \"psw\").send_keys(Keys.RETURN)\n",
    "#     print(\"‚úÖ Logged in successfully!\")\n",
    "#     time.sleep(5)\n",
    "\n",
    "# def set_date_range(start, end):\n",
    "#     driver.get(attendance_url)\n",
    "#     time.sleep(2)\n",
    "#     from_date_input = wait.until(EC.presence_of_element_located((By.ID, \"from_attendance_date\")))\n",
    "#     driver.execute_script(\"arguments[0].removeAttribute('readonly')\", from_date_input)\n",
    "#     from_date_input.clear()\n",
    "#     from_date_input.send_keys(start)\n",
    "\n",
    "#     to_date_input = wait.until(EC.presence_of_element_located((By.ID, \"to_attendance_date\")))\n",
    "#     driver.execute_script(\"arguments[0].removeAttribute('readonly')\", to_date_input)\n",
    "#     to_date_input.clear()\n",
    "#     to_date_input.send_keys(end)\n",
    "\n",
    "#     print(f\"‚úÖ Date range set: {start} to {end}\")\n",
    "\n",
    "# def download_csv(filename):\n",
    "#     try:\n",
    "#         if os.path.exists(filename):\n",
    "#             os.remove(filename)\n",
    "#         download_button = wait.until(EC.element_to_be_clickable((By.ID, \"smaplecsv\")))\n",
    "#         download_button.click()\n",
    "#         time.sleep(8)\n",
    "#         downloaded = sorted(\n",
    "#             [f for f in os.listdir(download_folder) if f.endswith(\".csv\")],\n",
    "#             key=lambda x: os.path.getctime(os.path.join(download_folder, x)),\n",
    "#             reverse=True\n",
    "#         )[0]\n",
    "#         os.rename(os.path.join(download_folder, downloaded), filename)\n",
    "#         print(f\"‚úÖ Downloaded and renamed to: {filename}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Error downloading file: {e}\")\n",
    "\n",
    "# def date_batches(start, end, months=1):\n",
    "#     start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "#     end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "#     while start_date < end_date:\n",
    "#         batch_end = min(start_date + timedelta(days=30 * months), end_date)\n",
    "#         yield (start_date.strftime(\"%Y-%m-%d\"), batch_end.strftime(\"%Y-%m-%d\"))\n",
    "#         start_date = batch_end + timedelta(days=1)\n",
    "\n",
    "# def merge_csvs(folder, output_file, year_filter=\"2025-26\"):\n",
    "#     all_csvs = [\n",
    "#         os.path.join(folder, f)\n",
    "#         for f in os.listdir(folder)\n",
    "#         if f.endswith(\".csv\") and year_filter in f\n",
    "#     ]\n",
    "\n",
    "#     merged_df = pd.DataFrame()\n",
    "\n",
    "#     for f in all_csvs:\n",
    "#         try:\n",
    "#             df = pd.read_csv(f, low_memory=False)\n",
    "#             if \"Students Number\" in df.columns:\n",
    "#                 # Merge logic: remove duplicates by date + student number\n",
    "#                 df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "#                 df = df.dropna(subset=[\"Date\", \"Students Number\"])\n",
    "\n",
    "#                 # Merge with deduplication\n",
    "#                 if not merged_df.empty:\n",
    "#                     merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "#                     merged_df.drop_duplicates(subset=[\"Date\", \"Students Number\"], keep=\"last\", inplace=True)\n",
    "#                 else:\n",
    "#                     merged_df = df\n",
    "#                 print(f\"üîÑ Merged file (with Students Number): {os.path.basename(f)}\")\n",
    "#             else:\n",
    "#                 # Append directly if \"Students Number\" not found\n",
    "#                 merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "#                 print(f\"‚ûï Appended file (no Students Number): {os.path.basename(f)}\")\n",
    "#         except Exception as e:\n",
    "#             print(f\"‚ùå Error reading file {f}: {e}\")\n",
    "\n",
    "#     merged_df.to_csv(output_file, index=False)\n",
    "#     print(f\"‚úÖ Final merged file saved: {output_file}\")\n",
    "\n",
    "#     # ‚úÖ MAIN Execution\n",
    "# login()\n",
    "\n",
    "# # ‚úÖ MAIN Execution\n",
    "# login()\n",
    "\n",
    "# for year, (start, end) in academic_ranges.items():\n",
    "#     print(f\"\\nüìÖ Downloading attendance for {year}\")\n",
    "#     for i, (s, e) in enumerate(date_batches(start, end)):\n",
    "#         s_fmt = datetime.strptime(s, \"%Y-%m-%d\")\n",
    "#         e_fmt = datetime.strptime(e, \"%Y-%m-%d\")\n",
    "#         filename = f\"Attendance_{year}_{s_fmt.strftime('%b')}_{e_fmt.strftime('%b')}.csv\"\n",
    "#         filepath = os.path.join(download_folder, filename)\n",
    "#         set_date_range(s_fmt.strftime(\"%Y-%m-%d\"), e_fmt.strftime(\"%Y-%m-%d\"))\n",
    "#         download_csv(filepath)\n",
    "\n",
    "# # ‚ùå No merging now ‚Äì only individual files will be downloaded and renamed\n",
    "# # merge_csvs(download_folder, merged_output_path, year_filter=\"2025-26\")\n",
    "\n",
    "# driver.quit()\n",
    "# print(\"‚úÖ All attendance downloads complete ‚Äì individual files saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine\n",
    "# import logging\n",
    "# import numpy as np\n",
    "# import urllib\n",
    "# import traceback\n",
    "# from datetime import datetime\n",
    "\n",
    "\n",
    "# # * Configure logging\n",
    "# logging.basicConfig(filename=r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.log\", level=logging.ERROR, \n",
    "#                     format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 2: Define PostgreSQl Credentials & Table Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # * POSTGRES_CREDENTIALS\n",
    "# POSTGRES_CREDENTIALS = {\n",
    "#     \"username\": \"postgres\",\n",
    "#     \"password\": \"Hari@123\",\n",
    "#     \"host\": \"localhost\",\n",
    "#     \"port\": \"5432\",\n",
    "#     \"database\": \"ksdb\",\n",
    "# }\n",
    "# TABLE_NAME = \"attendance_report\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 3: Load and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# def load_and_clean_data(file1, file2, file3=None, file4=None, file5=None):\n",
    "#     # Load DataFrames (read as str to avoid DtypeWarning)\n",
    "#     dfs = [pd.read_csv(f, dtype=str) for f in [file1, file2, file3, file4, file5] if f is not None]\n",
    "\n",
    "#     # Clean column names\n",
    "#     for i in range(len(dfs)):\n",
    "#         dfs[i].columns = dfs[i].columns.str.strip().str.replace('\"', '', regex=False)\n",
    "\n",
    "#         # Strip spaces from all string columns\n",
    "#         dfs[i] = dfs[i].apply(lambda x: x.str.strip() if x.dtype == \"object\" else x)\n",
    "\n",
    "#         # üõ† Date cleaning (if column exists)\n",
    "#         if 'Date' in dfs[i].columns:\n",
    "#             dfs[i]['Date'] = pd.to_datetime(dfs[i]['Date'], dayfirst=True, errors='coerce')\n",
    "\n",
    "#             # Warn about invalid dates\n",
    "#             bad_dates = dfs[i][dfs[i]['Date'].isna()]\n",
    "#             if not bad_dates.empty:\n",
    "#                 print(f\"‚ö†Ô∏è Invalid dates found in file {i+1}:\")\n",
    "#                 print(bad_dates[['Date']].head(10))  # Show first 10\n",
    "\n",
    "#     # Merge logic\n",
    "#     base_df = dfs[0]\n",
    "#     for df in dfs[1:]:\n",
    "#         conflict_cols = [col for col in df.columns if col in base_df.columns and col != 'Students Number']\n",
    "#         df = df.drop(columns=conflict_cols, errors='ignore')\n",
    "#         base_df = base_df.merge(df, on=\"Students Number\", how=\"outer\")\n",
    "\n",
    "#     df = base_df\n",
    "\n",
    "#     # Merge fields like Name, Class if duplicated\n",
    "#     for field in ['Name', 'Class']:\n",
    "#         col_x, col_y = f\"{field}_x\", f\"{field}_y\"\n",
    "#         if col_x in df.columns and col_y in df.columns:\n",
    "#             df[field] = df[col_x].combine_first(df[col_y])\n",
    "#             df.drop([col_x, col_y], axis=1, inplace=True)\n",
    "#         elif col_x in df.columns:\n",
    "#             df[field] = df.pop(col_x)\n",
    "#         elif col_y in df.columns:\n",
    "#             df[field] = df.pop(col_y)\n",
    "\n",
    "#     # Drop remaining _x/_y columns\n",
    "#     df = df.drop(columns=[col for col in df.columns if col.endswith('_x') or col.endswith('_y')], errors='ignore')\n",
    "\n",
    "#     # Rename key identifier\n",
    "#     df = df.rename(columns={\"Students Number\": \"AdmissionNo\"})\n",
    "\n",
    "#     # Drop unnecessary columns\n",
    "#     drop_cols = ['Present Days', 'Absent Days', 'Toral Working Days']\n",
    "#     df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "#     # Reorder columns\n",
    "#     key_cols = ['AdmissionNo', 'Name', 'Class']\n",
    "#     other_cols = [col for col in df.columns if col not in key_cols]\n",
    "#     df = df[key_cols + other_cols]\n",
    "\n",
    "#     return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìå Step 4: Process Attendance Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def process_attendance_data(df):\n",
    "#     # * Step 1: Clean AdmissionNo (remove 786 and purely alphabetical ones)\n",
    "#     df = df[~(df[\"AdmissionNo\"].astype(str) == \"786\") & ~df[\"AdmissionNo\"].astype(str).str.match(r\"^[a-zA-Z]+$\")].copy()\n",
    "\n",
    "#     # * Step 2: Clean Class name (remove ICSE wrapper)\n",
    "#     df[\"Class\"] = df[\"Class\"].astype(str).str.replace(r\"ICSE \\((.*?)\\)\", r\"\\1\", regex=True)\n",
    "\n",
    "#     # * Step 3: Load class info with academic year\n",
    "#     student_df = pd.read_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fees_report.csv\")[[\"adm_no\", \"academic_year_id\", \"class\"]]\n",
    "#     print(\"‚úÖ Students Before Merging\\n\", len(df[\"AdmissionNo\"].unique()))\n",
    "#     df = df[df[\"AdmissionNo\"].isin(student_df[\"adm_no\"])]\n",
    "#     print(\"‚úÖ Students After Merging\\n\", len(df[\"AdmissionNo\"].unique()))\n",
    "\n",
    "#     # * Step 4: Unpivot attendance columns to Date-wise rows\n",
    "#     df_unpivot = pd.melt(df, id_vars=[\"AdmissionNo\", \"Name\", \"Class\"], var_name=\"Date\", value_name=\"AttendanceStatus\")\n",
    "#     df_unpivot[\"Date\"] = pd.to_datetime(df_unpivot[\"Date\"], format='%d.%m.%Y', errors='coerce')\n",
    "\n",
    "#     # * Step 5: Remove invalid past records for new students\n",
    "#     numeric_mask = df_unpivot[\"AdmissionNo\"].str.isnumeric()\n",
    "#     df_unpivot.loc[numeric_mask, \"adm_no_int\"] = df_unpivot.loc[numeric_mask, \"AdmissionNo\"].astype(int)\n",
    "#     df_unpivot = df_unpivot[\n",
    "#         ~((df_unpivot[\"Date\"] < datetime(2024, 4, 1)) & (df_unpivot[\"adm_no_int\"] > 17165))\n",
    "#     ]\n",
    "#     df_unpivot.drop(columns=[\"adm_no_int\"], inplace=True)\n",
    "\n",
    "#     df_unpivot[\"id\"] = range(1, len(df_unpivot) + 1)\n",
    "\n",
    "#     if df_unpivot[\"Date\"].isna().sum() > 0:\n",
    "#         print(\"‚ö†Ô∏è Warning: Some Date values were invalid and converted to NaT.\")\n",
    "\n",
    "#     df_unpivot = df_unpivot.sort_values(\"Date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "#     # * Step 6: Mark \"Not Joined\"\n",
    "#     first_attendance_dates = df_unpivot[df_unpivot['AttendanceStatus'].notna()].groupby('AdmissionNo')['Date'].min()\n",
    "#     df_unpivot['AttendanceStatus'] = df_unpivot.apply(\n",
    "#         lambda row: \"Not Joined\" if row['Date'] < first_attendance_dates.get(row['AdmissionNo'], row['Date']) else row['AttendanceStatus'],\n",
    "#         axis=1\n",
    "#     )\n",
    "\n",
    "#     # * Step 7: Prioritize and deduplicate attendance\n",
    "#     priority_map = {'P': 2, 'A': 1, 'H': 3, 'Not Joined': 4, 'TC': 5}\n",
    "#     df_unpivot['Priority'] = df_unpivot[\"AttendanceStatus\"].map(priority_map)\n",
    "#     df_unpivot = df_unpivot.sort_values(by=['AdmissionNo', 'Date', 'Priority']) \\\n",
    "#                            .drop_duplicates(subset=['AdmissionNo', 'Date'], keep='first') \\\n",
    "#                            .drop(columns=['Priority'])\n",
    "\n",
    "#     # * Step 8: Clean Class + Standardize AttendanceStatus\n",
    "#     df_unpivot['Class'] = df_unpivot['Class'].str.replace(\"Pre KG - \", \"Pre KG\")\n",
    "#     df_unpivot[\"AttendanceStatus\"] = df_unpivot[\"AttendanceStatus\"].replace({\n",
    "#         'P': \"Present\", 'A': \"Absent\", 'H': \"Holiday\"\n",
    "#     })\n",
    "#     df_unpivot.sort_values(by=['Date'], ascending=False, inplace=True)\n",
    "\n",
    "#     # * Step 9: Assign academic year from Date\n",
    "#     df_unpivot['academic_year_id'] = df_unpivot['Date'].apply(\n",
    "#         lambda d: 1 if pd.Timestamp(\"2024-07-17\") <= d <= pd.Timestamp(\"2025-03-31\")\n",
    "#         else 2 if pd.Timestamp(\"2025-06-16\") <= d <= pd.Timestamp(datetime.today().date())\n",
    "#         else \"\"\n",
    "#     )\n",
    "\n",
    "#     # * Step 10: Assign ClassNo by academic year\n",
    "#     student_df[\"adm_no\"] = student_df[\"adm_no\"].astype(str)\n",
    "#     lookup_2024 = student_df[student_df[\"academic_year_id\"] == 1]\n",
    "#     lookup_2025 = student_df[student_df[\"academic_year_id\"] == 2]\n",
    "\n",
    "#     lookup_map_2024 = {row[\"adm_no\"]: row[\"class\"] for _, row in lookup_2024.iterrows()}\n",
    "#     lookup_map_2025 = {row[\"adm_no\"]: row[\"class\"] for _, row in lookup_2025.iterrows()}\n",
    "\n",
    "#     class_mapping = {\n",
    "#         \"Pre KG\": 1, \"LKG - A\": 2, \"LKG - B\": 3, \"UKG - A\": 4, \"UKG - B\": 5, \"UKG - C\": 6,\n",
    "#         \"I - A\": 7, \"I - B\": 8, \"I - C\": 9, \"I - D\": 10,\n",
    "#         \"II - A\": 11, \"II - B\": 12, \"II - C\": 13, \"II - D\": 14,\n",
    "#         \"III - A\": 15, \"III - B\": 16, \"III - C\": 17, \"III - D\": 18,\n",
    "#         \"IV - A\": 19, \"IV - B\": 20, \"IV - C\": 21, \"IV - D\": 22,\n",
    "#         \"V - A\": 23, \"V - B\": 24, \"V - C\": 25, \"V - D\": 26,\n",
    "#         \"VI - A\": 27, \"VI - B\": 28, \"VI - C\": 29, \"VI - D\": 30,\n",
    "#         \"VII - A\": 31, \"VII - B\": 32, \"VII - C\": 33, \"VII - D\": 34,\n",
    "#         \"VIII - A\": 35, \"VIII - B\": 36, \"VIII - C\": 37, \"VIII - D\": 38,\n",
    "#         \"IX - A\": 39, \"IX - B\": 40, \"IX - C\": 41,\n",
    "#         \"X - A\": 42, \"X - B\": 43, \"X - C\": 44\n",
    "#     }\n",
    "\n",
    "#     def get_class_no_2024(adm_no):\n",
    "#         class_name = lookup_map_2024.get(str(adm_no), \"\")\n",
    "#         return class_mapping.get(class_name, np.nan)\n",
    "\n",
    "#     def get_class_no_2025(adm_no):\n",
    "#         class_name = lookup_map_2025.get(str(adm_no), \"\")\n",
    "#         return class_mapping.get(class_name, np.nan)\n",
    "\n",
    "#     df_2024 = df_unpivot[df_unpivot[\"academic_year_id\"] == 1].copy()\n",
    "#     df_2025 = df_unpivot[df_unpivot[\"academic_year_id\"] == 2].copy()\n",
    "\n",
    "#     valid_adm_nos_2025 = set(lookup_map_2025.keys())\n",
    "#     df_2025 = df_2025[df_2025[\"AdmissionNo\"].astype(str).isin(valid_adm_nos_2025)].copy()\n",
    "\n",
    "#     df_2024[\"ClassNo\"] = df_2024[\"AdmissionNo\"].apply(get_class_no_2024)\n",
    "#     df_2025[\"ClassNo\"] = df_2025[\"AdmissionNo\"].apply(get_class_no_2025)\n",
    "\n",
    "#     df_unpivot = pd.concat([df_2024, df_2025], ignore_index=True)\n",
    "#     df_unpivot[\"ClassNo\"] = df_unpivot[\"ClassNo\"].fillna(0).astype(int)\n",
    "\n",
    "#     # * Step 11: Grade level (classId)\n",
    "#     grade_mapping = [\n",
    "#         (\"Pre KG\", 1), (\"LKG\", 2), (\"UKG\", 3),\n",
    "#         (\"I\", 4), (\"II\", 5), (\"III\", 6), (\"IV\", 7), (\"V\", 8),\n",
    "#         (\"VI\", 9), (\"VII\", 10), (\"VIII\", 11), (\"IX\", 12), (\"X\", 13)\n",
    "#     ]\n",
    "#     conditions = [df_unpivot['Class'].str.contains(fr\"\\b{k}\\b\", na=False, regex=True) for k, _ in grade_mapping]\n",
    "#     choices = [v for _, v in grade_mapping]\n",
    "#     df_unpivot['classId'] = np.select(conditions, choices, default=0).astype(int)\n",
    "\n",
    "#     # * Step 12: AttendanceStatusId\n",
    "#     AttendanceStatus_mapping = [(\"Absent\", 1), (\"Present\", 2), (\"Not Joined\", 3), (\"Holiday\", 4)]\n",
    "#     conditions = [df_unpivot['AttendanceStatus'].str.contains(k, na=False) for k, _ in AttendanceStatus_mapping]\n",
    "#     choices = [v for _, v in AttendanceStatus_mapping]\n",
    "#     df_unpivot['AttendanceStatusId'] = np.select(conditions, choices, default=0).astype(int)\n",
    "\n",
    "#     # * Step 13: BranchId\n",
    "#     branch_mapping = [\n",
    "#         ('Pre KG', 1), ('LKG', 1), ('UKG', 1),\n",
    "#         ('I', 2), ('II', 2), ('III', 2), ('IV', 2), ('V', 2),\n",
    "#         ('VI', 3), ('VII', 3), ('VIII', 3), ('IX', 3), ('X', 3)\n",
    "#     ]\n",
    "#     conditions = [df_unpivot['Class'].str.contains(fr\"\\b{k}\\b\", na=False, regex=True) for k, _ in branch_mapping]\n",
    "#     choices = [v for _, v in branch_mapping]\n",
    "#     df_unpivot['branchId'] = np.select(conditions, choices, default=0).astype(int)\n",
    "\n",
    "#     # ‚úÖ Final output\n",
    "#     df_unpivot = df_unpivot[[\n",
    "#         \"id\", \"Date\", \"AdmissionNo\", \"ClassNo\", \"classId\", \"branchId\", \"AttendanceStatusId\", \"academic_year_id\"\n",
    "#     ]]\n",
    "#     df_unpivot.columns = [c.lower() for c in df_unpivot.columns]\n",
    "\n",
    "#     print(f\"‚úÖ Processed data with {len(df_unpivot)} rows.\")\n",
    "#     print(f\"‚úÖ Columns are:\\n {df_unpivot.columns}\")\n",
    "#     return df_unpivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 5: Insert Data into PostgreSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import text\n",
    "\n",
    "# def ensure_table_exists():\n",
    "#     create_table_sql = f\"\"\"\n",
    "#     CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "#         id SERIAL PRIMARY KEY,\n",
    "#         date DATE,\n",
    "#         admissionno TEXT,\n",
    "#         classno INTEGER,\n",
    "#         classid INTEGER,\n",
    "#         branchid INTEGER,\n",
    "#         attendancestatusid INTEGER,\n",
    "#         academic_year_id INTEGER\n",
    "#     );\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         with engine.begin() as connection:  # ‚úÖ ensures DDL is committed\n",
    "#             connection.execute(text(create_table_sql))\n",
    "#         print(f\"‚úÖ Table '{TABLE_NAME}' ensured.\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå Failed to create or check table '{TABLE_NAME}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create database engine\n",
    "# password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "# engine = create_engine(\n",
    "#     f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "#     f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "# )\n",
    "\n",
    "# def update_database(df):\n",
    "#     \"\"\"Use PostgreSQL COPY for ultra-fast data insertion.\"\"\"\n",
    "#     csv_path = (r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.csv\")\n",
    "\n",
    "#     # ‚úÖ Ensure column names are lowercase to match table definition\n",
    "#     df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "#     # ‚úÖ Save DataFrame to CSV\n",
    "#     df.to_csv(csv_path, index=False, header=False)\n",
    "\n",
    "#     try:\n",
    "#         conn = engine.raw_connection()\n",
    "#         cursor = conn.cursor()\n",
    "\n",
    "#         print(f\"üîÑ Truncating table: {TABLE_NAME}\")\n",
    "#         cursor.execute(f\"TRUNCATE TABLE {TABLE_NAME};\")\n",
    "#         conn.commit()\n",
    "\n",
    "#         with open(csv_path, \"r\") as f:\n",
    "#             cursor.copy_from(f, TABLE_NAME, sep=\",\")  # ‚úÖ lowercase and unquoted\n",
    "\n",
    "#         conn.commit()\n",
    "#         cursor.close()\n",
    "#         conn.close()\n",
    "\n",
    "#         print(f\"‚úÖ Data copied to '{TABLE_NAME}' using COPY command!\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå COPY failed: {e}\")\n",
    "#         logging.error(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 6: Run the Full Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     # üìå Already-clean 2024-25 data\n",
    "#     file_2024_25 = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report_2024_25.csv\"\n",
    "\n",
    "#     # üìå Raw 2025-26 files\n",
    "#     file4 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jun_Jul.csv\"\n",
    "#     file5 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jul_Aug.csv\"\n",
    "\n",
    "#     output_file = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.csv\"\n",
    "\n",
    "#     try:\n",
    "#         print(\"üìÇ Loading already-clean 2024-25 data...\\n\")\n",
    "#         df_2024 = pd.read_csv(file_2024_25)\n",
    "#         print(f\"‚úÖ 2024-25 data loaded with {df_2024.shape[0]} rows.\")\n",
    "\n",
    "#         print(\"\\nüõ† Cleaning and loading 2025-26 raw files...\\n\")\n",
    "#         df_2025 = load_and_clean_data(file4, file5)  # No need to pass None for unused files\n",
    "#         print(f\"‚úÖ 2025-26 data cleaned with {df_2025.shape[0]} rows.\")\n",
    "\n",
    "#         # üìå Combine datasets\n",
    "#         df = pd.concat([df_2024, df_2025], ignore_index=True)\n",
    "#         print(f\"\\nüîÑ Combined dataset has {df.shape[0]} rows.\\n\")\n",
    "\n",
    "#         # üìå Process attendance data\n",
    "#         print(\"‚öôÔ∏è Processing attendance data...\\n\")\n",
    "#         df_unpivot = process_attendance_data(df)\n",
    "#         df_unpivot.to_csv(output_file, index=False)\n",
    "\n",
    "#         print(f\"‚úÖ Processed data saved with {df_unpivot.shape[0]} rows.\\n\")\n",
    "#         print(\"‚úÖ Columns are:\\n\", df_unpivot.columns)\n",
    "#         if not df_unpivot.empty:\n",
    "#             print(\"üìÖ Max date in dataset:\", df_unpivot[\"date\"].max())\n",
    "#         print(df_unpivot.head())\n",
    "\n",
    "#         # üìå Update database\n",
    "#         print(\"\\nüíæ Updating database...\\n\")\n",
    "#         ensure_table_exists()\n",
    "#         update_database(df_unpivot)\n",
    "\n",
    "#         print(\"\\nüéØ Attendance report processing completed successfully!\")\n",
    "#         print(f\"üìä Final row count: {df_unpivot.shape[0]}\\n\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"‚ùå An unexpected error occurred: {e}\\n\")\n",
    "#         logging.error(f\"‚ùå Unexpected error: {e}\\n\")\n",
    "\n",
    "\n",
    "# # Run script\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>Class Table</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sqlalchemy import create_engine, text\n",
    "\n",
    "# POSTGRES_CREDENTIALS = {\n",
    "#     \"username\": \"postgres\",\n",
    "#     \"password\": \"Hari@123\",\n",
    "#     \"host\": \"localhost\",\n",
    "#     \"port\": \"5432\",\n",
    "#     \"database\": \"ksdb\",\n",
    "# }\n",
    "# TABLE_NAME = \"class_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\class_section_grade_table.csv\")\n",
    "# # df[\"ClassNo\"] = df[\"ClassNo\"].astype(int)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import traceback\n",
    "# import logging\n",
    "# import pandas as pd\n",
    "# import urllib\n",
    "# import io\n",
    "# from sqlalchemy import create_engine, text\n",
    "# from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# # Retry settings\n",
    "# MAX_RETRIES = 3\n",
    "# RETRY_DELAY = 5  # Seconds\n",
    "\n",
    "# def bulk_insert_postgres(df, conn, table_name):\n",
    "#     \"\"\"Fast bulk insert using PostgreSQL COPY command.\"\"\"\n",
    "#     with conn.connection.cursor() as cur:\n",
    "#         output = io.StringIO()\n",
    "#         df.to_csv(output, sep=\"\\t\", index=False, header=False)\n",
    "#         output.seek(0)\n",
    "#         cur.copy_from(output, table_name, sep=\"\\t\", null=\"NULL\")\n",
    "#         conn.connection.commit()\n",
    "\n",
    "# def update_database(df):\n",
    "#     \"\"\"Insert attendance data into PostgreSQL database with retry logic.\"\"\"\n",
    "#     password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "#     engine = create_engine(f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "#                            f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\")\n",
    "\n",
    "#     for attempt in range(1, MAX_RETRIES + 1):\n",
    "#         try:\n",
    "#             print(f\"üîÑ Attempt {attempt}: Connecting to database {POSTGRES_CREDENTIALS['database']} at {POSTGRES_CREDENTIALS['host']}...\")\n",
    "#             with engine.begin() as conn:\n",
    "#                 print(f\"‚úÖ Connection established.\")\n",
    "\n",
    "#                 # Create Table if it does not exist\n",
    "#                 print(f\"Checking if table '{TABLE_NAME}' exists...\")\n",
    "                \n",
    "#                 # Truncate the table before inserting data\n",
    "#                 print(f\"Truncating existing table: {TABLE_NAME}\")\n",
    "#                 conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} CASCADE;\"))\n",
    "                \n",
    "#                 print(f\"Deleting data from {TABLE_NAME} table...\")\n",
    "#                 conn.execute(text(f\"DELETE FROM {TABLE_NAME};\"))\n",
    "\n",
    "\n",
    "#                 # Fast Bulk Insert\n",
    "#                 print(f\"Inserting data into {TABLE_NAME} table...\")\n",
    "#                 bulk_insert_postgres(df, conn, TABLE_NAME)\n",
    "\n",
    "#                 print(f\"‚úÖ Data successfully inserted into '{TABLE_NAME}' table.\")\n",
    "#                 return  # Exit function if successful\n",
    "\n",
    "#         except OperationalError as e:\n",
    "#             print(f\"‚ùå OperationalError: {e}\")\n",
    "#             logging.error(f\"‚ùå OperationalError: {e}\")\n",
    "#             logging.error(\"Error Traceback:\\n\" + traceback.format_exc())\n",
    "\n",
    "#             if attempt < MAX_RETRIES:\n",
    "#                 print(f\"üîÑ Retrying in {RETRY_DELAY} seconds...\")\n",
    "#                 time.sleep(RETRY_DELAY)\n",
    "#             else:\n",
    "#                 print(\"‚ùå Max retries reached. Could not update the database.\")\n",
    "#                 logging.error(\"‚ùå Max retries reached. Could not update the database.\")\n",
    "#                 return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update_database(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE COLLECTION REPORT 2024-25</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Logging\n",
    "logging.basicConfig(filename=\"fee_collection_merge.log\", level=logging.ERROR)\n",
    "\n",
    "# üîê Credentials & URLs\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "credentials = {\"uname\": \"harikiran\", \"psw\": \"812551\"}\n",
    "urls = {\n",
    "    \"2024_25\": \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_consolidate_report_print?&from=2025-04-01&academic_years_id=1&status=1&imageField=Search\",\n",
    "    \"2025_26\": \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_consolidate_report_print?&from=2024-04-01&academic_years_id=7&status=1&imageField=Search\"\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"fees_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîå Create Engine\n",
    "def get_engine():\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    return create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "# üîë Login\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    response = session.post(login_url, data=credentials)\n",
    "    if \"Invalid\" in response.text:\n",
    "        print(\"‚ùå Login failed!\")\n",
    "        return None\n",
    "    print(\"‚úÖ Login successful!\")\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Convert HTML table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
    "    rows = [[td.get_text(strip=True) for td in tr.find_all(\"td\")] for tr in table.find_all(\"tr\")[1:]]\n",
    "    return pd.DataFrame(rows, columns=headers) if rows else None\n",
    "\n",
    "# üì• Fetch fee table from a given URL\n",
    "def fetch_fee_table(session, url):\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\", class_=\"b-t\")\n",
    "    all_data = []\n",
    "\n",
    "    for table in tables:\n",
    "        df = table_to_dataframe(table)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, academic_year):\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.startswith(\"Total\", na=False)].copy()\n",
    "    df[\"Admin No.\"] = df[\"Admin No.\"].astype(str)\n",
    "\n",
    "    if academic_year == \"2025_26\":\n",
    "        df.columns = ['SNo', 'AdmissionNo', 'Name', 'Abacus1', 'TermFee1', 'TermFee2',\n",
    "                      'Total_Fees', 'Abacus2', 'TermFee3', 'TermFee4',\n",
    "                      'Total_Fee_Paid', 'Discount_Concession', 'Total_Due']\n",
    "        df = df.drop(columns=[\"SNo\", \"Abacus1\", \"Abacus2\", \"TermFee1\", \"TermFee2\", \"TermFee3\", \"TermFee4\"])\n",
    "\n",
    "    elif academic_year == \"2024_25\":\n",
    "        df.columns = ['SNo', 'AdmissionNo', 'Name', 'Abacus1', 'TermFee1',\n",
    "                      'Total_Fees', 'Abacus2', 'TermFee2',\n",
    "                      'Total_Fee_Paid', 'Discount_Concession', 'Total_Due']\n",
    "        df = df.drop(columns=[\"SNo\", \"Abacus1\", \"Abacus2\", \"TermFee1\", \"TermFee2\"])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown academic year structure: {academic_year}\")\n",
    "\n",
    "    # Convert numeric columns safely\n",
    "    numeric_columns = [\"Total_Fees\", \"Total_Fee_Paid\", \"Discount_Concession\", \"Total_Due\"]\n",
    "    for col in numeric_columns:\n",
    "        df[col] = (\n",
    "            df[col].astype(str)\n",
    "            .str.replace(\",\", \"\", regex=False)\n",
    "            .replace([\"\", \"None\", \"nan\", \"NaN\", np.nan], 0)\n",
    "        )\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    df[\"academic_year_id\"] = 1 if academic_year ==\"2024_25\" else 2\n",
    "\n",
    "    # df = df[~((df[\"AdmissionNo\"].str.extract(r\"(\\d+)\").astype(int) > 17164) & (df[\"academic_year_id\"] == 1))].copy()\n",
    "\n",
    "    df.to_csv(f\"D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fees_collection.csv\", index=False)\n",
    "    df = df.drop(columns=[\"Name\"])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "def ensure_fees_collection_table(engine):\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fees_collection (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        admissionno TEXT,\n",
    "        total_fee_paid INTEGER,\n",
    "        academic_year_id INTEGER NOT NULL,\n",
    "        total_fees INTEGER default 0,\n",
    "        discount_concession INTEGER default 0,\n",
    "        total_due INTEGER default 0\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_sql))\n",
    "        print(\"‚úÖ Table 'fees_collection' ensured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ¢Ô∏è Insert into PostgreSQL\n",
    "def update_database(df, table_name):\n",
    "    engine = get_engine()\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            print(f\"‚ö†Ô∏è Deleting old records from '{table_name}'...\")\n",
    "            conn.execute(text(f\"DELETE FROM {table_name};\"))\n",
    "            print(f\"‚úÖ Table '{table_name}' cleared.\")\n",
    "        df.columns = df.columns.str.lower()\n",
    "        print(f\"üì• Inserting {len(df)} rows...\")\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "        print(f\"‚úÖ Inserted into '{table_name}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inserting: {e}\")\n",
    "        logging.error(f\"Database insert error: {e}\")\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Main Logic\n",
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for year, url in urls.items():\n",
    "        print(f\"\\nüîÑ Fetching data for {year}...\")\n",
    "        raw_df = fetch_fee_table(session, url)\n",
    "        if raw_df.empty:\n",
    "            print(f\"‚ùå No data for {year}!\")\n",
    "            continue\n",
    "        clean_df = clean_data(raw_df, academic_year=year)\n",
    "        merged_df = pd.concat([merged_df, clean_df], ignore_index=True)\n",
    "\n",
    "    if merged_df.empty:\n",
    "        print(\"‚ùå No data collected from any year!\")\n",
    "        return\n",
    "\n",
    "    # Save CSV (optional)\n",
    "    merged_df.to_csv(\"merged_fee_collection.csv\", index=False)\n",
    "    print(\"üìÅ Saved to merged_fee_collection.csv\")\n",
    "\n",
    "    # Ensure table exists\n",
    "    engine = get_engine()\n",
    "    ensure_fees_collection_table(engine)\n",
    "    print(\"‚úÖ Fees collection table ensured.\")\n",
    "    # Push to DB\n",
    "    update_database(merged_df, TABLE_NAME)\n",
    "    print(f\"‚úÖ All done! Total records: {len(merged_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE CONCESSION REPORT</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import logging\n",
    "from datetime import date\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# ------------------ Configuration ------------------\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "data_url_2024_25 = \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_discounts_report_receipt_wise_print?&academic_years_id=1\"\n",
    "data_url_2025_26 = \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_discounts_report_receipt_wise_print?&academic_years_id=7\"\n",
    "\n",
    "credentials = {\n",
    "    \"uname\": \"harikiran\",\n",
    "    \"psw\": \"812551\"\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"fee_concession_report\"\n",
    "OUTPUT_PATH = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fee_concession_report.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Login Function ------------------\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    login_response = session.post(login_url, data=credentials)\n",
    "\n",
    "    if login_response.status_code != 200:\n",
    "        print(\"‚ùå Login request failed! Server error.\\n\")\n",
    "        return None\n",
    "\n",
    "    soup = BeautifulSoup(login_response.text, \"html.parser\")\n",
    "    if soup.find(\"div\", class_=\"alert-danger\"):\n",
    "        print(\"‚ùå Login failed! Check credentials.\\n\")\n",
    "        return None\n",
    "\n",
    "    print(\"‚úÖ Login successful!\\n\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Fetch Table Data ------------------\n",
    "def fetch_all_concession_tables(session, data_url):\n",
    "    response = session.get(data_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    tables = soup.find_all(\"table\", class_=\"table_view\")\n",
    "    if not tables:\n",
    "        print(\"‚ùå No fee tables found! The page structure may have changed.\")\n",
    "        return None\n",
    "\n",
    "    all_data = []\n",
    "    for table in tables:\n",
    "        df = table_to_dataframe(table)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"‚ùå No data extracted from tables.\")\n",
    "        return None\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ HTML Table to DataFrame ------------------\n",
    "def table_to_dataframe(table):\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
    "    if len(headers) > 8:\n",
    "        headers = headers[:8]\n",
    "\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        if len(cells) >= 8:\n",
    "            rows.append(cells[:8])\n",
    "\n",
    "    return pd.DataFrame(rows, columns=headers) if rows else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Clean DataFrame ------------------\n",
    "def clean_data(df):\n",
    "    df.columns = df.columns.str.strip().str.replace(\" \", \"_\").str.lower()\n",
    "    df = df.dropna(subset=[\"student_number\"])\n",
    "    df[\"student_number\"] = df[\"student_number\"].astype(str).str.strip()\n",
    "    df[\"discount_given\"] = pd.to_numeric(df[\"discount_given\"], errors=\"coerce\").fillna(0.00)\n",
    "    df.drop(columns=['receipt_no', 'fee_name', 'fee_amount', 'total_due_amount'], errors=\"ignore\", inplace=True)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"].astype(str).str.strip(), errors=\"coerce\").dt.date\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "\n",
    "    df[\"id\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # Ensure academic_year is kept if present\n",
    "    cols = ['id', 'date', 'student_number', 'student_name', 'discount_given']\n",
    "    if \"academic_year\" in df.columns:\n",
    "        cols.append(\"academic_year\")\n",
    "\n",
    "    df = df[cols]\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    df['academic_year_id'] = df['academic_year'].apply(\n",
    "        lambda x: 1 if x == \"2024-25\" else 2 if x == \"2025-26\" else None\n",
    "    )\n",
    "    \n",
    "    df.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"‚úÖ Cleaned data saved to {OUTPUT_PATH}\\n\")\n",
    "\n",
    "    df = df.drop(columns=['student_name', \"academic_year\"], errors=\"ignore\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df: pd.DataFrame, table_name: str, postgres_credentials: dict):\n",
    "    password = urllib.parse.quote(postgres_credentials[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{postgres_credentials['username']}:{password}\"\n",
    "        f\"@{postgres_credentials['host']}:{postgres_credentials['port']}/{postgres_credentials['database']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            print(f\"üîÑ Connecting to database {postgres_credentials['database']}...\")\n",
    "\n",
    "            # ‚úÖ Create table if not exists\n",
    "            conn.execute(text(f\"\"\"\n",
    "                CREATE TABLE IF NOT EXISTS {table_name} (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    date DATE,\n",
    "                    student_number VARCHAR(20),\n",
    "                    discount_given NUMERIC(10, 2),\n",
    "                    academic_year_id INTEGER\n",
    "                );\n",
    "            \"\"\"))\n",
    "            print(f\"‚úÖ Ensured '{table_name}' table exists.\")\n",
    "\n",
    "            # üîÑ Clear existing records\n",
    "            print(f\"‚ö†Ô∏è Deleting existing records from: {table_name}\")\n",
    "            conn.execute(text(f\"DELETE FROM {table_name};\"))\n",
    "            print(f\"‚úÖ Table '{table_name}' cleared.\\n\")\n",
    "\n",
    "        # üì• Insert Data\n",
    "        print(f\"üì• Inserting data into {table_name} table...\")\n",
    "        df.to_sql(name=table_name, con=engine, if_exists=\"append\", index=False, method=\"multi\", chunksize=1000)\n",
    "        print(f\"‚úÖ Data successfully inserted into '{table_name}' table.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error updating database: {e}\", exc_info=True)\n",
    "        print(f\"‚ùå Error occurred while updating database: {e}\")\n",
    "\n",
    "    finally:\n",
    "        engine.dispose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    df_2024_25 = fetch_all_concession_tables(session, data_url_2024_25)\n",
    "    df_2025_26 = fetch_all_concession_tables(session, data_url_2025_26)\n",
    "\n",
    "    if df_2024_25 is None or df_2025_26 is None:\n",
    "        print(\"‚ùå Could not fetch data for one or both academic years.\")\n",
    "        return\n",
    "\n",
    "    df_2024_25[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025_26[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    merged_df = pd.concat([df_2024_25, df_2025_26], ignore_index=True)\n",
    "\n",
    "    print(\"‚úÖ Data extracted successfully! Cleaning data...\\n\")\n",
    "    cleaned_df = clean_data(merged_df)\n",
    "\n",
    "    output_file = r\"D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fee_concession_report_combined.csv\"\n",
    "    cleaned_df.to_csv(output_file, index=False)\n",
    "    print(cleaned_df.columns)\n",
    "    print(f\"‚úÖ Data saved to '{output_file}'\\n\")\n",
    "\n",
    "    update_database(cleaned_df, TABLE_NAME, POSTGRES_CREDENTIALS)\n",
    "    print(f\"‚úÖ {len(cleaned_df)} records entered into the database\")\n",
    "\n",
    "    print(cleaned_df.to_string())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Run Script ------------------\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
