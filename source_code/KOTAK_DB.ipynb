{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\"><b>KOTAK SALESIAN SCHOOL</b></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>STUDENTS DATABASE MANAGEMENT</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from sqlalchemy import create_engine, text\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "import urllib.parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Backup Files Before running New**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Backup successful: D:/postgres_backups\\backup_kotakschooldb_2025-07-27_20-09-31.sql\n"
     ]
    }
   ],
   "source": [
    "# * PostgreSQL Credentials\n",
    "DB_USER = \"postgres\"\n",
    "DB_PASSWORD = \"Hari@123\"\n",
    "DB_NAME = \"kotakschooldb\"\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = \"5432\"\n",
    "BACKUP_DIR = \"D:/postgres_backups\"  # * Backup directory\n",
    "\n",
    "# * Full path to pg_dump (if needed)\n",
    "PG_DUMP_PATH = r\"C:\\Program Files\\PostgreSQL\\17\\bin\\pg_dump.exe\"\n",
    "\n",
    "# * Ensure the backup directory exists\n",
    "os.makedirs(BACKUP_DIR, exist_ok=True)\n",
    "\n",
    "# * Generate a timestamp for the backup file\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "backup_file = os.path.join(BACKUP_DIR, f\"backup_{DB_NAME}_{timestamp}.sql\")\n",
    "\n",
    "# * Run pg_dump\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\n",
    "            PG_DUMP_PATH,  # Use full path if not in PATH\n",
    "            \"-U\", DB_USER,\n",
    "            \"-h\", DB_HOST,\n",
    "            \"-p\", DB_PORT,\n",
    "            \"-F\", \"c\",\n",
    "            \"-b\",\n",
    "            \"-v\",\n",
    "            \"-f\", backup_file,\n",
    "            DB_NAME\n",
    "        ],\n",
    "        env={**os.environ, \"PGPASSWORD\": DB_PASSWORD},\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "    )\n",
    "\n",
    "    # * Check for errors\n",
    "    if result.returncode == 0:\n",
    "        print(f\"‚úÖ Backup successful: {backup_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Backup failed!\\nError: {result.stderr}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ö†Ô∏è pg_dump not found at {PG_DUMP_PATH}. Check PostgreSQL installation or system PATH.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries & Define Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "GOOGLE_JSON_PATHS = {\n",
    "    \"2024-25\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2024-25.json\",\n",
    "    \"2025-26\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2025-26.json\",\n",
    "}\n",
    "\n",
    "GOOGLE_SHEET_TITLES = {\n",
    "    \"2024-25\": \"STUDENTS DETAILS 2024-25\",\n",
    "    \"2025-26\": \"STUDENTS DETAILS 2025-26\",\n",
    "}\n",
    "\n",
    "UNIQUE_KEY = \"Adm No.\"\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"students\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Extract Data from Google Sheet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FETCH GOOGLE SHEET ===\n",
    "def fetch_data(sheet_title, worksheet_name=\"Overall\", json_path=None):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(sheet_title)\n",
    "    sheet = spreadsheet.worksheet(worksheet_name)\n",
    "    data = sheet.get_all_records(head=3)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === CLEAN COLUMN NAMES ===\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MERGE AND TAG STATUS ===\n",
    "def merge_and_tag():\n",
    "    df_2024 = clean_column_names(fetch_data(GOOGLE_SHEET_TITLES[\"2024-25\"], \"Overall\", GOOGLE_JSON_PATHS[\"2024-25\"]))\n",
    "    df_2025 = clean_column_names(fetch_data(GOOGLE_SHEET_TITLES[\"2025-26\"], \"Overall\", GOOGLE_JSON_PATHS[\"2025-26\"]))\n",
    "\n",
    "    df_2024[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    codes_2024 = set(df_2024[UNIQUE_KEY])\n",
    "    codes_2025 = set(df_2025[UNIQUE_KEY])\n",
    "\n",
    "    continuing = codes_2024.intersection(codes_2025)\n",
    "    left = codes_2024 - codes_2025\n",
    "    new = codes_2025 - codes_2024\n",
    "\n",
    "    df_2024[\"status\"] = df_2024[UNIQUE_KEY].apply(lambda x: \"left\" if x in left else \"continuing\")\n",
    "    df_2025[\"status\"] = df_2025[UNIQUE_KEY].apply(lambda x: \"new\" if x in new else \"continuing\")\n",
    "\n",
    "    return pd.concat([df_2024, df_2025], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # ‚ú® Rename columns to match your database structure\n",
    "    df.columns = [\n",
    "        \"sno\", \"adm_no\", \"name\", \"class\", \"gender\", \"mother_name\", \"father_name\",\n",
    "        \"pen_number\", \"dob\", \"phone_no\", \"religion\", \"caste\", \"sub_caste\",\n",
    "        \"second_lang\", \"remarks\", \"class_nos\", \"joined_year\", \"grades\",\n",
    "        \"academic_year\", \"status\"\n",
    "    ]\n",
    "\n",
    "    # Lowercase and strip spaces for consistency\n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # üóìÔ∏è Convert DOB to MySQL/PostgreSQL date format\n",
    "    df[\"dob\"] = pd.to_datetime(df[\"dob\"], format=\"%d-%m-%Y\", errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    # üî¢ Convert joined_year to integer\n",
    "    df[\"joined_year\"] = pd.to_numeric(df[\"joined_year\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # üßπ Optional: remove known junk column\n",
    "    if \"apaar_status\" in df.columns:\n",
    "        df.drop(columns=[\"apaar_status\"], inplace=True)\n",
    "\n",
    "    df = df.sort_values(by=[\"academic_year\", \"class_nos\", \"gender\",\"name\"  ], ascending=[True,  True, True, True])\n",
    "\n",
    "    df[\"gender\"] = df[\"gender\"].str.upper()\n",
    "\n",
    "    df[\"sno\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # üü¢ Ensure no NaN values in non-nullable fields\n",
    "    df[\"adm_no\"] = df[\"adm_no\"].astype(str).str.strip()\n",
    "    df[\"name\"] = df[\"name\"].astype(str).str.strip()\n",
    "    df[\"academic_year\"] = df[\"academic_year\"].astype(str).str.strip()\n",
    "\n",
    "    # üßæ Save to CSV for audit/debug\n",
    "    output_path = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\students_data.csv\"\n",
    "    df.to_csv(output_path, index=False)\n",
    "    print(f\"‚úÖ Cleaned data saved to {output_path}\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df):\n",
    "    import numpy as np\n",
    "    password = urllib.parse.quote_plus(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\",\n",
    "        echo=False\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # Step 1: Truncate the table\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} RESTART IDENTITY CASCADE;\"))\n",
    "            print(f\"üóëÔ∏è Old records deleted from '{TABLE_NAME}'.\")\n",
    "\n",
    "        # Step 2: Prepare DataFrame for PostgreSQL\n",
    "        df = df.replace({pd.NA: None, np.nan: None})\n",
    "\n",
    "        print(\"‚è≥ Inserting data into DB...\")\n",
    "\n",
    "        # Step 3: Insert with chunking to prevent timeout\n",
    "        df.to_sql(name=TABLE_NAME, con=engine, if_exists='append', index=False, method='multi', chunksize=500)\n",
    "\n",
    "        # Step 4: Confirm insert count\n",
    "        with engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"SELECT COUNT(*) FROM {TABLE_NAME};\"))\n",
    "            count = result.scalar()\n",
    "            print(f\"‚úÖ Insert complete. üìä Table now contains {count} records.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error updating database: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Clean Extracted Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting full student import pipeline...\n",
      "\n",
      "‚úÖ Cleaned data saved to D:\\GITHUB\\kotak-school-dbms\\output_data\\students_data.csv\n",
      "üóëÔ∏è Old records deleted from 'students'.\n",
      "‚è≥ Inserting data into DB...\n",
      "‚úÖ Insert complete. üìä Table now contains 3382 records.\n",
      "\n",
      "üéâ All done!\n"
     ]
    }
   ],
   "source": [
    "# === RUN THE PIPELINE ===\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting full student import pipeline...\\n\")\n",
    "    merged_df = merge_and_tag()\n",
    "    cleaned_df = clean_data(merged_df)\n",
    "    update_database(cleaned_df)\n",
    "    print(\"üéâ All done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE REPORTS</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Google Console Service Account: myschooldb@woven-solution-446513-f2.iam.gserviceaccount.com**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Necessary Libraries & Define Global Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIG ===\n",
    "GOOGLE_JSON_PATHS = {\n",
    "    \"2024-25\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2024-25-fees.json\",\n",
    "    \"2025-26\": r\"D:\\GITHUB\\kotak-school-dbms\\google_api_keys\\woven-solution-446513-f2-2025-26-fees.json\"\n",
    "}\n",
    "\n",
    "GOOGLE_SHEET_TITLES = {\n",
    "    \"2024-25\": \"Fee Reports 2024-25\",\n",
    "    \"2025-26\": \"Fee Reports 2025-26\",\n",
    "}\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"fees_table\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Fetching Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FETCH GOOGLE SHEET ===\n",
    "def fetch_data(sheet_title, worksheet_name=\"Overall Sheet\", json_path=None):\n",
    "    scope = [\"https://spreadsheets.google.com/feeds\", \"https://www.googleapis.com/auth/drive\"]\n",
    "    creds = ServiceAccountCredentials.from_json_keyfile_name(json_path, scope)\n",
    "    client = gspread.authorize(creds)\n",
    "    spreadsheet = client.open(sheet_title)\n",
    "    sheet = spreadsheet.worksheet(worksheet_name)\n",
    "    data = sheet.get_all_records(head=3)\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# === CLEAN COLUMN NAMES ===\n",
    "def clean_column_names(df):\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === MERGE AND TAG STATUS ===\n",
    "def merge_and_tag():\n",
    "    df_2024 = clean_column_names(fetch_data(GOOGLE_SHEET_TITLES[\"2024-25\"], \"Overall Sheet\", GOOGLE_JSON_PATHS[\"2024-25\"]))\n",
    "    df_2025 = clean_column_names(fetch_data(GOOGLE_SHEET_TITLES[\"2025-26\"], \"Overall Sheet\", GOOGLE_JSON_PATHS[\"2025-26\"]))\n",
    "\n",
    "    df_2024[\"academic_year\"] = \"2024-25\"\n",
    "    df_2025[\"academic_year\"] = \"2025-26\"\n",
    "\n",
    "    return pd.concat([df_2024, df_2025], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Cleaning Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    df = df[:-1][:-6]\n",
    "\n",
    "    df.columns = ['SNo', 'STUDENT_NAME', 'ADM_NO', 'FB_NO', 'CLASS',\n",
    "                  'Term1', 'Term2', 'Term3', 'Term4', 'Total_Fee_Paid',\n",
    "                  'Discount_Concession', 'Total_Fee_Due', 'PermissionUpto',\n",
    "                  'Fine', 'Payment_Status', 'ClassNo',\"AcNo\",'Concession_type', \"staff_name\", \"academic_year\"]\n",
    "    \n",
    "    df.columns = df.columns.str.strip().str.lower()\n",
    "\n",
    "    # üö´ Remove blank admission numbers\n",
    "    df = df[df[\"adm_no\"].astype(str).str.strip() != \"\"]\n",
    "    df = df.dropna(subset=[\"adm_no\"])\n",
    "    # üö´ Remove blank admission numbers\n",
    "    df = df[df[\"student_name\"].astype(str).str.strip() != \"\"]\n",
    "    df = df.dropna(subset=[\"student_name\"])\n",
    "\n",
    "    columns_to_convert = [\"term1\", \"term2\", \"term3\", \"term4\", \"total_fee_paid\",\n",
    "                          \"discount_concession\", \"total_fee_due\", \"fine\"]\n",
    "    \n",
    "    df[columns_to_convert] = df[columns_to_convert].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "    df = df.drop(columns=[\"acno\", 'concession_type'])\n",
    "\n",
    "    df[\"sno\"] = range(1, len(df) + 1)\n",
    "\n",
    "    df = df.sort_values(by=[\"sno\"])\n",
    "\n",
    "    df[\"total_fees\"] = df[\"total_fee_paid\"] + df[\"discount_concession\"] + df[\"total_fee_due\"]\n",
    "\n",
    "    df = df.sort_values(by=[\"academic_year\", \"classno\", \"student_name\"], ascending=[True, True, True])\n",
    "    \n",
    "    df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fees_report.csv\", index=False)\n",
    "    \n",
    "    PaymentStatus = df[[\"payment_status\"]].drop_duplicates().reset_index(drop=True)\n",
    "    PaymentStatus[\"payment_status_id\"] = range(1, len(PaymentStatus) + 1)\n",
    "    PaymentStatus = PaymentStatus[[\"payment_status_id\", \"payment_status\"]]\n",
    "    PaymentStatus.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\payment_status_table.csv\", index=False)\n",
    "    print(\"‚úÖ Payment Status Table created successfully.\\n\")\n",
    "    \n",
    "    df = pd.merge(df, PaymentStatus, on=\"payment_status\", how=\"left\")\n",
    "    \n",
    "    df.drop(columns=[\"permissionupto\",\"payment_status\"], inplace=True)\n",
    "    \n",
    "    df.columns = df.columns.str.lower().str.strip()\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function for Updating the Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PostgreSQL Connection Credentials\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "# Encode password for URL safety\n",
    "password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "\n",
    "# Create Engine\n",
    "engine = create_engine(f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "                       f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\")\n",
    "\n",
    "def table_exists(table_name):\n",
    "    \"\"\"Check if table exists in PostgreSQL database\"\"\"\n",
    "    check_query = f\"\"\"\n",
    "    SELECT EXISTS (\n",
    "        SELECT FROM information_schema.tables \n",
    "        WHERE table_name = '{table_name}'\n",
    "    );\n",
    "    \"\"\"\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(check_query)).scalar()\n",
    "    return result\n",
    "\n",
    "def create_table():\n",
    "    \"\"\"Create table only if it does not exist\"\"\"\n",
    "    table_name = \"fees_table\"\n",
    "    \n",
    "    if table_exists(table_name):\n",
    "        print(f\"‚úÖ Table '{table_name}' already exists.\")\n",
    "        return\n",
    "\n",
    "    create_table_query = \"\"\"\n",
    "    CREATE TABLE fees_table (\n",
    "    sno SERIAL PRIMARY KEY,\n",
    "    student_name TEXT NOT NULL,\n",
    "    adm_no TEXT,\n",
    "    fb_no TEXT,\n",
    "    class TEXT,\n",
    "    term1 NUMERIC DEFAULT 0,\n",
    "    term2 NUMERIC DEFAULT 0,\n",
    "    term3 NUMERIC DEFAULT 0,\n",
    "    term4 NUMERIC DEFAULT 0,\n",
    "    total_fee_paid NUMERIC DEFAULT 0,\n",
    "    discount_concession NUMERIC DEFAULT 0,\n",
    "    total_fee_due NUMERIC DEFAULT 0,\n",
    "    fine NUMERIC DEFAULT 0,\n",
    "    classno TEXT,\n",
    "    staff_name TEXT,\n",
    "    academic_year TEXT NOT NULL,\n",
    "    total_fees INTEGER DEFAULT 0,\n",
    "    payment_status_id INTEGER\n",
    ");\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(create_table_query))\n",
    "            print(f\"‚úÖ Table '{table_name}' created successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "def update_database(df):\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            # ‚úÖ Truncate existing table and reset serial ID\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} RESTART IDENTITY CASCADE;\"))\n",
    "            print(f\"‚úÖ All records from the '{TABLE_NAME}' table have been deleted.\\n\")\n",
    "\n",
    "            # ‚úÖ Add UNIQUE constraint on 'admissionno' (if it doesn't exist)\n",
    "            conn.execute(text(f\"\"\"\n",
    "                DO $$ \n",
    "                BEGIN \n",
    "                    -- Drop old constraint if exists\n",
    "                    IF EXISTS (\n",
    "                        SELECT 1 FROM information_schema.table_constraints \n",
    "                        WHERE table_name = '{TABLE_NAME}' AND constraint_name = 'unique_admissionno'\n",
    "                    ) THEN\n",
    "                        ALTER TABLE {TABLE_NAME} DROP CONSTRAINT unique_admissionno;\n",
    "                    END IF;\n",
    "\n",
    "                    -- Add new composite unique constraint if not exists\n",
    "                    IF NOT EXISTS (\n",
    "                        SELECT 1 FROM information_schema.table_constraints \n",
    "                        WHERE table_name = '{TABLE_NAME}' AND constraint_name = 'unique_adm_year'\n",
    "                    ) THEN\n",
    "                        ALTER TABLE {TABLE_NAME} ADD CONSTRAINT unique_adm_year UNIQUE (\"adm_no\", \"academic_year\");\n",
    "                    END IF;\n",
    "                END $$;\n",
    "            \"\"\"))\n",
    "\n",
    "            print(f\"‚úÖ Unique constraint on 'admissionno' ensured in the '{TABLE_NAME}' table.\\n\")\n",
    "\n",
    "        print(\"‚úÖ Table cleared. Proceeding with data insertion...\\n\")\n",
    "\n",
    "        # ‚úÖ Normalize column names\n",
    "        df.columns = df.columns.str.lower()\n",
    "\n",
    "        # ‚úÖ Drop generated or unneeded columns\n",
    "        if \"totalfees\" in df.columns:\n",
    "            df = df.drop(columns=[\"totalfees\"])\n",
    "            print(\"‚úÖ 'totalfees' column removed before insertion (generated column).\\n\")\n",
    "\n",
    "        # ‚úÖ Insert data in chunks\n",
    "        df.to_sql(\n",
    "            name=TABLE_NAME,\n",
    "            con=engine,\n",
    "            if_exists='append',\n",
    "            index=False,\n",
    "            method='multi',\n",
    "            chunksize=1000\n",
    "        )\n",
    "\n",
    "        print(f\"‚úÖ {len(df)} records successfully inserted into '{TABLE_NAME}'.\\n\")\n",
    "\n",
    "    except SQLAlchemyError as e:\n",
    "        print(f\"‚ùå An error occurred during database update: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Main Execution Block**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Raw data merged from both years.\n",
      "\n",
      "‚úÖ Payment Status Table created successfully.\n",
      "\n",
      "‚úÖ Data cleaned and transformed successfully.\n",
      "\n",
      "‚úÖ Final columns are:\n",
      " ['sno', 'student_name', 'adm_no', 'fb_no', 'class', 'term1', 'term2', 'term3', 'term4', 'total_fee_paid', 'discount_concession', 'total_fee_due', 'fine', 'classno', 'staff_name', 'academic_year', 'total_fees', 'payment_status_id']\n",
      "‚úÖ Table 'fees_table' already exists.\n",
      "\n",
      "‚úÖ Table check/creation complete.\n",
      "\n",
      "‚úÖ Deduplicated. Final records to upload: 3382\n",
      "\n",
      "‚úÖ All records from the 'fees_table' table have been deleted.\n",
      "\n",
      "‚úÖ Unique constraint on 'admissionno' ensured in the 'fees_table' table.\n",
      "\n",
      "‚úÖ Table cleared. Proceeding with data insertion...\n",
      "\n",
      "‚úÖ 3382 records successfully inserted into 'fees_table'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # * Merge and tag both years\n",
    "    combined_df = merge_and_tag()\n",
    "    print(\"‚úÖ Raw data merged from both years.\\n\")\n",
    "\n",
    "    # * Clean and process the merged data\n",
    "    cleaned_df = clean_data(combined_df)\n",
    "    print(\"‚úÖ Data cleaned and transformed successfully.\\n\")\n",
    "    print(\"‚úÖ Final columns are:\\n\", cleaned_df.columns.to_list())\n",
    "\n",
    "    # * Create table if it does not exist\n",
    "    create_table()\n",
    "    print(\"\\n‚úÖ Table check/creation complete.\\n\")\n",
    "\n",
    "    # * Drop duplicates by adm_no + year before insert\n",
    "    cleaned_df = cleaned_df.drop_duplicates(subset=[\"adm_no\", \"academic_year\"])\n",
    "    print(f\"‚úÖ Deduplicated. Final records to upload: {len(cleaned_df)}\\n\")\n",
    "\n",
    "    # * Upload data using safe insertion\n",
    "    update_database(cleaned_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>DAY WISE REPORTS</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Login Credentials and MySQL Credentials**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * üîπ Login Credentials\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "urls_with_years = [\n",
    "    (\"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_reports_day_wise_receipt_wise_print?academic_years_id=1\", \"2024-25\"),\n",
    "    (\"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_reports_day_wise_receipt_wise_print?academic_years_id=7\", \"2025-26\"),\n",
    "]\n",
    "\n",
    "credentials = {\n",
    "    \"uname\": \"harikiran\",\n",
    "    \"psw\": \"812551\"\n",
    "}\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"daywise_fees_collection\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Define Functions for Each Step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    login_response = session.post(login_url, data=credentials)\n",
    "\n",
    "    if \"Invalid\" in login_response.text:\n",
    "        print(\"‚ùå Login failed! Check credentials.\\n\")\n",
    "        return None\n",
    "    else:\n",
    "        print(\"‚úÖ Login successful!\\n\")\n",
    "        return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Fetch Fee Report Page**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_fee_report_page(session, data_url):\n",
    "    response = session.get(data_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    table = soup.find(\"table\")\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Extract Data from Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_table(table):\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        cols = [td.text.strip() for td in tr.find_all(\"td\")]\n",
    "        if cols:\n",
    "            rows.append(cols)\n",
    "\n",
    "    if not rows:\n",
    "        raise ValueError(\"No rows found in table\")\n",
    "\n",
    "    num_cols = len(rows[0])\n",
    "\n",
    "    if num_cols == 12:\n",
    "        header_row = [\n",
    "            \"SNo\", \"RecieptNo\", \"Class\", \"AdmissionNo\", \"StudentName\", \n",
    "            \"Date\", \"-\", \"Abacus / Vediic Maths\", \"TERM FEE\", \"TERM FEE2\", \n",
    "            \"ReceivedAmount\", \"Remarks\"\n",
    "        ]\n",
    "    elif num_cols == 11:\n",
    "        header_row = [\n",
    "            \"SNo\", \"RecieptNo\", \"Class\", \"AdmissionNo\", \"StudentName\", \n",
    "            \"Date\", \"-\", \"Abacus / Vediic Maths\", \"TERM FEE\", \n",
    "            \"ReceivedAmount\", \"Remarks\"\n",
    "        ]\n",
    "    else:\n",
    "        raise ValueError(f\"‚ö†Ô∏è Unexpected number of columns: {num_cols}\")\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=header_row)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_tag_data(df, academic_year):\n",
    "    df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y', errors='coerce')\n",
    "    df['AdmissionNo'] = df['AdmissionNo'].astype(str)\n",
    "    df[\"Class\"] = df[\"Class\"].str.replace(\"/\", \" - \")\n",
    "    df[\"ReceivedAmount\"] = pd.to_numeric(df[\"ReceivedAmount\"], errors=\"coerce\")\n",
    "    df[\"academic_year\"] = academic_year  # Add academic year tag\n",
    "\n",
    "    # Apply \"TERM\" row slicing only for 2024-25\n",
    "    if academic_year == \"2024-25\":\n",
    "        term_index = df[df[\"SNo\"].str.contains(\"TERM\", na=False)].index\n",
    "        if not term_index.empty:\n",
    "            df = df.iloc[:term_index[0]]\n",
    "\n",
    "    # Safely drop any of these columns only if they exist\n",
    "    cols_to_drop = [\"-\", \"Abacus / Vediic Maths\", \"TERM FEE\", \"TERM FEE2\"]\n",
    "\n",
    "    df = df.drop(columns=[col for col in cols_to_drop if col in df.columns], errors=\"ignore\")\n",
    "\n",
    "    df = df.dropna(subset=[\"Date\" ])\n",
    "\n",
    "    df\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Function to Update Database**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_database(df):\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "    # Define SQL table creation (only if not exists)\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        \"SNo\" TEXT,\n",
    "        \"RecieptNo\" TEXT,\n",
    "        \"Class\" TEXT,\n",
    "        \"AdmissionNo\" TEXT,\n",
    "        \"StudentName\" TEXT,\n",
    "        \"Date\" DATE,\n",
    "        \"ReceivedAmount\" NUMERIC,\n",
    "        \"Remarks\" TEXT,\n",
    "        \"academic_year\" TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with engine.connect() as conn:\n",
    "            conn.execute(text(create_table_sql))  # ‚úÖ Ensure table exists\n",
    "\n",
    "            conn.execute(text(\"BEGIN;\"))\n",
    "            conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME};\"))\n",
    "            conn.execute(text(\"COMMIT;\"))\n",
    "            print(f\"‚úÖ Table '{TABLE_NAME}' ensured and truncated.\\n\")\n",
    "\n",
    "            # ‚úÖ Insert DataFrame\n",
    "            df.to_sql(name=TABLE_NAME, con=engine, if_exists='append', index=False, method=\"multi\", chunksize=1000)\n",
    "            print(f\"‚úÖ {len(df)} records inserted into '{TABLE_NAME}' successfully.\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error inserting data: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for url, year in urls_with_years:\n",
    "        print(f\"üìÑ Fetching data for academic year: {year}\")\n",
    "        table = fetch_fee_report_page(session, url)\n",
    "\n",
    "        if table:\n",
    "            df = extract_data_from_table(table)\n",
    "            print(df.info())\n",
    "            \n",
    "            df = clean_and_tag_data(df, year)\n",
    "            print(f\"‚úÖ Data extracted for year {year} with {len(df)} records.\\n\")\n",
    "\n",
    "                        \n",
    "            print(df.info())\n",
    "            print(df.head())\n",
    "            df.to_csv(fr\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fee_collection_{year}.csv\", index=False)\n",
    "            all_dfs.append(df)\n",
    "        else:\n",
    "            print(f\"‚ùå Table not found for year {year}!\")\n",
    "\n",
    "    if all_dfs:\n",
    "        final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "        final_df.to_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\merged_fee_collection.csv\", index=False)\n",
    "        print(\"‚úÖ Data saved to merged_fee_collection.csv\\n\")\n",
    "\n",
    "        update_database(final_df)\n",
    "        print(\"‚úÖ Columns:\\n\", final_df.columns)\n",
    "        print(f\"‚úÖ Total {len(final_df)} records entered into database.\")\n",
    "    else:\n",
    "        print(\"‚ùå No data to process!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Run the Main Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n",
      "\n",
      "üìÑ Fetching data for academic year: 2024-25\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6618 entries, 0 to 6617\n",
      "Data columns (total 12 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   SNo                    6618 non-null   object\n",
      " 1   RecieptNo              6618 non-null   object\n",
      " 2   Class                  6614 non-null   object\n",
      " 3   AdmissionNo            6614 non-null   object\n",
      " 4   StudentName            6614 non-null   object\n",
      " 5   Date                   6614 non-null   object\n",
      " 6   -                      6614 non-null   object\n",
      " 7   Abacus / Vediic Maths  6358 non-null   object\n",
      " 8   TERM FEE               6358 non-null   object\n",
      " 9   TERM FEE2              6358 non-null   object\n",
      " 10  ReceivedAmount         5816 non-null   object\n",
      " 11  Remarks                5816 non-null   object\n",
      "dtypes: object(12)\n",
      "memory usage: 620.6+ KB\n",
      "None\n",
      "‚úÖ Data extracted for year 2024-25 with 5816 records.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5816 entries, 0 to 5815\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   SNo             5816 non-null   object        \n",
      " 1   RecieptNo       5816 non-null   object        \n",
      " 2   Class           5816 non-null   object        \n",
      " 3   AdmissionNo     5816 non-null   object        \n",
      " 4   StudentName     5816 non-null   object        \n",
      " 5   Date            5816 non-null   datetime64[ns]\n",
      " 6   ReceivedAmount  5816 non-null   float64       \n",
      " 7   Remarks         5816 non-null   object        \n",
      " 8   academic_year   5816 non-null   object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(7)\n",
      "memory usage: 409.1+ KB\n",
      "None\n",
      "  SNo RecieptNo   Class AdmissionNo                    StudentName       Date  \\\n",
      "0   1     00001  II - D       16335                    SHAIK SALMA 2024-07-15   \n",
      "1   2     00002  II - A       16365           SHEIK VISHITA KAUSAR 2024-07-15   \n",
      "2   3     00003  II - C       16516                PITTA DHASWANTH 2024-06-15   \n",
      "3   4     00004  II - B       16534  JAKKAMSETTY GNAN ANTONY CHRIS 2024-07-12   \n",
      "4   5     00005  II - A       16547                  ALIYA AASMEEN 2024-06-29   \n",
      "\n",
      "   ReceivedAmount Remarks academic_year  \n",
      "0          7150.0               2024-25  \n",
      "1          7150.0               2024-25  \n",
      "2          7150.0               2024-25  \n",
      "3          7150.0               2024-25  \n",
      "4          7150.0               2024-25  \n",
      "üìÑ Fetching data for academic year: 2025-26\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 988 entries, 0 to 987\n",
      "Data columns (total 11 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   SNo                    988 non-null    object\n",
      " 1   RecieptNo              988 non-null    object\n",
      " 2   Class                  986 non-null    object\n",
      " 3   AdmissionNo            986 non-null    object\n",
      " 4   StudentName            986 non-null    object\n",
      " 5   Date                   986 non-null    object\n",
      " 6   -                      986 non-null    object\n",
      " 7   Abacus / Vediic Maths  982 non-null    object\n",
      " 8   TERM FEE               982 non-null    object\n",
      " 9   ReceivedAmount         982 non-null    object\n",
      " 10  Remarks                982 non-null    object\n",
      "dtypes: object(11)\n",
      "memory usage: 85.0+ KB\n",
      "None\n",
      "‚úÖ Data extracted for year 2025-26 with 982 records.\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 982 entries, 0 to 981\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   SNo             982 non-null    object        \n",
      " 1   RecieptNo       982 non-null    object        \n",
      " 2   Class           982 non-null    object        \n",
      " 3   AdmissionNo     982 non-null    object        \n",
      " 4   StudentName     982 non-null    object        \n",
      " 5   Date            982 non-null    datetime64[ns]\n",
      " 6   ReceivedAmount  982 non-null    float64       \n",
      " 7   Remarks         982 non-null    object        \n",
      " 8   academic_year   982 non-null    object        \n",
      "dtypes: datetime64[ns](1), float64(1), object(7)\n",
      "memory usage: 76.7+ KB\n",
      "None\n",
      "  SNo RecieptNo   Class AdmissionNo             StudentName       Date  \\\n",
      "0   1     06659  IV - B       16795  MOHAMMAD ALINA SAMDANI 2025-06-05   \n",
      "1   2     06660   V - B       16070  DONGA YASMITH JAYA SAI 2025-06-09   \n",
      "2   3     06661   V - D       15746    NAKKA YASHA YASHMINA 2025-06-11   \n",
      "3   4     06662   I - A       17109        BURLA KARTHIKEYA 2025-06-12   \n",
      "4   5     06663  II - A       16324      CHINMAYI CHANDRANA 2025-06-12   \n",
      "\n",
      "   ReceivedAmount Remarks academic_year  \n",
      "0         16600.0               2025-26  \n",
      "1         33400.0               2025-26  \n",
      "2         17200.0               2025-26  \n",
      "3         23750.0               2025-26  \n",
      "4         16000.0               2025-26  \n",
      "‚úÖ Data saved to merged_fee_collection.csv\n",
      "\n",
      "‚úÖ Table 'daywise_fees_collection' ensured and truncated.\n",
      "\n",
      "‚úÖ 6798 records inserted into 'daywise_fees_collection' successfully.\n",
      "\n",
      "‚úÖ Columns:\n",
      " Index(['SNo', 'RecieptNo', 'Class', 'AdmissionNo', 'StudentName', 'Date',\n",
      "       'ReceivedAmount', 'Remarks', 'academic_year'],\n",
      "      dtype='object')\n",
      "‚úÖ Total 6798 records entered into database.\n"
     ]
    }
   ],
   "source": [
    "# * Run the main function\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>ATTENDANCE REPORT</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Login to Website**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ Config\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "attendance_url = \"https://app.myskoolcom.tech/kotak_vizag/admin/attedance_grid\"\n",
    "\n",
    "credentials = {\n",
    "    \"uname\": \"harikiran\",\n",
    "    \"psw\": \"812551\"\n",
    "}\n",
    "\n",
    "download_folder = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\"\n",
    "merged_output_path = os.path.join(download_folder, \"MergedAttendance_2025_26.csv\")\n",
    "\n",
    "academic_ranges = {\n",
    "    \"2025-26\": (\"2025-06-16\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "    # \"2025-26\": (\"2025-06-16\", datetime.today().strftime(\"%Y-%m-%d\"))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Logged in successfully!\n",
      "‚úÖ Logged in successfully!\n",
      "\n",
      "üìÖ Downloading attendance for 2025-26\n",
      "‚úÖ Date range set: 2025-06-16 to 2025-07-16\n",
      "‚úÖ Downloaded and renamed to: D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jun_Jul.csv\n",
      "‚úÖ Date range set: 2025-07-17 to 2025-07-27\n",
      "‚úÖ Downloaded and renamed to: D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jul_Jul.csv\n",
      "‚úÖ All attendance downloads complete ‚Äì individual files saved!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Setup Chrome Driver\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "prefs = {\"download.default_directory\": download_folder}\n",
    "chrome_options.add_experimental_option(\"prefs\", prefs)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# ‚úÖ Functions\n",
    "def login():\n",
    "    driver.get(login_url)\n",
    "    wait.until(EC.presence_of_element_located((By.NAME, \"uname\"))).send_keys(credentials[\"uname\"])\n",
    "    driver.find_element(By.NAME, \"psw\").send_keys(credentials[\"psw\"])\n",
    "    driver.find_element(By.NAME, \"psw\").send_keys(Keys.RETURN)\n",
    "    print(\"‚úÖ Logged in successfully!\")\n",
    "    time.sleep(5)\n",
    "\n",
    "def set_date_range(start, end):\n",
    "    driver.get(attendance_url)\n",
    "    time.sleep(2)\n",
    "    from_date_input = wait.until(EC.presence_of_element_located((By.ID, \"from_attendance_date\")))\n",
    "    driver.execute_script(\"arguments[0].removeAttribute('readonly')\", from_date_input)\n",
    "    from_date_input.clear()\n",
    "    from_date_input.send_keys(start)\n",
    "\n",
    "    to_date_input = wait.until(EC.presence_of_element_located((By.ID, \"to_attendance_date\")))\n",
    "    driver.execute_script(\"arguments[0].removeAttribute('readonly')\", to_date_input)\n",
    "    to_date_input.clear()\n",
    "    to_date_input.send_keys(end)\n",
    "\n",
    "    print(f\"‚úÖ Date range set: {start} to {end}\")\n",
    "\n",
    "def download_csv(filename):\n",
    "    try:\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        download_button = wait.until(EC.element_to_be_clickable((By.ID, \"smaplecsv\")))\n",
    "        download_button.click()\n",
    "        time.sleep(8)\n",
    "        downloaded = sorted(\n",
    "            [f for f in os.listdir(download_folder) if f.endswith(\".csv\")],\n",
    "            key=lambda x: os.path.getctime(os.path.join(download_folder, x)),\n",
    "            reverse=True\n",
    "        )[0]\n",
    "        os.rename(os.path.join(download_folder, downloaded), filename)\n",
    "        print(f\"‚úÖ Downloaded and renamed to: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error downloading file: {e}\")\n",
    "\n",
    "def date_batches(start, end, months=1):\n",
    "    start_date = datetime.strptime(start, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end, \"%Y-%m-%d\")\n",
    "    while start_date < end_date:\n",
    "        batch_end = min(start_date + timedelta(days=30 * months), end_date)\n",
    "        yield (start_date.strftime(\"%Y-%m-%d\"), batch_end.strftime(\"%Y-%m-%d\"))\n",
    "        start_date = batch_end + timedelta(days=1)\n",
    "\n",
    "def merge_csvs(folder, output_file, year_filter=\"2025-26\"):\n",
    "    all_csvs = [\n",
    "        os.path.join(folder, f)\n",
    "        for f in os.listdir(folder)\n",
    "        if f.endswith(\".csv\") and year_filter in f\n",
    "    ]\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "\n",
    "    for f in all_csvs:\n",
    "        try:\n",
    "            df = pd.read_csv(f, low_memory=False)\n",
    "            if \"Students Number\" in df.columns:\n",
    "                # Merge logic: remove duplicates by date + student number\n",
    "                df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "                df = df.dropna(subset=[\"Date\", \"Students Number\"])\n",
    "\n",
    "                # Merge with deduplication\n",
    "                if not merged_df.empty:\n",
    "                    merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "                    merged_df.drop_duplicates(subset=[\"Date\", \"Students Number\"], keep=\"last\", inplace=True)\n",
    "                else:\n",
    "                    merged_df = df\n",
    "                print(f\"üîÑ Merged file (with Students Number): {os.path.basename(f)}\")\n",
    "            else:\n",
    "                # Append directly if \"Students Number\" not found\n",
    "                merged_df = pd.concat([merged_df, df], ignore_index=True)\n",
    "                print(f\"‚ûï Appended file (no Students Number): {os.path.basename(f)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error reading file {f}: {e}\")\n",
    "\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Final merged file saved: {output_file}\")\n",
    "\n",
    "    # ‚úÖ MAIN Execution\n",
    "login()\n",
    "\n",
    "# ‚úÖ MAIN Execution\n",
    "login()\n",
    "\n",
    "for year, (start, end) in academic_ranges.items():\n",
    "    print(f\"\\nüìÖ Downloading attendance for {year}\")\n",
    "    for i, (s, e) in enumerate(date_batches(start, end)):\n",
    "        s_fmt = datetime.strptime(s, \"%Y-%m-%d\")\n",
    "        e_fmt = datetime.strptime(e, \"%Y-%m-%d\")\n",
    "        filename = f\"Attendance_{year}_{s_fmt.strftime('%b')}_{e_fmt.strftime('%b')}.csv\"\n",
    "        filepath = os.path.join(download_folder, filename)\n",
    "        set_date_range(s_fmt.strftime(\"%Y-%m-%d\"), e_fmt.strftime(\"%Y-%m-%d\"))\n",
    "        download_csv(filepath)\n",
    "\n",
    "# ‚ùå No merging now ‚Äì only individual files will be downloaded and renamed\n",
    "# merge_csvs(download_folder, merged_output_path, year_filter=\"2025-26\")\n",
    "\n",
    "driver.quit()\n",
    "print(\"‚úÖ All attendance downloads complete ‚Äì individual files saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 1: Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import logging\n",
    "import numpy as np\n",
    "import urllib\n",
    "import logging\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "# * Configure logging\n",
    "logging.basicConfig(filename=r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.log\", level=logging.ERROR, \n",
    "                    format=\"%(asctime)s - %(levelname)s - %(message)s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 2: Define PostgreSQl Credentials & Table Name**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# * MySQL Credentials\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "TABLE_NAME = \"attendance_report\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 3: Load and Clean Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_data(file1, file2, file3, file4=None, file5=None):\n",
    "    # Load DataFrames\n",
    "    dfs = [pd.read_csv(f) for f in [file1, file2, file3, file4, file5] if f is not None]\n",
    "\n",
    "    # Clean column names\n",
    "    for i in range(len(dfs)):\n",
    "        dfs[i].columns = dfs[i].columns.str.strip().str.replace('\"', '', regex=False)\n",
    "\n",
    "    # Ensure unique columns before merge to avoid suffix collision\n",
    "    base_df = dfs[0]\n",
    "\n",
    "    for df in dfs[1:]:\n",
    "        # Remove columns from df that would cause _x/_y clashes (except 'Students Number')\n",
    "        conflict_cols = [col for col in df.columns if col in base_df.columns and col != 'Students Number']\n",
    "        df = df.drop(columns=conflict_cols, errors='ignore')\n",
    "\n",
    "        # Merge safely\n",
    "        base_df = base_df.merge(df, on=\"Students Number\", how=\"outer\")\n",
    "\n",
    "    df = base_df\n",
    "\n",
    "    # Merge fields like Name, Class if duplicated from earlier columns\n",
    "    for field in ['Name', 'Class']:\n",
    "        col_x, col_y = f\"{field}_x\", f\"{field}_y\"\n",
    "        if col_x in df.columns and col_y in df.columns:\n",
    "            df[field] = df[col_x].combine_first(df[col_y])\n",
    "            df.drop([col_x, col_y], axis=1, inplace=True)\n",
    "        elif col_x in df.columns:\n",
    "            df[field] = df.pop(col_x)\n",
    "        elif col_y in df.columns:\n",
    "            df[field] = df.pop(col_y)\n",
    "\n",
    "    # Drop any remaining _x/_y suffix columns\n",
    "    df = df.drop(columns=[col for col in df.columns if col.endswith('_x') or col.endswith('_y')], errors='ignore')\n",
    "\n",
    "    # Rename key identifier\n",
    "    df = df.rename(columns={\"Students Number\": \"AdmissionNo\"})\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    drop_cols = ['Present Days', 'Absent Days', 'Toral Working Days']\n",
    "    df = df.drop(columns=[col for col in drop_cols if col in df.columns], errors='ignore')\n",
    "\n",
    "    # Reorder columns\n",
    "    key_cols = ['AdmissionNo', 'Name', 'Class']\n",
    "    other_cols = [col for col in df.columns if col not in key_cols]\n",
    "    df = df[key_cols + other_cols]\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **üìå Step 4: Process Attendance Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def process_attendance_data(df):\n",
    "    # * Step 1: Clean 'AdmissionNo'\n",
    "    df = df[~(df[\"AdmissionNo\"].astype(str) == \"786\") & ~df[\"AdmissionNo\"].astype(str).str.match(r\"^[a-zA-Z]\")].copy()\n",
    "\n",
    "    # * Step 2: Extract Class and Section\n",
    "    df[\"Class\"] = df[\"Class\"].astype(str).str.replace(r\"ICSE \\((.*?)\\)\", r\"\\1\", regex=True)\n",
    "\n",
    "    # * Load students with academic_year info\n",
    "    student_df = pd.read_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\fees_report.csv\")[[\"adm_no\", \"academic_year\"]]\n",
    "    print(\"‚úÖ Students Before Merging\\n\", len(df[\"AdmissionNo\"].unique()))\n",
    "\n",
    "    # * Keep only students of academic_year 2024-25\n",
    "    student_df_2024_25 = student_df[student_df[\"academic_year\"] == \"2024-25\"]\n",
    "    student_df_2025_26 = student_df[student_df[\"academic_year\"] == \"2025-26\"]\n",
    "\n",
    "    df = df[df[\"AdmissionNo\"].isin(student_df[\"adm_no\"])]\n",
    "    print(\"‚úÖ Students After Merging\\n\", len(df[\"AdmissionNo\"].unique()))\n",
    "\n",
    "    # * Step 3: Unpivot DataFrame\n",
    "    df_unpivot = pd.melt(df, id_vars=[\"AdmissionNo\", \"Name\", \"Class\"], var_name=\"Date\", value_name=\"AttendanceStatus\")\n",
    "    df_unpivot[\"Date\"] = pd.to_datetime(df_unpivot[\"Date\"], format='%d.%m.%Y', errors='coerce')\n",
    "    df_unpivot[\"id\"] = range(1, len(df_unpivot) + 1)\n",
    "\n",
    "    if df_unpivot[\"Date\"].isna().sum() > 0:\n",
    "        print(\"‚ö†Ô∏è Warning: Some Date values were invalid and converted to NaT.\")\n",
    "\n",
    "    df_unpivot = df_unpivot.sort_values(\"Date\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "    first_attendance_dates = df_unpivot[df_unpivot['AttendanceStatus'].notna()].groupby('AdmissionNo')['Date'].min()\n",
    "    df_unpivot['AttendanceStatus'] = df_unpivot.apply(\n",
    "        lambda row: \"Not Joined\" if row['Date'] < first_attendance_dates.get(row['AdmissionNo'], row['Date']) else row['AttendanceStatus'],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    priority_map = {'P': 2, 'A': 1, 'H': 3, 'Not Joined': 4, 'TC': 5}\n",
    "    df_unpivot['Priority'] = df_unpivot[\"AttendanceStatus\"].map(priority_map)\n",
    "    df_unpivot = df_unpivot.sort_values(by=['AdmissionNo', 'Date', 'Priority']) \\\n",
    "                           .drop_duplicates(subset=['AdmissionNo', 'Date'], keep='first') \\\n",
    "                           .drop(columns=['Priority'])\n",
    "\n",
    "    df_unpivot = df_unpivot[[\"id\", 'Date', 'AdmissionNo', 'Name', 'Class', 'AttendanceStatus']]\n",
    "    df_unpivot.sort_values(by=['Date'], ascending=False, inplace=True)\n",
    "    df_unpivot['Class'] = df_unpivot['Class'].str.replace(\"Pre KG - \", \"Pre KG\")\n",
    "\n",
    "    df_unpivot[\"AttendanceStatus\"] = df_unpivot[\"AttendanceStatus\"].replace({\n",
    "        'P': \"Present\", 'A': \"Absent\", 'H': \"Holiday\"\n",
    "    })\n",
    "\n",
    "    # * Step X: Add Academic Year\n",
    "    df_unpivot['academic_year'] = df_unpivot['Date'].apply(\n",
    "        lambda d: \"2024-25\" if pd.Timestamp(\"2024-07-17\") <= d <= pd.Timestamp(\"2025-03-31\")\n",
    "        else \"2025-26\" if pd.Timestamp(\"2025-06-16\") <= d <= pd.Timestamp(datetime.today().date())\n",
    "        else \"\"\n",
    "    )\n",
    "\n",
    "    # * Step X: Class & Section Mappings\n",
    "    class_mapping_2024_25 = {\n",
    "        \"Pre KG\": 1, \"LKG - A\": 2, \"LKG - B\": 3, \"UKG - A\": 4, \"UKG - B\": 5, \"UKG - C\": 6,\n",
    "        \"I - A\": 7, \"I - B\": 8, \"I - C\": 9, \"I - D\": 10, \"II - A\": 11, \"II - B\": 12, \"II - C\": 13, \"II - D\": 14,\n",
    "        \"III - A\": 15, \"III - B\": 16, \"III - C\": 17, \"III - D\": 18, \"IV - A\": 19, \"IV - B\": 20, \"IV - C\": 21, \"IV - D\": 22,\n",
    "        \"V - A\": 23, \"V - B\": 24, \"V - C\": 25, \"V - D\": 26, \"VI - A\": 27, \"VI - B\": 28, \"VI - C\": 29, \"VI - D\": 30,\n",
    "        \"VII - A\": 31, \"VII - B\": 32, \"VII - C\": 33, \"VII - D\": 34, \"VIII - A\": 35, \"VIII - B\": 36, \"VIII - C\": 37,\n",
    "        \"IX - A\": 38, \"IX - B\": 39, \"IX - C\": 40, \"X - A\": 41, \"X - B\": 42, \"X - C\": 43\n",
    "    }\n",
    "\n",
    "    class_mapping_2025_26 = {\n",
    "        \"Pre KG\": 1, \"LKG - A\": 2, \"LKG - B\": 3, \"UKG - A\": 4, \"UKG - B\": 5, \"UKG - C\": 6,\n",
    "        \"I - A\": 7, \"I - B\": 8, \"I - C\": 9,  # no I - D\n",
    "        \"II - A\": 10, \"II - B\": 11, \"II - C\": 12, \"II - D\": 13,\n",
    "        \"III - A\": 14, \"III - B\": 15, \"III - C\": 16, \"III - D\": 17,\n",
    "        \"IV - A\": 18, \"IV - B\": 19, \"IV - C\": 20, \"IV - D\": 21,\n",
    "        \"V - A\": 22, \"V - B\": 23, \"V - C\": 24, \"V - D\": 25,\n",
    "        \"VI - A\": 26, \"VI - B\": 27, \"VI - C\": 28, \"VI - D\": 29,\n",
    "        \"VII - A\": 30, \"VII - B\": 31, \"VII - C\": 32, \"VII - D\": 33,\n",
    "        \"VIII - A\": 34, \"VIII - B\": 35, \"VIII - C\": 36, \"VIII - D\": 37,\n",
    "        \"IX - A\": 38, \"IX - B\": 39, \"IX - C\": 40,\n",
    "        \"X - A\": 41, \"X - B\": 42, \"X - C\": 43\n",
    "    }\n",
    "\n",
    "    # * Apply correct mapping based on academic year\n",
    "    def map_class_no(row):\n",
    "        if row['academic_year'] == '2024-25':\n",
    "            return class_mapping_2024_25.get(row['Class'], np.nan)\n",
    "        elif row['academic_year'] == '2025-26':\n",
    "            return class_mapping_2025_26.get(row['Class'], np.nan)\n",
    "        return np.nan\n",
    "\n",
    "    df_unpivot['ClassNo'] = df_unpivot.apply(map_class_no, axis=1)\n",
    "\n",
    "    print(\"‚úÖ ClassNo assigned based on academic year and Class.\")\n",
    "    print(df_unpivot[df_unpivot[['Class', 'ClassNo', 'academic_year']].isna()])\n",
    "\n",
    "    # * Step 14‚Äì17: Same as before...\n",
    "    grade_mapping = [\n",
    "        (\"Pre KG\", 1), (\"LKG\", 2), (\"UKG\", 3),\n",
    "        (\"I\", 4), (\"II\", 5), (\"III\", 6), (\"IV\", 7), (\"V\", 8),\n",
    "        (\"VI\", 9), (\"VII\", 10), (\"VIII\", 11), (\"IX\", 12), (\"X\", 13)\n",
    "    ]\n",
    "    conditions = [df_unpivot['Class'].str.contains(fr\"\\b{k}\\b\", na=False, regex=True) for k, _ in grade_mapping]\n",
    "    choices = [v for _, v in grade_mapping]\n",
    "    df_unpivot['classId'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "    AttendanceStatus_mapping = [(\"Absent\", 1), (\"Present\", 2), (\"Not Joined\", 3), (\"Holiday\", 4)]\n",
    "    conditions = [df_unpivot['AttendanceStatus'].str.contains(k, na=False) for k, _ in AttendanceStatus_mapping]\n",
    "    choices = [v for _, v in AttendanceStatus_mapping]\n",
    "    df_unpivot['AttendanceStatusId'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "    branch_mapping = [\n",
    "        ('Pre KG', 1), ('LKG', 1), ('UKG', 1),\n",
    "        ('I', 2), ('II', 2), ('III', 2), ('IV', 2), ('V', 2),\n",
    "        ('VI', 3), ('VII', 3), ('VIII', 3), ('IX', 3), ('X', 3)\n",
    "    ]\n",
    "    conditions = [df_unpivot['Class'].str.contains(fr\"\\b{k}\\b\", na=False, regex=True) for k, _ in branch_mapping]\n",
    "    choices = [v for _, v in branch_mapping]\n",
    "    df_unpivot['branchId'] = np.select(conditions, choices, default=0)\n",
    "\n",
    "    # ‚úÖ Final output: lowercase and return\n",
    "    df_unpivot = df_unpivot[[\"id\", 'Date', 'AdmissionNo', 'ClassNo', 'classId', 'branchId', 'AttendanceStatusId', 'academic_year']]\n",
    "    df_unpivot.columns = [c.lower() for c in df_unpivot.columns]\n",
    "    return df_unpivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 5: Insert Data into PostgreSQL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "def ensure_table_exists():\n",
    "    create_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        date DATE,\n",
    "        admissionno TEXT,\n",
    "        attendancestatusid INTEGER,\n",
    "        academic_year TEXT NOT NULL\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as connection:  # ‚úÖ ensures DDL is committed\n",
    "            connection.execute(text(create_table_sql))\n",
    "        print(f\"‚úÖ Table '{TABLE_NAME}' ensured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Failed to create or check table '{TABLE_NAME}': {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create database engine\n",
    "password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "    f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    ")\n",
    "\n",
    "def update_database(df):\n",
    "    \"\"\"Use PostgreSQL COPY for ultra-fast data insertion.\"\"\"\n",
    "    csv_path = (r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.csv\")\n",
    "\n",
    "    # ‚úÖ Ensure column names are lowercase to match table definition\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # ‚úÖ Save DataFrame to CSV\n",
    "    df.to_csv(csv_path, index=False, header=False)\n",
    "\n",
    "    try:\n",
    "        conn = engine.raw_connection()\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        print(f\"üîÑ Truncating table: {TABLE_NAME}\")\n",
    "        cursor.execute(f\"TRUNCATE TABLE {TABLE_NAME};\")\n",
    "        conn.commit()\n",
    "\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            cursor.copy_from(f, TABLE_NAME, sep=\",\")  # ‚úÖ lowercase and unquoted\n",
    "\n",
    "        conn.commit()\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "        print(f\"‚úÖ Data copied to '{TABLE_NAME}' using COPY command!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå COPY failed: {e}\")\n",
    "        logging.error(traceback.format_exc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **üìå Step 6: Run the Full Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n",
      "\n",
      "‚úÖ Data loaded with 1989 rows.\n",
      "\n",
      "Processing attendance data...\n",
      "\n",
      "‚úÖ Students Before Merging\n",
      " 1969\n",
      "‚úÖ Students After Merging\n",
      " 1939\n",
      "‚úÖ ClassNo assigned based on academic year and Class.\n",
      "‚úÖ Processed data with 560371 rows.\n",
      "\n",
      "‚úÖ Columns are:\n",
      " Index(['id', 'date', 'admissionno', 'attendancestatusid', 'academic_year'], dtype='object')\n",
      "2025-07-16 00:00:00\n",
      "          id       date admissionno  attendancestatusid academic_year\n",
      "0     560371 2025-07-16       17371                   2       2025-26\n",
      "256   559324 2025-07-16       16162                   2       2025-26\n",
      "1363  560188 2025-07-16       17188                   2       2025-26\n",
      "580   558615 2025-07-16       14510                   1       2025-26\n",
      "230   558995 2025-07-16       15523                   2       2025-26\n",
      "Updating database...\n",
      "\n",
      "‚úÖ Table 'attendance_report' ensured.\n",
      "üîÑ Truncating table: attendance_report\n",
      "‚úÖ Data copied to 'attendance_report' using COPY command!\n",
      "‚úÖ Data updated successfully!\n",
      "\n",
      "‚úÖ Attendance report processing completed successfully!\n",
      "\n",
      "‚úÖ No of Rows: 560371\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    file1 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\AttendanceReportUptoSeptember_2024_25.csv\"\n",
    "    file2 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\AttendanceOctoberToDecember_2024_25.csv\"\n",
    "    file3 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\AttendanceUptoMarch_2024_25.csv\"\n",
    "    file4 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jun_Jul.csv\"\n",
    "    file5 = r\"D:\\GITHUB\\kotak-school-dbms\\source_data\\Attendance Reports\\Attendance_2025-26_Jun_Jul.csv\"\n",
    "    output_file = r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\attendance_report.csv\"\n",
    "    \n",
    "    try:\n",
    "        print(\"Loading and cleaning data...\\n\")\n",
    "        df = load_and_clean_data(file1, file2, file3, file4, file5)\n",
    "        print(f\"‚úÖ Data loaded with {df.shape[0]} rows.\\n\")\n",
    "        \n",
    "        print(\"Processing attendance data...\\n\")\n",
    "        df_unpivot = process_attendance_data(df)\n",
    "        df_unpivot.to_csv(output_file, index=False)\n",
    "        print(f\"‚úÖ Processed data with {df_unpivot.shape[0]} rows.\\n\")\n",
    "        print(\"‚úÖ Columns are:\\n\", df_unpivot.columns)\n",
    "        print(max(df_unpivot[\"date\"]))\n",
    "        print((df_unpivot.head()))\n",
    "                \n",
    "        print(\"Updating database...\\n\")\n",
    "        ensure_table_exists()\n",
    "        update_database(df_unpivot)\n",
    "        print(\"‚úÖ Data updated successfully!\\n\")\n",
    "\n",
    "        print(\"‚úÖ Attendance report processing completed successfully!\\n\")\n",
    "        print(f\"‚úÖ No of Rows: {df_unpivot.shape[0]}\\n\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An unexpected error occurred. Error: {e}\\n\")\n",
    "        logging.error(f\"‚ùå Unexpected error: {e}\\n\")\n",
    "\n",
    "\n",
    "# * Run the script\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>Class Table</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "TABLE_NAME = \"class_table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ClassNo</th>\n",
       "      <th>Class</th>\n",
       "      <th>classId</th>\n",
       "      <th>className</th>\n",
       "      <th>branchId</th>\n",
       "      <th>branchName</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Pre KG</td>\n",
       "      <td>1</td>\n",
       "      <td>Pre KG</td>\n",
       "      <td>1</td>\n",
       "      <td>Kindergarten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>LKG - A</td>\n",
       "      <td>2</td>\n",
       "      <td>LKG</td>\n",
       "      <td>1</td>\n",
       "      <td>Kindergarten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>LKG - B</td>\n",
       "      <td>2</td>\n",
       "      <td>LKG</td>\n",
       "      <td>1</td>\n",
       "      <td>Kindergarten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>UKG - A</td>\n",
       "      <td>3</td>\n",
       "      <td>UKG</td>\n",
       "      <td>1</td>\n",
       "      <td>Kindergarten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>UKG - B</td>\n",
       "      <td>3</td>\n",
       "      <td>UKG</td>\n",
       "      <td>1</td>\n",
       "      <td>Kindergarten</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ClassNo    Class  classId className  branchId    branchName\n",
       "0        1   Pre KG        1    Pre KG         1  Kindergarten\n",
       "1        2  LKG - A        2       LKG         1  Kindergarten\n",
       "2        3  LKG - B        2       LKG         1  Kindergarten\n",
       "3        4  UKG - A        3       UKG         1  Kindergarten\n",
       "4        5  UKG - B        3       UKG         1  Kindergarten"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"D:\\GITHUB\\kotak-school-dbms\\output_data\\class_section_grade_table.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ClassNo', 'Class', 'classId', 'className', 'branchId', 'branchName'], dtype='object')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import traceback\n",
    "import logging\n",
    "import pandas as pd\n",
    "import urllib\n",
    "import io\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import OperationalError\n",
    "\n",
    "# Retry settings\n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 5  # Seconds\n",
    "\n",
    "def bulk_insert_postgres(df, conn, table_name):\n",
    "    \"\"\"Fast bulk insert using PostgreSQL COPY command.\"\"\"\n",
    "    with conn.connection.cursor() as cur:\n",
    "        output = io.StringIO()\n",
    "        df.to_csv(output, sep=\"\\t\", index=False, header=False)\n",
    "        output.seek(0)\n",
    "        cur.copy_from(output, table_name, sep=\"\\t\", null=\"NULL\")\n",
    "        conn.connection.commit()\n",
    "\n",
    "def update_database(df):\n",
    "    \"\"\"Insert attendance data into PostgreSQL database with retry logic.\"\"\"\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    engine = create_engine(f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "                           f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\")\n",
    "\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            print(f\"üîÑ Attempt {attempt}: Connecting to database {POSTGRES_CREDENTIALS['database']} at {POSTGRES_CREDENTIALS['host']}...\")\n",
    "            with engine.begin() as conn:\n",
    "                print(f\"‚úÖ Connection established.\")\n",
    "\n",
    "                # Create Table if it does not exist\n",
    "                print(f\"Checking if table '{TABLE_NAME}' exists...\")\n",
    "                \n",
    "                # Truncate the table before inserting data\n",
    "                print(f\"Truncating existing table: {TABLE_NAME}\")\n",
    "                conn.execute(text(f\"TRUNCATE TABLE {TABLE_NAME} CASCADE;\"))\n",
    "                \n",
    "                print(f\"Deleting data from {TABLE_NAME} table...\")\n",
    "                conn.execute(text(f\"DELETE FROM {TABLE_NAME};\"))\n",
    "\n",
    "\n",
    "                # Fast Bulk Insert\n",
    "                print(f\"Inserting data into {TABLE_NAME} table...\")\n",
    "                bulk_insert_postgres(df, conn, TABLE_NAME)\n",
    "\n",
    "                print(f\"‚úÖ Data successfully inserted into '{TABLE_NAME}' table.\")\n",
    "                return  # Exit function if successful\n",
    "\n",
    "        except OperationalError as e:\n",
    "            print(f\"‚ùå OperationalError: {e}\")\n",
    "            logging.error(f\"‚ùå OperationalError: {e}\")\n",
    "            logging.error(\"Error Traceback:\\n\" + traceback.format_exc())\n",
    "\n",
    "            if attempt < MAX_RETRIES:\n",
    "                print(f\"üîÑ Retrying in {RETRY_DELAY} seconds...\")\n",
    "                time.sleep(RETRY_DELAY)\n",
    "            else:\n",
    "                print(\"‚ùå Max retries reached. Could not update the database.\")\n",
    "                logging.error(\"‚ùå Max retries reached. Could not update the database.\")\n",
    "                return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Attempt 1: Connecting to database schooldb at localhost...\n",
      "‚úÖ Connection established.\n",
      "Checking if table 'class_table_2024_25' exists...\n",
      "Truncating existing table: class_table_2024_25\n",
      "Deleting data from class_table_2024_25 table...\n",
      "Inserting data into class_table_2024_25 table...\n",
      "‚úÖ Data successfully inserted into 'class_table_2024_25' table.\n"
     ]
    }
   ],
   "source": [
    "update_database(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE COLLECTION REPORT 2024-25</b></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import logging\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Logging\n",
    "logging.basicConfig(filename=\"fee_collection_merge.log\", level=logging.ERROR)\n",
    "\n",
    "# üîê Credentials & URLs\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "credentials = {\"uname\": \"harikiran\", \"psw\": \"812551\"}\n",
    "urls = {\n",
    "    \"2024_25\": \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_consolidate_report_print?&from=2024-04-01&academic_years_id=7&status=1&imageField=Search\",\n",
    "    \"2025_26\": \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_consolidate_report_print?&from=2025-04-01&academic_years_id=1&status=1&imageField=Search\"\n",
    "}\n",
    "\n",
    "# üõ†Ô∏è PostgreSQL Config\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "TABLE_NAME = \"fees_collection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üîå Create Engine\n",
    "def get_engine():\n",
    "    password = urllib.parse.quote(POSTGRES_CREDENTIALS[\"password\"])\n",
    "    return create_engine(\n",
    "        f\"postgresql+psycopg2://{POSTGRES_CREDENTIALS['username']}:{password}\"\n",
    "        f\"@{POSTGRES_CREDENTIALS['host']}:{POSTGRES_CREDENTIALS['port']}/{POSTGRES_CREDENTIALS['database']}\"\n",
    "    )\n",
    "\n",
    "# üîë Login\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    response = session.post(login_url, data=credentials)\n",
    "    if \"Invalid\" in response.text:\n",
    "        print(\"‚ùå Login failed!\")\n",
    "        return None\n",
    "    print(\"‚úÖ Login successful!\")\n",
    "    return session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßæ Convert HTML table to DataFrame\n",
    "def table_to_dataframe(table):\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
    "    rows = [[td.get_text(strip=True) for td in tr.find_all(\"td\")] for tr in table.find_all(\"tr\")[1:]]\n",
    "    return pd.DataFrame(rows, columns=headers) if rows else None\n",
    "\n",
    "# üì• Fetch fee table from a given URL\n",
    "def fetch_fee_table(session, url):\n",
    "    response = session.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    tables = soup.find_all(\"table\", class_=\"b-t\")\n",
    "    all_data = []\n",
    "\n",
    "    for table in tables:\n",
    "        df = table_to_dataframe(table)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n",
    "\n",
    "\n",
    "# üßπ Clean data\n",
    "def clean_data(df, academic_year):\n",
    "    df = df[~df.iloc[:, 0].astype(str).str.startswith(\"Total\", na=False)].copy()\n",
    "    df[\"Admin No.\"] = df[\"Admin No.\"].astype(str)\n",
    "    \n",
    "    df.columns = ['SNo', 'AdmissionNo', 'Name', 'Abacus1', 'TermFee1',\n",
    "                  'Total_Fees', 'Abacus2', 'TermFee2',\n",
    "                  'Total_Fee_Paid', 'Discount_Concession', 'Total_Due']\n",
    "    \n",
    "    numeric_columns = [\"Total_Fees\", \"Total_Fee_Paid\", \"Discount_Concession\", \"Total_Due\"]\n",
    "    for col in numeric_columns:\n",
    "        df[col] = df[col].astype(str).str.replace(\",\", \"\").replace([\"\", \"None\", \"nan\", \"NaN\", np.nan], 0)\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "    df = df.drop(columns=[\"SNo\", \"Abacus1\", \"Abacus2\", \"TermFee1\", \"TermFee2\"])\n",
    "    df[\"academic_year\"] = academic_year  # Add source year column\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import text\n",
    "\n",
    "def ensure_fees_collection_table(engine):\n",
    "    create_table_sql = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fees_collection (\n",
    "        id SERIAL PRIMARY KEY,\n",
    "        admissionno TEXT,\n",
    "        name TEXT,\n",
    "        total_fees INTEGER,\n",
    "        total_fee_paid INTEGER,\n",
    "        discount_concession INTEGER,\n",
    "        total_due INTEGER,\n",
    "        academic_year TEXT\n",
    "    );\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            conn.execute(text(create_table_sql))\n",
    "        print(\"‚úÖ Table 'fees_collection' ensured.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating table: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üõ¢Ô∏è Insert into PostgreSQL\n",
    "def update_database(df, table_name):\n",
    "    engine = get_engine()\n",
    "    try:\n",
    "        with engine.begin() as conn:\n",
    "            print(f\"‚ö†Ô∏è Deleting old records from '{table_name}'...\")\n",
    "            conn.execute(text(f\"DELETE FROM {table_name};\"))\n",
    "            print(f\"‚úÖ Table '{table_name}' cleared.\")\n",
    "        df.columns = df.columns.str.lower()\n",
    "        print(f\"üì• Inserting {len(df)} rows...\")\n",
    "        df.to_sql(name=table_name, con=engine, if_exists='append', index=False, method='multi', chunksize=1000)\n",
    "        print(f\"‚úÖ Inserted into '{table_name}' successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error inserting: {e}\")\n",
    "        logging.error(f\"Database insert error: {e}\")\n",
    "    finally:\n",
    "        engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n",
      "\n",
      "üîÑ Fetching data for 2024_25...\n",
      "\n",
      "üîÑ Fetching data for 2025_26...\n",
      "üìÅ Saved to merged_fee_collection.csv\n",
      "‚úÖ Table 'fees_collection' ensured.\n",
      "‚úÖ Fees collection table ensured.\n",
      "‚ö†Ô∏è Deleting old records from 'fees_collection'...\n",
      "‚úÖ Table 'fees_collection' cleared.\n",
      "üì• Inserting 3228 rows...\n",
      "‚úÖ Inserted into 'fees_collection' successfully.\n",
      "‚úÖ All done! Total records: 3228\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Main Logic\n",
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "\n",
    "    merged_df = pd.DataFrame()\n",
    "    \n",
    "    for year, url in urls.items():\n",
    "        print(f\"\\nüîÑ Fetching data for {year}...\")\n",
    "        raw_df = fetch_fee_table(session, url)\n",
    "        if raw_df.empty:\n",
    "            print(f\"‚ùå No data for {year}!\")\n",
    "            continue\n",
    "        clean_df = clean_data(raw_df, academic_year=year)\n",
    "        merged_df = pd.concat([merged_df, clean_df], ignore_index=True)\n",
    "\n",
    "    if merged_df.empty:\n",
    "        print(\"‚ùå No data collected from any year!\")\n",
    "        return\n",
    "\n",
    "    # Save CSV (optional)\n",
    "    merged_df.to_csv(\"merged_fee_collection.csv\", index=False)\n",
    "    print(\"üìÅ Saved to merged_fee_collection.csv\")\n",
    "\n",
    "    # Ensure table exists\n",
    "    engine = get_engine()\n",
    "    ensure_fees_collection_table(engine)\n",
    "    print(\"‚úÖ Fees collection table ensured.\")\n",
    "    # Push to DB\n",
    "    update_database(merged_df, TABLE_NAME)\n",
    "    print(f\"‚úÖ All done! Total records: {len(merged_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\"><b>FEE CONCESSION REPORT 2024-25</b></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.dialects.postgresql import insert\n",
    "from datetime import date\n",
    "\n",
    "\n",
    "## **Define Login Credentials and MySQL Credentials*\n",
    "login_url = \"https://app.myskoolcom.tech/kotak_vizag/login\"\n",
    "data_url = \"https://app.myskoolcom.tech/kotak_vizag/office_fee/fee_discounts_report_receipt_wise_print?&academic_years_id=1\"\n",
    "\n",
    "credentials = {\n",
    "    \"uname\": \"harikiran\",\n",
    "    \"psw\": \"812551\"\n",
    "}\n",
    "\n",
    "POSTGRES_CREDENTIALS = {\n",
    "    \"username\": \"postgres\",\n",
    "    \"password\": \"Hari@123\",\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": \"5432\",\n",
    "    \"database\": \"kotakschooldb\",\n",
    "}\n",
    "\n",
    "TABLE_NAME = \"fee_concession_2024_25\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Function to Log in to Website*\n",
    "def login_to_website():\n",
    "    session = requests.Session()\n",
    "    login_response = session.post(login_url, data=credentials)\n",
    "    \n",
    "    if login_response.status_code != 200:\n",
    "        print(\"‚ùå Login request failed! Server error.\\n\")\n",
    "        return None\n",
    "    \n",
    "    soup = BeautifulSoup(login_response.text, \"html.parser\")\n",
    "    if soup.find(\"div\", class_=\"alert-danger\"):\n",
    "        print(\"‚ùå Login failed! Check credentials.\\n\")\n",
    "        return None\n",
    "    \n",
    "    print(\"‚úÖ Login successful!\\n\")\n",
    "    return session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Function to Fetch All Fee Tables*\n",
    "def fetch_all_concession_tables(session):\n",
    "    response = session.get(data_url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "    \n",
    "    concession_tables = soup.find_all(\"table\", class_=\"table_view\")\n",
    "    if not concession_tables:\n",
    "        print(\"‚ùå No fee tables found! The page structure may have changed.\")\n",
    "        return None\n",
    "\n",
    "    all_data = []\n",
    "    for table in concession_tables:\n",
    "        df = table_to_dataframe(table)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"‚ùå No data extracted from tables.\")\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## **Function to Convert HTML Table to DataFrame*\n",
    "def table_to_dataframe(table):\n",
    "    headers = [th.get_text(strip=True) for th in table.find_all(\"th\")]\n",
    "    if len(headers) > 8:\n",
    "        headers = headers[:8]  # * Keep only the first 8 columns\n",
    "    \n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\")[1:]:\n",
    "        cells = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "        if len(cells) >= 8:\n",
    "            rows.append(cells[:8])  # * Keep only the first 8 columns\n",
    "    \n",
    "    df = pd.DataFrame(rows, columns=headers) if rows else None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def clean_data(df):\n",
    "    # Standardize column names: lowercase, replace spaces with underscores\n",
    "    df.columns = df.columns.str.strip().str.replace(\" \", \"_\").str.lower()\n",
    "    \n",
    "    # Drop rows where 'student_number' is missing (ensuring data consistency)\n",
    "    df = df.dropna(subset=[\"student_number\"])\n",
    "    df[\"student_number\"] = df[\"student_number\"].astype(str).str.strip()\n",
    "\n",
    "    # Convert 'discount_given' to numeric, handling errors gracefully\n",
    "    df[\"discount_given\"] = pd.to_numeric(df[\"discount_given\"], errors=\"coerce\").fillna(0.00)\n",
    "\n",
    "    # Drop extra unnecessary columns if they exist\n",
    "    df.drop(columns=['receipt_no', 'fee_name', 'fee_amount', 'total_due_amount'], errors=\"ignore\", inplace=True)\n",
    "    \n",
    "    df[\"date\"] = df[\"date\"].astype(str).str.strip()  # Remove spaces\n",
    "\n",
    "    # Ensure 'date' is in correct format & drop invalid entries\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"], errors=\"coerce\").dt.date\n",
    "    df = df.dropna(subset=[\"date\"])  # Drop rows with invalid dates\n",
    "    \n",
    "    # Add an ID column (Auto-incremental)\n",
    "    df[\"id\"] = range(1, len(df) + 1)\n",
    "\n",
    "    # Reorder columns for consistency\n",
    "    df = df[['id', 'date', 'student_number', 'student_name', 'discount_given']]\n",
    "\n",
    "    # Reset index (ensures DataFrame integrity)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import urllib.parse\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "def update_database(df: pd.DataFrame, table_name: str, postgres_credentials: dict):\n",
    "    \"\"\"Insert fee collection data into PostgreSQL efficiently, fixing column name issues.\"\"\"\n",
    "\n",
    "    # Encode password to handle special characters safely\n",
    "    password = urllib.parse.quote(postgres_credentials[\"password\"])\n",
    "\n",
    "    # Create PostgreSQL connection engine\n",
    "    engine = create_engine(\n",
    "        f\"postgresql+psycopg2://{postgres_credentials['username']}:{password}\"\n",
    "        f\"@{postgres_credentials['host']}:{postgres_credentials['port']}/{postgres_credentials['database']}\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        with engine.begin() as conn:  # Auto-commits or rollbacks if an error occurs\n",
    "            print(f\"üîÑ Connecting to database {postgres_credentials['database']} at {postgres_credentials['host']}...\")\n",
    "\n",
    "            # ‚úÖ Standardize column names to lowercase\n",
    "            df.columns = df.columns.str.lower()\n",
    "            print(\"‚úÖ Fixed column names:\", df.columns.tolist())\n",
    "\n",
    "            # üîπ Use DELETE instead of TRUNCATE for transactional safety\n",
    "            print(f\"‚ö†Ô∏è Deleting existing records from: {table_name}\")\n",
    "            conn.execute(text(f\"DELETE FROM {table_name};\"))\n",
    "            print(f\"‚úÖ Table '{table_name}' cleared.\\n\")\n",
    "\n",
    "        # üì• Insert Data Efficiently\n",
    "        print(f\"üì• Inserting data into {table_name} table...\")\n",
    "        df.to_sql(name=table_name, con=engine, if_exists=\"append\", index=False, method=\"multi\", chunksize=1000)\n",
    "\n",
    "        print(f\"‚úÖ Data successfully inserted into '{table_name}' table.\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error updating database: {e}\", exc_info=True)\n",
    "        print(f\"‚ùå Error occurred while updating database: {e}\")\n",
    "\n",
    "    finally:\n",
    "        engine.dispose()  # Ensure connection is closed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Login successful!\n",
      "\n",
      "‚úÖ Data extracted successfully! Cleaning data...\n",
      "\n",
      "‚úÖ Data cleaned successfully!\n",
      "\n",
      "‚úÖ Columns are:\n",
      " Index(['id', 'date', 'student_number', 'student_name', 'discount_given'], dtype='object')\n",
      "id                  int64\n",
      "date               object\n",
      "student_number     object\n",
      "student_name       object\n",
      "discount_given    float64\n",
      "dtype: object\n",
      "‚úÖ Data saved to 'D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fee_concession_report_2024_25.csv'\n",
      "\n",
      "üîÑ Connecting to database schooldb at localhost...\n",
      "‚úÖ Fixed column names: ['id', 'date', 'student_number', 'student_name', 'discount_given']\n",
      "‚ö†Ô∏è Deleting existing records from: fee_concession_2024_25\n",
      "‚úÖ Table 'fee_concession_2024_25' cleared.\n",
      "\n",
      "üì• Inserting data into fee_concession_2024_25 table...\n",
      "‚úÖ Data successfully inserted into 'fee_concession_2024_25' table.\n",
      "\n",
      "‚úÖ 122 records entered into the database\n",
      "      id        date student_number                                           student_name  discount_given\n",
      "0      1  2024-07-27          15660                           YAASHVAN POTHIRENDI (IV - A)           500.0\n",
      "1      2  2024-07-27          16070                        YASMITH JAYA SAI DONGA (IV - A)           500.0\n",
      "2      3  2024-07-27          16105                              GEETIKA JAGARANA (IV - A)           500.0\n",
      "3      4  2024-07-27          16165                               SUMUEL NAGARAPU (IX - B)           500.0\n",
      "4      5  2024-07-27          15018                                 ESTHER GADHAM (IX - C)           500.0\n",
      "5      6  2024-07-27          15363                             GEETHIKA MADAGALA (IX - C)           500.0\n",
      "6      7  2024-07-27          16921                                 SAANVI AKULA (LKG - A)           500.0\n",
      "7      8  2024-07-27          16970                                SATHWIK DUMMU (LKG - A)           500.0\n",
      "8      9  2024-07-27          16985                    MONISH SIVA KUMAR TEKKALI (LKG - B)           500.0\n",
      "9     10  2024-07-27          16779                   SANJAY SAMRAT KUMAR RAVURI (UKG - B)           500.0\n",
      "10    11  2024-07-27          16967                               JATIN TADITURI (UKG - B)           500.0\n",
      "11    12  2024-07-27          15213              PRAERAN HARISH KUMAR RAGHUPATRUNI (V - A)           500.0\n",
      "12    13  2024-07-27          15532                         SURYA CHARAN THUPAKULA (V - B)           500.0\n",
      "13    14  2024-07-27          15526                               AARADHYA NISTALA (V - C)           500.0\n",
      "14    15  2024-07-27          16765                               REVANTH KADIYALA (V - C)           500.0\n",
      "15    16  2024-07-27          15048              VENKATA SAI KARTHIKEYA VANAPALLI (VI - A)           500.0\n",
      "16    17  2024-07-27          15261                             CHARISHMA NISTALA (VI - A)           500.0\n",
      "17    18  2024-07-27          16499                   AMRUTHA SESHAKUMARI CHAVALI (VI - A)           500.0\n",
      "18    19  2024-07-27          16416                              ANANYA VASUPALLI (VI - D)           500.0\n",
      "19    20  2024-07-27          15278                                JOSEPH UPPADA (VII - A)           500.0\n",
      "20    21  2024-07-27          17014                      JAI KRISHNA KIRLAMPALLI (VII - A)           500.0\n",
      "21    22  2024-07-27          14497                             DEERAJ SARIPILLI (VII - C)           500.0\n",
      "22    23  2024-07-27          14861                   CHARANYA PRANITHA TUPAKULA (VII - C)           500.0\n",
      "23    24  2024-07-27          15019                                HANNAH GADHAM (VII - C)           500.0\n",
      "24    25  2024-07-27          15277                               ANJALI UPPADA (VIII - A)           500.0\n",
      "25    26  2024-07-27          14502                            SUDIKSHA NEELAPU (VIII - C)           500.0\n",
      "26    27  2024-07-27          16132                                PRANITHA PALURI (X - A)           500.0\n",
      "27    28  2024-07-27          15275                                  ADITYA PALEPU (X - B)           500.0\n",
      "28    29  2024-07-27          14606                       GAGAN VATSAV CHELLUBOINA (X - C)           500.0\n",
      "29    30  2024-07-27          15402                                KUSUMA KURMANNA (X - C)           500.0\n",
      "30    31  2024-07-29          16587                             JAYVARDHAN MOOGI (III - D)           500.0\n",
      "31    32  2024-07-29          16393                          VEDASYA GOWRI TEKKALI (I - C)           400.0\n",
      "32    33  2024-07-29          16393                          VEDASYA GOWRI TEKKALI (I - C)           500.0\n",
      "33    34  2024-07-29          16340                               JAAHNAVI NIMMALA (I - D)           500.0\n",
      "34    35  2024-07-29          17091                     KRISHNA KUMAR TIRUMAREDDY (II - A)           500.0\n",
      "35    36  2024-07-29          16441                        SONALI SINGH KSHATRIYA (II - C)           500.0\n",
      "36    37  2024-07-29          16500                  TIRUMALA SAI SARANYA CHAVALI (II - D)           500.0\n",
      "37    38  2024-07-29          17060                                SHAAMISH VANAM (II - D)           500.0\n",
      "38    39  2024-07-29          15711                           SANCHITA MAHAPATRA (III - A)           500.0\n",
      "39    40  2024-07-30          15324                              SOURAV MAHAPATRA (VI - D)           500.0\n",
      "40    41  2024-07-30          16192                             MADHULIKA MADAGALA (V - D)           500.0\n",
      "41    42  2024-07-31          16324                             CHINMAYI CHANDRANA (I - C)           500.0\n",
      "42    43  2024-08-01          15748                                 VAIBHAV SHARMA (X - B)           500.0\n",
      "43    44  2024-08-02          14606                       GAGAN VATSAV CHELLUBOINA (X - C)         20900.0\n",
      "44    45  2024-08-13          14881                                 ANEESA SHAIK (VII - D)           500.0\n",
      "45    46  2024-08-19          15347                                RACHANA DHANALA (V - D)         15400.0\n",
      "46    47  2024-08-19          14615                              ASHIKA DHANALA (VIII - B)         19800.0\n",
      "47    48  2024-09-14          16228                            DAKSHETH EVAN ADARI (X - C)         20900.0\n",
      "48    49  2024-09-28          14321                         RANVITHA RAGHNI KARRI (IX - B)         20900.0\n",
      "49    50  2024-10-18          15744                           MOHAMMED ABDUL AAIZ (IV - C)         14850.0\n",
      "50    51  2024-10-29          14747                              SAMANTHA AKULA (VIII - C)         19800.0\n",
      "51    52  2024-12-23          15402                                KUSUMA KURMANNA (X - C)         20400.0\n",
      "52    53  2024-12-24          15402                                KUSUMA KURMANNA (X - C)           500.0\n",
      "53    54  2025-02-04          16183                         MOHAMMAD ALIYA REHAMAN (X - B)         20900.0\n",
      "54    55  2025-02-04          13917                    VENKATA NAGA AASHINI KORADA (X - B)         20900.0\n",
      "55    56  2025-02-04          14406                 LEISHA SRI AAPYAYA GANDEPALLI (IX - B)         20900.0\n",
      "56    57  2025-02-09          17069                               VIDYASRI GUNTURU (V - D)          7700.0\n",
      "57    58  2025-02-09          17070                             DIVYASRI GUNTURU (VII - B)          7975.0\n",
      "58    59  2025-02-09          16405                                   AVNIKA PAUL (II - A)         14300.0\n",
      "59    60  2025-02-09          15274                PAVAN KARTHIKEYA CHINTAPALLI (VIII - A)          2500.0\n",
      "60    61  2025-02-09          15546                      THANVI LASYA CHINTHAPALLI (V - A)          2500.0\n",
      "61    62  2025-02-09          16184                             HETHANSHRI DALLI (III - D)          7425.0\n",
      "62    63  2025-02-19          16600          RAVI KUMAR RANVITHA PRAYSHITHA SONGA (II - B)         14300.0\n",
      "63    64  2025-02-19          15980                             NITYA SREE DANDA (III - B)          7425.0\n",
      "64    65  2025-02-19          16975                         MISHIKA POLAMARASETTI (IV - A)         14850.0\n",
      "65    66  2025-02-20          15602                           CHETHAN SREEKAR PYLA (V - C)          7700.0\n",
      "66    67  2025-02-20          16564                         TEJASWI POLAMARASETTY (VI - D)          7700.0\n",
      "67    68  2025-02-20          14932                          HANVITH CHARAN PYLA (VII - C)          7975.0\n",
      "68    69  2025-02-20          15325                              MOKSHITHA CHALLA (VI - A)          5000.0\n",
      "69    70  2025-02-21          16163                        NAVYA KEERTHANA KONADA (IV - C)          2500.0\n",
      "70    71  2025-02-21          15526                               AARADHYA NISTALA (V - C)          7700.0\n",
      "71    72  2025-02-21          15612                            SIDDHISWARI VIYYAPU (V - C)          4000.0\n",
      "72    73  2025-02-21          15261                             CHARISHMA NISTALA (VI - A)          7700.0\n",
      "73    74  2025-02-21          14254                       SIRI CHANDANA SARIPILLI (IX - C)          4000.0\n",
      "74    75  2025-03-03          14850                            KIRANMAYI MADDILA (VII - D)          3000.0\n",
      "75    76  2025-03-04          14606                       GAGAN VATSAV CHELLUBOINA (X - C)           500.0\n",
      "76    77  2025-03-08          14000                                SHASANK GUVVADA (X - A)         10450.0\n",
      "77    78  2025-03-08          14931                           SHASANK KANDIPILLI (VII - A)          2500.0\n",
      "78    79  2025-03-08          15775                           JASMITHA KANDIPILLI (IV - C)          2500.0\n",
      "79    80  2025-03-08          16917                         RUSHANTH BABU DOPPPA (LKG - A)          6050.0\n",
      "80    81  2025-03-08          16347                            SASHANTH BABU DOPPA (I - C)          7150.0\n",
      "81    82  2025-03-08          14868                          HOLYEVANGILEN AKULA (VII - A)          3000.0\n",
      "82    83  2025-03-08          15869                              ALOYSIUS VERSILA (VI - C)          7700.0\n",
      "83    84  2025-03-08          15939                               ALPHONSA VERSILA (X - A)         10450.0\n",
      "84    85  2025-03-08          17020                            SIDDHARDHA JINNALA (II - A)          7150.0\n",
      "85    86  2025-03-08          13845                     THANOJ KRISHNA SAI JONNADA (X - B)          6000.0\n",
      "86    87  2025-03-08          14228                          JAYA SREE YALLAMALLI (IX - A)          8000.0\n",
      "87    88  2025-03-08          16374                               PAARDHAVI MOYYA (II - A)          5000.0\n",
      "88    89  2025-03-08          15648                              PREM SATWIK MOYYA (V - B)          5000.0\n",
      "89    90  2025-03-08          14108                            BLESSY AKULA (W.H) (IX - B)          3000.0\n",
      "90    91  2025-03-19          16143                    VELANGINI YASHMITHA KOYYA (III - A)          7425.0\n",
      "91    92  2025-03-19          15788                           DEEKSHITH ADDIPILLI (IV - B)          5000.0\n",
      "92    93  2025-03-19          15511                              NAYANA SRI BOOTHU (V - C)          7700.0\n",
      "93    94  2025-03-19          16200                            SUREKHA SRI BOOTHU (II - C)          7150.0\n",
      "94    95  2025-03-19          16024                       YOGITASREE YADAV KOYYA (III - B)          3000.0\n",
      "95    96  2025-03-19          15231                        GANESH CHAITANYA KOYYA (VI - B)          3000.0\n",
      "96    97  2025-03-19          16637                         DEETHYA LAXMANA VASU (UKG - A)          6050.0\n",
      "97    98  2025-03-19          15945                        LEKSHITA LAASYA POTNURU (V - A)             0.0\n",
      "98    99  2025-03-19          15945                        LEKSHITA LAASYA POTNURU (V - A)         15400.0\n",
      "99   100  2025-03-19          14224                           MAHENDRA SAI GUNANA (IX - B)         10450.0\n",
      "100  101  2025-03-19          16026                         DEEKSHITHA SRI KOYYA (III - D)          3000.0\n",
      "101  102  2025-03-19          15880                               MANJEET BOBBADI (VI - A)          7700.0\n",
      "102  103  2025-03-20          15479                            ASHLIN MOSES CHEEDI (V - C)         15400.0\n",
      "103  104  2025-03-20          16984                         CHARLIN MOSES CHEEDI (LKG - B)         12100.0\n",
      "104  105  2025-03-21          15780                              GAGANA SREE KODI (IV - D)          3000.0\n",
      "105  106  2025-03-22          17052                                NIHAARIKA FRANK (I - B)          7150.0\n",
      "106  107  2025-03-22          15923                              TRISWANTH MORSA (III - D)          7425.0\n",
      "107  108  2025-03-22          15301                              NAVAKANTH MORSA (VII - B)          7975.0\n",
      "108  109  2025-04-05          13834                                    FAZILA KHAN (X - B)         20900.0\n",
      "109  110  2025-04-05          17126                          BHAGATSINGH PONNADA (LKG - A)          6050.0\n",
      "110  111  2025-04-05          17125                           GOWRILANKESH PONNADA (I - A)          7150.0\n",
      "111  112  2025-04-05          15158  VARAHA SRINIVASA SURYA SATYA SAI JATIN TELU (VII - B)          7975.0\n",
      "112  113  2025-04-05          16783                      GOPIKA SNIGDHA VURIMITI (III - B)          4000.0\n",
      "113  114  2025-04-05          16782                         ROSHITA SREE VURIMITI (VI - C)          4000.0\n",
      "114  115  2025-04-05          14488                                  RIANNA SIMON (IX - A)         10450.0\n",
      "115  116  2025-04-05          15746                          YASHA YASHMINA NAKKA (IV - B)          7425.0\n",
      "116  117  2025-04-05          15746                          YASHA YASHMINA NAKKA (IV - B)          7425.0\n",
      "117  118  2025-04-05          15625                    JAHNAVI DEVI SRI SAI SEERA (IV - C)          7425.0\n",
      "118  119  2025-04-05          16969                                AKILESH SEERA (LKG - A)          6050.0\n",
      "119  120  2025-04-05          14099                            RHEON MARIO REBEIRO (X - A)         10450.0\n",
      "120  121  2025-04-05          15631                                  HANEEFA SHAIK (V - A)          7700.0\n",
      "121  122  2025-04-05          16317                               SHAIK HANEESHA (III - C)          7425.0\n"
     ]
    }
   ],
   "source": [
    "## **Main Execution Flow*\n",
    "def main():\n",
    "    session = login_to_website()\n",
    "    if session is None:\n",
    "        return\n",
    "    \n",
    "    df = fetch_all_concession_tables(session)\n",
    "    if df is None or df.empty:\n",
    "        print(\"‚ùå No data found! The page structure might have changed.\")\n",
    "        return\n",
    "    \n",
    "    print(\"‚úÖ Data extracted successfully! Cleaning data...\\n\")\n",
    "    df = clean_data(df)\n",
    "    print(\"‚úÖ Data cleaned successfully!\\n\")\n",
    "    \n",
    "    print(\"‚úÖ Columns are:\\n\", df.columns)\n",
    "    print(df.dtypes)\n",
    "\n",
    "    \n",
    "    output_file = r\"D:\\\\GITHUB\\\\kotak-school-dbms\\\\output_data\\\\fee_concession_report_2024_25.csv\"\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"‚úÖ Data saved to '{output_file}'\\n\")\n",
    "    \n",
    "    update_database(df, TABLE_NAME, POSTGRES_CREDENTIALS)\n",
    "    print(f\"‚úÖ {len(df)} records entered into the database\")\n",
    "    \n",
    "    print(df.to_string())\n",
    "\n",
    "## **Run the Main Function*\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
